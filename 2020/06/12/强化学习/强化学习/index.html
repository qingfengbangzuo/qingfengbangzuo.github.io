<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="RL," />





  <link rel="alternate" href="/atom.xml" title="Qingfengbangzuo" type="application/atom+xml" />






<meta name="description" content="Incremental Implementation Q_{n+1} &#x3D; Q_n + \frac{1}{N(A)}[R_n - Q_n]注意step_size $N(A)$，它随着时间步的累计而线性增长，但是通常，最近的奖赏相比过去很久的奖赏更有说服力，因此，我们希望给最近发生的奖赏一个稍大的权重。解决办法是将$N(A)$设置为固定值。  \begin{align*} Q_{n+1}&amp; &#x3D; Q_n">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习一些笔记">
<meta property="og:url" content="https://qingfengbangzuo.github.io/2020/06/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Qingfengbangzuo">
<meta property="og:description" content="Incremental Implementation Q_{n+1} &#x3D; Q_n + \frac{1}{N(A)}[R_n - Q_n]注意step_size $N(A)$，它随着时间步的累计而线性增长，但是通常，最近的奖赏相比过去很久的奖赏更有说服力，因此，我们希望给最近发生的奖赏一个稍大的权重。解决办法是将$N(A)$设置为固定值。  \begin{align*} Q_{n+1}&amp; &#x3D; Q_n">
<meta property="article:published_time" content="2020-06-11T16:00:00.000Z">
<meta property="article:modified_time" content="2020-06-22T12:16:44.189Z">
<meta property="article:author" content="qingfengbangzuo">
<meta property="article:tag" content="RL">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":20,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://qingfengbangzuo.github.io/2020/06/12/强化学习/强化学习/"/>





  <title>强化学习一些笔记 | Qingfengbangzuo</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    
    <a href="https://your-url" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Qingfengbangzuo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">私人博客</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://qingfengbangzuo.github.io/2020/06/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="qingfengbangzuo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qingfengbangzuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">强化学习一些笔记</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-12T00:00:00+08:00">
                2020-06-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Reinforcement Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3.8k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  15
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="Incremental-Implementation"><a href="#Incremental-Implementation" class="headerlink" title="Incremental Implementation"></a>Incremental Implementation</h3><script type="math/tex; mode=display">
Q_{n+1} = Q_n + \frac{1}{N(A)}[R_n - Q_n]</script><p>注意step_size $N(A)$，它随着时间步的累计而线性增长，但是通常，最近的奖赏相比过去很久的奖赏更有说服力，因此，我们希望给最近发生的奖赏一个稍大的权重。解决办法是将$N(A)$设置为固定值。</p>
<script type="math/tex; mode=display">
\begin{align*}
Q_{n+1}& = Q_n + \alpha[R_n - Q_n]\\
 &=\alpha R_n +(1-\alpha)Q_n
 \\&=\alpha R_n +(1-\alpha)[\alpha R_{n-1}+(1-\alpha)Q_{n-1 }]
 \\&=\alpha R_n+(1-\alpha)\alpha R_{n-1}+(1-\alpha)^2\alpha R_{n-2}+...+(1-\alpha)^{n-1}\alpha R_1+(1-\alpha)^n Q_1
 \\&=(1-\alpha)^n Q_1+\sum_{i=1}^n \alpha(1-\alpha)^{n-i}R_i
\end{align*}</script><p>可以看到，当设置步长为一个固定值的时候，我们实现了如期的效果。原文称之为:<em>exponential<br>recency-weighted average.</em></p>
<p>对于采样平均策略中$a = N(A)$,由大数定理可知必会依概率1收敛。</p>
<script type="math/tex; mode=display">
\sum_{n=1}^{\infty} a_n(a) = \infty \\
\sum_{n=1}^{\infty}a_n^2(a) < \infty</script><p>第一个条件保证了步长足够大使得最终克服初始值或随机影响而收敛。第二个条件保证了最终，步长变得足够小从而保证收敛。</p>
<p>固定值$a$是不满足上述两个条件的，表明预测值不会完全收敛但是会继续随着最近接收到的reward而变化，这种属性适合nonstaionary 环境或者nonstationart问题。并且，满足上述条件的步长$a$需要调参来达到合适的收敛率，这在理论工作中比较常用，但是在实际问题中并不常用。</p>
<h3 id="optimistic-initial-values"><a href="#optimistic-initial-values" class="headerlink" title="optimistic initial values"></a>optimistic initial values</h3><p>书中提到了三种简单的method，一种是sample-average，一种是exponential recency-weighted average,最后一种是这块要介绍的optimistic initial values。</p>
<p>这种方案是聚焦在初始值上，因为大多数stationary问题都需要依赖初始的Q值，被称为偏差，如上一块公式所示，这个偏差随着时间并不会消失（虽然随着时间在减小）。显然我们不满足于将初始值设为0，乐观初始值是指将初始值设为 一个比奖赏大的数，比如+5，这样，因为所有的奖赏都小于这个预设值，因此，一轮更新过后，agent会竭尽所能的去探索。因此，它的最优率相对于sample-average收敛较快。</p>
<h3 id="Upper-Confidence-Bound-Action-Selection"><a href="#Upper-Confidence-Bound-Action-Selection" class="headerlink" title="Upper-Confidence-Bound Action Selection"></a>Upper-Confidence-Bound Action Selection</h3><script type="math/tex; mode=display">
A_t = argmax_a[Q_t(a)+c\sqrt{\frac{lnt}{N_t(a)}}]</script><p>c是调节参数，$lnt$表示随着时间增大，不确定性的增长变缓。$N_t(a)$表示t之前动作a被选择的次数。整个公式的作用是随着时间的进行，那些Q值小的动作和那些被选过好多次的动作再次被选中的频率会下降，而那些未被选择过得动作则有越来越大的机会被选中。最终所有的动作都会被选中，实验效果表明，采用UCB的效果较$\epsilon$-greedy略好。</p>
<h3 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h3><script type="math/tex; mode=display">
V_*(s) = max_a E[R_{t+1}+\gamma V_*(S_{t+1})|S_t = s,A_t =a]\\
=max_a \sum_{s',r}p(s',r|s,a)[r+\gamma V_*(s')]\\
q_*(s,a) = E[R_{t+1}+\gamma max_{a'}q_*(S_{t+1},a')|S_t=s,A_t=a]\\
= \sum_{s',r}p(s',r|s,a)[r+\gamma max_{a'}q_*(s',a')]</script><p>贝尔曼最优方程有这样一个特性，即V（s） = $max_a$q(s,a),即它的状态值不断靠近当前状态下的最大动作值。</p>
<h3 id="policy-evaluation"><a href="#policy-evaluation" class="headerlink" title="policy evaluation"></a>policy evaluation</h3><script type="math/tex; mode=display">
V_\pi(s)= \sum_a\pi(a|s)\sum_{s',r} p(s',r|s,a)[r+\gamma V_\pi(s')]</script><p>给定一确定策略，通过Policy evaluation可以得到该策略下的值函数。（多次迭代，最终收敛于一个个确定值）</p>
<h3 id="policy-Increament"><a href="#policy-Increament" class="headerlink" title="policy Increament"></a>policy Increament</h3><script type="math/tex; mode=display">
q_\pi (s,\pi'(s))\geq v_\pi(s)</script><p>对于所有的s满足上式，则$\pi’$优于$\pi$。</p>
<p>Greedy策略满足上述要求</p>
<script type="math/tex; mode=display">
\pi'(s ) = argmax_a q_\pi(s,a)
=argmax_a\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]</script><script type="math/tex; mode=display">
v_{\pi'}(s) = max_a E[R_{t+1}+\gamma v_{\pi'}(S_{t+1})|S_t =s,A_t =a]\\
= max_a \sum_{s',r}p(s',r|s,a)[r+\gamma v_{\pi'}(s')]</script><h3 id="policy-Iteration"><a href="#policy-Iteration" class="headerlink" title="policy Iteration"></a>policy Iteration</h3><script type="math/tex; mode=display">
\pi_0 \rarr v_{\pi_0}\rarr\pi_1\rarr v_{\pi_1} \rarr \pi_2\rarr...\rarr\pi_*\rarr  v_{*}</script><p>给定一个策略，根据policy evaluation可以计算当前策略下的值函数，然后对当前策略进行提升，循环往复，最终收敛到最优策略和最优值函数。</p>
<h3 id="value-Iteration"><a href="#value-Iteration" class="headerlink" title="value Iteration"></a>value Iteration</h3><p>one update of each state is called value iteration.相比于策略迭代，需要迭代到该策略下的最优value值。</p>
<p>利用贝尔曼最优方程：</p>
<script type="math/tex; mode=display">
V_{k+1}(s) = max_a E[R_{t+1}+\gamma V_k(S_{t+1})|S_t = s,A_t =a]\\
=max_a \sum_{s',r}p(s',r|s,a)[r+\gamma V_k(s')]\\</script><p>对于任意$v_0$，序列{$v_k$}在相同的条件下能够收敛到$v_*$。</p>
<p>如何终止呢？</p>
<p>当值函数变化被压缩在一个阈值$\theta$内时，就可以强行进行终止了。</p>
<p>Value Iteration加快了收敛速度，因为不需要进行完整的evaluation,它是通过使用贝尔曼最优方程的特性，使得V(s)不断地靠地$max_a$ q(s,a)</p>
<h3 id="Asynchronous-Dynamic-Programming"><a href="#Asynchronous-Dynamic-Programming" class="headerlink" title="Asynchronous Dynamic Programming"></a>Asynchronous Dynamic Programming</h3><p>这里的非同步动态规划是指取消了更新轮数的限制，即并非在一轮更新中更新所有的状态，而是仅仅针对某些或者某个状态进行更新，这样就会出现，某些状态可能更新了n多次，但是某些状态仅仅只更新了一两次。</p>
<p>policy evaluation 、value Iteration、asynchronous dynamic programming三者的区别：</p>
<p>policy evaluation和value Iterationd的区别在于前者更新知道达到当前策略下的最优v值，而后者通过更新公式仅仅更新一次（轮）。非同步动态规划与前两者的区别是它没有更新轮次的概念，可能仅仅与最优行为有关，那些与最优行为无关的状态可能不会得到太大的关注。它的更新粒度最小。</p>
<h3 id="MOnte-carlo-methord"><a href="#MOnte-carlo-methord" class="headerlink" title="MOnte carlo methord"></a>MOnte carlo methord</h3><p>较DP优势：</p>
<p>1、没有用到bootstrap,且每个状态的estimate都是独立的，因为在估计状态值的时候没有用到其他状态值。</p>
<p>2、估计单个状态值的计算代价与状态的数量无关。只与采样有关。</p>
<p>3、我们可以从任何状态起开始采样，且状态值估计不受这个影响。</p>
<p>蒙特卡罗采样只与experience有关，这个experience可以是实际经历也可以是virtual experience，分为every-visit average 和 first-visit average。这两种平均都能收敛到$v_\pi$，前提是episodes的数量趋近于无穷。</p>
<p>在环境信息已知的情况下，通过v值就可以确定最优策略，但是在蒙特卡罗采样策略下，只通过v值确定最优策略是不充分的，我们还需要确定$q_\pi（s,a）$。</p>
<p>在蒙特卡罗采样策略下，因为v值和q值都是通过从experience中采样得到的，但是在stationary策略下，每个状态下的动作是确定的，也就是说experience中可能只有某个状态下的某个动作（或状态），其他动作不会出现，而他们相应的q值因为无法采样而为0.这会影响收敛，因为收敛需要遍历每个状态和动作。</p>
<p>解决这个问题有两种方法，一种是对初始状态进行随机化，每个状态以某个概率随机出现，这样，随着episodes数量趋于无穷，状态会得到遍历，最终会收敛，这被称作<strong>Exloration Starting</strong>。但是这种方法在实际场景中不能保证收敛性，也不实用。另一种是在每个状态下，给予其他动作以一定的出现概率。这样就会使得策略变成non-stationary。但是效果较好。</p>
<p>目前讨论的蒙特卡罗策略有两个假设。</p>
<p>One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes.</p>
<p>下面的内容介绍了去除这两个假设的方法。</p>
<p>去掉假设2是相当对容易的，有两种思路，一种是在evaluation中保证足够多的迭代次数。另一种是引入Value Iteration的思想，不需要得到准确的$v_\pi$值，换成蒙特卡罗采样也就是，在策略$\pi$下，不需要大量采样，只需要一次采样，即一个episode（蒙特卡洛的采样数量对应了DP的更新迭代次数，因为DP下，更新次数越多，越接近$V_\pi$，但在蒙特卡罗策略下采样次数决定了靠近$V_\pi$的距离）。这样也就是变向的将evaluation和improvement交替进行，即下面Monte carlo ES所示。</p>
<h3 id="Monte-carlo-ES"><a href="#Monte-carlo-ES" class="headerlink" title="Monte carlo ES"></a>Monte carlo ES</h3><p>这个算法的特殊之处在于它将evaluation过程和improvement过程交替进行，这样依然会收敛于最优策略，但是似乎还未被证明。</p>
<p>可以使用Incremental Implementation来替代上述中维护return列表的做法。</p>
<p>而解决假设1的也有两种方案，即on-policy和off-policy。</p>
<p>终于找到了on-policy和off-policy的区别。这两种方法被用来解决Exploration Staring问题。</p>
<p>On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas o↵-policy methods evaluate or improve a policy di↵erent from that used to generate the data.</p>
<h3 id="On-policy-and-Off-policy"><a href="#On-policy-and-Off-policy" class="headerlink" title="On-policy and Off-policy"></a>On-policy and Off-policy</h3><p>在继续介绍on-policy和off-policy之前，先给出两个概念：</p>
<p>所谓的off-policy是指使用生成数据的策略和要学习的策略是不同的两个策略，前者被称为<strong>behaivior policy</strong>后者被称为<strong>target policy</strong>，这两个策略是不同的，也就是说我们用来学习数据是通过其他的策略产生的，或者说这个数据本身就是一些经验数据，但是我们需要从这些经验数据中学习我们的最优策略。所谓的学习策略就是更新策略。on-policy就是将生成数据的策略与要学习的策略为同一策略，是off-policy的一种特例。</p>
<p>On - policy策略一般是soft的，即$\pi(a|s)&gt;0$，也就是每个动作的选择概率均大于0，将贪婪策略soft化，就是常见的$\epsilon-greedy$策略。</p>
<p>在off-policy control中，生成数据的策略通常也使用$\epsilon-greedy$ policy，因为behavior-policy需要是exporation policy。而学习策略一般是greedy policy。</p>
<p>注：off-policy的一个优点</p>
<p>‘’An advantage of this separation is that the target policy may be deterministic (e.g., greedy), while   the behavior policy can continue to sample all possible actions.‘’</p>
<p>由于我们使用的数据是经过behavior policy下产生的，所以我们得到的v值是behavior policy策略下的，而不是我们的学习策略下的v值。由于我们的最终目标是不断改善我们的target policy，得到最优v值和最优策略，因此我们需要得到的是target policy下的v值，这就产生了一个问题，如何转换？</p>
<h3 id="Importance-Sampling"><a href="#Importance-Sampling" class="headerlink" title="Importance Sampling"></a>Importance Sampling</h3><p>重要性采样用来解决上述问题。</p>
<p>假设有N个episodes，每个episodes在不同的策略下都对应这一个出现概率，计算方法如下：</p>
<script type="math/tex; mode=display">
Pr = \prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)</script><p>定义重要性采样率（importance-sampling ratio）</p>
<script type="math/tex; mode=display">
p_{t:T-1} = \frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_k,A_k)}=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}</script><p>通过转换公式</p>
<script type="math/tex; mode=display">
E[p_{t:T-1}G_t|S_t=s] = v_\pi(s)</script><p>以上就是重要性采样的思想。</p>
<p>那么计算$v_\pi（s）$有两种metrics（“初访” 情况下）。分别是</p>
<p>ordinary importance sample :</p>
<script type="math/tex; mode=display">
V(s) = \frac{\sum_{t\in J(s)}p_{t:T-1}G_t}{|J(s)|}</script><p>the set of all time steps in which state s is visited, denoted J(s).</p>
<p>weighted importance sampling：</p>
<script type="math/tex; mode=display">
V(s) = \frac{\sum_{t\in J(s)}p_{t:T-1}G_t}{\sum_{t\in J(s)}p_{t:T-1}}</script><p>可见weighted importance sampling对V（s）进行了方差处理，而ordinary importance sampling没有，实际上ordinary 的方差可以是无穷大的，但是它的biase很小，且随着samples数量趋近于无穷，它的方差也会随之下降，但是这是一个缓慢的过程，在例子中可以看到，millions steps后，它并没有准确收敛到1。而weighted效果要好一点，它始终是围绕着均值上下波动的，但是该均值是有偏的，但是同样会随着samples的增多而收敛到正确的均值，也就是最优v值。</p>
<p>在实际应用中，“每访”的效果更佳，但是”每访“情况下，ordinary和weighted都是有偏的，但是都会随着samples的增加而趋于正确的均值。</p>
<p>注：infinite variance</p>
<p>当MDP中出现了闭环，那么会导致无限方差的情况，这种情况下，随着sample增加到很大，可能也不能很好的收敛到正确值，具体例子看P107页例。</p>
<h3 id="Incremental-Implementation-on-Monte-carlo-sampling"><a href="#Incremental-Implementation-on-Monte-carlo-sampling" class="headerlink" title="Incremental Implementation on Monte carlo sampling"></a>Incremental Implementation on Monte carlo sampling</h3><p>根据前面提到的Incremental Implementation 方法来用于有重要性采样的Monte carlo问题中。</p>
<p>对于ordinary importance sampling，直接将采样公式变换成Incremental Implementation形式就行。</p>
<p>对于weighted 采样，需要做一点改变</p>
<script type="math/tex; mode=display">
V_{n+1} = V_n + \frac{W_n}{C_n}[G_n -V_n]\\
C_{n+1} = C_n + W_{n+1}</script><p>这个是”每访”下的算法，注意算法中的C和Q是两个列表，也就是说C和Q各维持了一个存放不同（S_t,A_t）的列表，每一轮episode来临时，都会更新列表中相应的状态动作对值，对于“初访”情形，需要设置一个条件来访问：</p>
<p>‘’’’’’——————————————————————————————</p>
<p>​    $G\larr\gamma G+R_{t+1}$</p>
<p>​        unless the Pair ($S_t,A_t$) appear in $S_0,A_0,S_1,A_1,…S_{t-1},A_{t-1}$</p>
<p>​            $C(S_t,A_t)\larr C(S_t,A_t)+W$</p>
<p>​            $Q(S_t,A_t)\larr Q(S_t,A_t)+ \frac{W}{C(S_t,A_t)}[G-Q(S_t,A_t)]$</p>
<p>​    $W \larr W\frac{\pi(A_t|S_t)}{b(A_t|S_t)}$</p>
<p>‘’’————————————————————————————————</p>
<p>最后一行，由于使用的是greedy策略，所以$\pi(a|s)= 1或0$,注意倒数第二句，如果$A_t \neq\pi(S_t)$程序就会退出，此时$\pi(A_t|S_t) = 0 $.</p>
<p><strong>当episode中大部分动作是non-greedy，尤其靠前部分是Non-greedy的时候，这个算法收敛将变得十分缓慢。</strong></p>
<p>出现这样的现象是直观的，因为这个算法倒数第二行设置了退出条件，由于算法是从尾部遍历到头部，所以当尾部出现了non-greedy的动作，那么就会退出内循环，开始下一个episode，这样就会造成该non-greedy动作之前的所有episode报废。</p>
<h3 id="Timporal-Difference-Learning"><a href="#Timporal-Difference-Learning" class="headerlink" title="Timporal-Difference Learning"></a>Timporal-Difference Learning</h3><p>TD是DP和Monte carlo采样的结合，因为在更新过程中，它既对$\pi$进行了采样，又使用了bootstrap，这里的bootstrap和DP中的不同，在DP中使用的expected return 和expected value（即根据分布得到的return 和value均值），而TD在更新时，需要根据一步采样，得到$R_{t+1}$和$S’$，使用$R_{t+1}$+$\gamma V(S’)$来更新，这里的$R_{t+1}$是采样值，这里的$V（S’）$是估计值。</p>
<p>无论是Mento carlo 还是TD，它们都是通过”error”来更新的，并且它们之间存在关系如下：</p>
<script type="math/tex; mode=display">
G_t - V(S_t) = R_{t+1} + \gamma G_{t+1} - V（S_t）-\gamma V(S_{t+1})\\
=\delta_t +\gamma (G_{t+1}-V(S_{t+1}))\\
=\delta_t + \gamma \delta_{t+1}+ \gamma^2（G_{t+2}-V（S_{t+2}))\\
=\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2}+...+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-1}(G_T-V(S_T))\\
=\delta_t + \gamma\delta_{t+1}+\gamma^2\delta_{t+2}+...+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t}(0-0)\\
=\sum_{k=t}^{T-1}\gamma^{k-t}\delta_k</script>
      
    </div>
    
    
    
    
    <div>
      
        
      
    </div>

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/RL/" rel="tag"><i class="fa fa-tag"></i>RL</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%91%E7%B1%BB%E6%A8%A1%E5%9E%8B/%E5%86%B3%E7%AD%96%E6%A0%91/" rel="next" title="决策树">
                <i class="fa fa-chevron-left"></i> 决策树
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/" rel="prev" title="高斯过程">
                高斯过程 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">qingfengbangzuo</p>
              <p class="site-description motion-element" itemprop="description">世界上只有一种英雄主义，那就是看清生活的真相之后依然热爱生活。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives%7C%7Carchive">
              
                  <span class="site-state-item-count">28</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/qingfengbangzuo" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="cqupt_lixz@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-globe"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                推荐阅读
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://ife.baidu.com/" title="百度前端技术学院" target="_blank">百度前端技术学院</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Incremental-Implementation"><span class="nav-number">1.</span> <span class="nav-text">Incremental Implementation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#optimistic-initial-values"><span class="nav-number">2.</span> <span class="nav-text">optimistic initial values</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Upper-Confidence-Bound-Action-Selection"><span class="nav-number">3.</span> <span class="nav-text">Upper-Confidence-Bound Action Selection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#贝尔曼最优方程"><span class="nav-number">4.</span> <span class="nav-text">贝尔曼最优方程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#policy-evaluation"><span class="nav-number">5.</span> <span class="nav-text">policy evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#policy-Increament"><span class="nav-number">6.</span> <span class="nav-text">policy Increament</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#policy-Iteration"><span class="nav-number">7.</span> <span class="nav-text">policy Iteration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#value-Iteration"><span class="nav-number">8.</span> <span class="nav-text">value Iteration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Asynchronous-Dynamic-Programming"><span class="nav-number">9.</span> <span class="nav-text">Asynchronous Dynamic Programming</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MOnte-carlo-methord"><span class="nav-number">10.</span> <span class="nav-text">MOnte carlo methord</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Monte-carlo-ES"><span class="nav-number">11.</span> <span class="nav-text">Monte carlo ES</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#On-policy-and-Off-policy"><span class="nav-number">12.</span> <span class="nav-text">On-policy and Off-policy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Importance-Sampling"><span class="nav-number">13.</span> <span class="nav-text">Importance Sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Incremental-Implementation-on-Monte-carlo-sampling"><span class="nav-number">14.</span> <span class="nav-text">Incremental Implementation on Monte carlo sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Timporal-Difference-Learning"><span class="nav-number">15.</span> <span class="nav-text">Timporal-Difference Learning</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">qingfengbangzuo</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">55.6k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  





  
  



  
  



  



  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="https://github.com/jjandxa/canvas_sphere"></script>
  

  
  
    <script type="text/javascript" src="https://github.com/zproo/canvas-ribbon"></script>
  

  
  
    <script id="ribbon" type="text/javascript" size="300" alpha="0.6"  zIndex="-1" src="https://github.com/ethantw/Han"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  


  
  <script src="/live2d-widget/autoload.js"></script>
  
</body>
</html>
