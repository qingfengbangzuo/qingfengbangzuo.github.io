<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="ML," />





  <link rel="alternate" href="/atom.xml" title="Qingfengbangzuo" type="application/atom+xml" />






<meta name="description" content="Xgboostxgboost是在回归决策树的基础上经过boosting后的一种集成算法。 学习一个算法需要从以下几点入手： 损失函数xgboost是在原gbdt的损失函数基础上，进行了正则项加持和二次泰勒展开。 加性树：  F^t(X) &#x3D; \sum_{i&#x3D;1}^t T_i(X) &#x3D; F^{t-1}(X) + T_t(X)前向分步算法：  argmin_{T_t(X)} L(Y,F^{t-1}(X">
<meta property="og:type" content="article">
<meta property="og:title" content="基于GBDT的算法">
<meta property="og:url" content="https://qingfengbangzuo.github.io/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/index.html">
<meta property="og:site_name" content="Qingfengbangzuo">
<meta property="og:description" content="Xgboostxgboost是在回归决策树的基础上经过boosting后的一种集成算法。 学习一个算法需要从以下几点入手： 损失函数xgboost是在原gbdt的损失函数基础上，进行了正则项加持和二次泰勒展开。 加性树：  F^t(X) &#x3D; \sum_{i&#x3D;1}^t T_i(X) &#x3D; F^{t-1}(X) + T_t(X)前向分步算法：  argmin_{T_t(X)} L(Y,F^{t-1}(X">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-07%2012-02-48%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-07%2012-28-23%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-07%2012-47-27%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-02%2017-39-46%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-02%2017-50-30%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-07%2018-32-27%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-08%2009-50-23%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-08%2010-18-30%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-08%2010-18-43%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png">
<meta property="article:published_time" content="2020-06-11T16:00:00.000Z">
<meta property="article:modified_time" content="2020-06-12T10:50:32.605Z">
<meta property="article:author" content="qingfengbangzuo">
<meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qingfengbangzuo.github.io/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-07%2012-02-48%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":20,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://qingfengbangzuo.github.io/2020/06/12/xgboost,lightgbm和catboost/"/>





  <title>基于GBDT的算法 | Qingfengbangzuo</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    
    <a href="https://your-url" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Qingfengbangzuo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">私人博客</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://qingfengbangzuo.github.io/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="qingfengbangzuo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qingfengbangzuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">基于GBDT的算法</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-12T00:00:00+08:00">
                2020-06-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/%E6%A0%91%E7%B1%BB%E7%AE%97%E6%B3%95/" itemprop="url" rel="index">
                    <span itemprop="name">树类算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  2k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  7
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Xgboost"><a href="#Xgboost" class="headerlink" title="Xgboost"></a>Xgboost</h1><p>xgboost是在回归决策树的基础上经过boosting后的一种集成算法。</p>
<p>学习一个算法需要从以下几点入手：</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>xgboost是在原gbdt的损失函数基础上，进行了正则项加持和二次泰勒展开。</p>
<p>加性树：</p>
<script type="math/tex; mode=display">
F^t(X) = \sum_{i=1}^t T_i(X) = F^{t-1}(X) + T_t(X)</script><p>前向分步算法：</p>
<script type="math/tex; mode=display">
argmin_{T_t(X)} L(Y,F^{t-1}(X)+T_t(X))</script><p>GBDT目标函数：</p>
<p>​    定义回归树的数学形式，$c_m$是叶子节点的值，$R_m$是叶子节点的范围(属于该点的点的集合)。</p>
<script type="math/tex; mode=display">
T(x_i) = \sum_{m} c_m\mathbf{1}\{x_i \in R_m\}</script><p>​    目标函数（j,s分别是最优特征和最优特征的最优分裂点）</p>
<script type="math/tex; mode=display">
min_{j,s}  min_{c_m} [\frac{L(y_i,F^{t-1}(x_i))}{F^{t-1}(x_i)} -\sum_{m}c_m\mathbf{1}\{x_i \in R_m\}] \quad for i = 1,2,...,n(数据个数)</script><p>ＧＤＢＴ最大的特点就是利用一阶导数来近似前t-1颗树的误差，然后再用第t颗树来拟合该误差，直到达到停止条件。</p>
<ul>
<li><p>为什么要使用一阶导数来近似误差？</p>
<p>为了适应不同的损失函数。提升树利用加法模型和前向分布算法，当损失函数是平方损失和指数损失函数时，每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化并不那么容易。</p>
</li>
<li><p>为什么一阶负梯度可以用来近似误差？</p>
<p>对于任何损失函数$L(y_i,F^{t-1}(x_i)＋f_t(x_i))$，若将其视为一个关于$F^{t-1}(x_i)$的函数，我们的目标是最小化该损失函数。那么我们沿着该损失函数的梯度下降可能是一个不错的选择。我们所说的用第t颗树去拟合前t-1颗树的误差是在损失函数为rmse的语境下成立的，若果是其他的损失函数，用梯度来拟合误差不是很恰当，此时可以理解为采用梯度下降来最小化损失函数。</p>
</li>
<li><p>注意一点：引入梯度会造成累积误差。</p>
</li>
</ul>
<p>xgboost目标函数：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^nL(y_i,F^{t}(x_i)) + \sum_{k=1}^t \Omega(f_k) \\
where \quad\Omega(f) = \gamma T+\frac{1}{2}\lambda||w||^2</script><p>前向分步版本：</p>
<script type="math/tex; mode=display">
L^{(t)} = \sum_{i=1}^n L(y_i,F^{t-1}(x_i)+T_t(x_i))+\Omega(T_t)</script><p>进行２阶泰勒展开</p>
<script type="math/tex; mode=display">
L^{(t)} \simeq \sum_{i=1}^n[L(y_i,F^{t-1}(x_i))+g_iT_t(x_i)+\frac{1}{2}h_iT_t^2(x_i)]+\Omega(T_t)</script><p>这里的g和h分别是损失函数对$F^{t-1}(x_i)$的一阶导和二阶导。</p>
<p>将树模型数学表达式和正则化项带入后可以得到：</p>
<script type="math/tex; mode=display">
\tilde{L}^{t} =\sum_{i=1}^n[g_iT_t(x_i)+\frac{1}{2}h_iT_t^2(x_i)]+\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2 \\
=\sum_{j=1}^T[(\sum_{i\in I_j}g_i)w_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda )w_j^2]+\gamma T　＋const</script><p>$T$指的是共有多少个叶子节点，$I_j$指的是每个叶子节点的范围。这个式子的意思表达的是，将所有属于$I_i$叶子内的所有点的一阶梯度和二阶梯度都加起来，使得构成一个关于$w_j$的二次函数。</p>
<p>注意：上面的式子假设我们已经知道了第t颗树的结构。</p>
<p>当我们已知第t颗树的结构时，我们可以根据上面式子，求解最优的$w_j^*$</p>
<script type="math/tex; mode=display">
w_j^* = -\frac{\sum_{i\in I_j}g_i}{\sum_{i \in I_j}h_i+\lambda}</script><p>带入原式子可以得到：</p>
<script type="math/tex; mode=display">
\tilde{L}^{(t)}(q) = -\frac{1}{2}\sum_{j=1}^T\frac{(\sum_{i\in I_j}g_i)^2}{\sum_{i \in I_j}h_i+\lambda} + \gamma T</script><p>我们定义该式子为<strong>结构分</strong>。该式子可以用来计算任意叶子$i\in I_j$结构分.</p>
<p>那么分裂点的增益就可以通过结构分来计算了：</p>
<script type="math/tex; mode=display">
\frac{1}{2}[\sum_{j=1}^T\frac{(\sum_{i\in I_L}g_i)^2}{\sum_{i \in I_L}h_i+\lambda} +\sum_{j=1}^T\frac{(\sum_{i\in I_R}g_i)^2}{\sum_{i \in I_R}h_i+\lambda}-\sum_{j=1}^T\frac{(\sum_{i\in I}g_i)^2}{\sum_{i \in I}h_i+\lambda}]- \gamma</script><p>至此，我们已经通过xgboost的目标函数得到了结构分，结构分的作用是计算分裂收益。注意得出结构分的前提依然是最小化目标函数，我们是在这个前提下推出结构分的，我们的目标依然是最小化目标函数，这点和GBDT一样，结构分只是告诉我们，当我们构建新树的时候，使用这样的结构分去构建，能够让目标函数下降的更快。</p>
<ul>
<li><p>怎样理解GBDT和xgboost目标函数的不同？</p>
<p>GBDT是利用变量沿梯度方向下降最快这个思想去最小化目标函数。是把$F^{t-1}＋T_t$当做一个变量，当这个变量沿着负梯度方向变化时，目标函数下降的最快。</p>
<p>xgboost对目标函数的解读方式不同，它将$T_t$视为一个变量，然后将目标函数进行泰勒展开。分析出了$T_t$进行怎么样的取值能够让目标函数最小。分析的结果其实挺简单，给我一个$T_t$值，我算出该值下的近似目标函数大小，然后比较哪个小，我就取哪个$T_t$，只不过变量$T_t$是一颗树，而不是一个单变量，因此需要进行分裂。</p>
<p>解读目标函数的方式不同。</p>
</li>
<li><p>为什么说GBDT树不如xgboost树好？</p>
<p>发生这种情况的原因之一是xgboost加了正则项。第二个原因是xgboost加入了列采样，三是使用了二阶导数信息。四是支持缺失值处理。五是并行支持效果要更好。</p>
<p>自己琢磨了两个原因：第一个还是对目标函数的解读。GBDT是用树去拟合一阶梯度。而xgboost是直接使用树取寻找使得目标函数最小的值，也就是说理想情况下xgboost的树能找到当前目标函数的近似最优点。而沿着梯度下降不一定。这个更好也表现在对树分裂时的约束更好。而且，假如每次迭代都是近似最优的树，那么第二种的收敛速度也更快。</p>
<p>第二个原因是受到了网上一篇帖子的启发。我们首先分清出目标函数和打分函数的区别，GBDT和xgboost的目标函数相差一个正则项，对于GBDT打分函数就是(g’-T)^2，而xgboost的打分函数是结构分（往上翻）。xgboost的打分函数使用了２阶信息，还是对树的约束比GBDT要好，因此收敛要快。</p>
<p>在catboost的论文中看到，无论是用使用一阶负梯度还是使用Newton method近似，原则都是某种梯度下降。从这个角度看，使用二阶梯度下降速度果然要快一点。</p>
</li>
</ul>
<h3 id="树分裂过程"><a href="#树分裂过程" class="headerlink" title="树分裂过程"></a>树分裂过程</h3><p><img src="/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-07 12-02-48屏幕截图.png" alt></p>
<p>xgboost给出的分裂方式为：笨方式，遍历所有特征的所有点，将分裂点依次设为某特征下的数据点，然后得出使得该特征收获最大分裂增益的分裂点。</p>
<p>较为好一点点的方法是事先对某特征下的数据分组（percentile分组），然后按组比较。这样的好处是加快了搜索的效率，而且可以部分避免过拟合。但是缺点是，效果不如笨办法精确。事实上在搜索效果上，远没有那么不堪。</p>
<p>分桶策略：</p>
<p>xgboost引入的分桶策略是这样的。</p>
<p>对数据集分配一个权重。该权重就是数据点的二阶导，对于第k个特征的排序数据集:$D_k = \{(x_{1,k},h_1),(x_{2,k},h_2),…,(x_{n,k},h_n)\}$.</p>
<p>定义一个rank函数：</p>
<script type="math/tex; mode=display">
r_k(z) = \frac{1}{\sum_{(x,h)\in D_k}h} \sum_{(x,h)\in D_k,x<z} h</script><p>分裂点集合：$\{s_{k1},s_{k2},s_{k3},…s_{kl}\}$</p>
<script type="math/tex; mode=display">
|r_k(s_{k,j} - r_k(s_{k,j+1}))|<\epsilon,s_{k1} = min_ix_{i,k},s_{k,l} = max_ix_{ik}</script><p>这意味这将有$1/\epsilon$个分裂点后选集。</p>
<p>给出的解释是：</p>
<p><img src="/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-07 12-28-23屏幕截图.png" alt></p>
<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>xgboost支持对缺失值的处理，具体的处理思想就是将缺失值单独分一个类。具体的做法就是将除了缺失值以外的数据排序，将缺失值全部扔到右边或者左边，对应的是升序和降序操作。然后利用好排序的数据来计算分裂收益，自然分裂点也是在排好序的数据点中产生，最终的效果是将缺失值数据扔到了右边或者左边的叶子里面。</p>
<p>注意：这样处理其实存在一个漏洞，对于缺失值很多的特征，只用仅有的几个数据点来计算分裂收益，而忽略了大部分缺失值的贡献（缺失值没有参与到分裂过程），容易导致过拟合。catboost的做法是给缺失值赋予一个极值（9999，-9999），让缺失值也参与到分裂过程中来。这样的效果是把缺失值全部都赶到了一个叶子节点中，其余剩下的有值的数据点在一个叶子里面。这样的处理方法是将缺失值当做了单独的一个属性值，效果要稍微好点。</p>
<p><img src="/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-07 12-47-27屏幕截图.png" alt></p>
<ul>
<li><p>如何解释要从两边分别计算分割点？</p>
<p>从两边各走一遍，是为了决定把缺失值丢到哪边更合适一点。</p>
</li>
</ul>
<h3 id="防止过拟合措施"><a href="#防止过拟合措施" class="headerlink" title="防止过拟合措施"></a>防止过拟合措施</h3><p>1、shrinkage和column subsample　收缩因子和列采样。</p>
<p>​    收缩因子是指每一次迭代都给一个树系数，相当于学习率，避免学习太快。列采样（同样支持行采样）使用批量特征来避免过拟合。</p>
<p>２、剪枝策略。xgboost是先生成所有树枝，采用的是level-wise，就是每一层都并行进行分裂，生成一颗很宽的树，然后再进行自底向上剪枝。</p>
<p>３、经典的树避免拟合的参数都有。</p>
<p>４、支持不平衡的数据，通过给类别不同的权重得以实现，没有看出与其他boost方案的不同。</p>
<h3 id="其他的一些实现"><a href="#其他的一些实现" class="headerlink" title="其他的一些实现"></a>其他的一些实现</h3><p>Column Block for Parallel Learning</p>
<p>预排数据，由于需要经常排序，因此算法在训练之前就对数据进行预排，我理解的这个预排就是事先根据每个特征对数据进行排序，然后把排好序的索引存在一个block里面，然后只要涉及到排序的地方，就扫描这个block，不用重新排序。</p>
<p>Cache-aware Access</p>
<p>问题的由来：由于数据的存储不是线性的，这个主要原因是因为经常需要排序，所以读取数据的时候，磁针需要来回的挪动，导致命中率下降，这里提供了一种方案来提高磁针的命中率。</p>
<p>Blocks for Out-of-core Computation</p>
<p>当数据集超出内存大小的时候，也即是不能一次性读取进内存，设计了一种使用磁盘排序的方案。</p>
<h1 id="Lightgbm"><a href="#Lightgbm" class="headerlink" title="Lightgbm"></a>Lightgbm</h1><p>一开始以为lightgbm是照着xgboost去打的，看了论文以后发现并不是，论文里面只是强调了两种提升训练速度的方案: GOSS和EFB。</p>
<p>lightgbm和xgboost有哪些不同呢：</p>
<p><img src="/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-02 17-39-46屏幕截图.png" alt="2020-05-02 17-39-46屏幕截图"></p>
<p>这是lightgbm的作者给出的介绍。总体来说就是在速度以及存储量上均得到了提高。有两点不同需要引起注意：</p>
<ul>
<li><p>树生长策略的不同，xgboost的生长策略是level-wise而lightgbm的树生长策略是leaf-wise。level-wise是指在树分裂的时候每一层同时进行分裂，是以曾为主导。这样做的好处是方便并行化，应该是并行的通信开销比较小吧。毕竟彼此的分裂都是互不相干的。坏处是，对于那些分裂增益已经很小的叶子，在以层为主导的分裂策略下，也会跟着被分裂，最后再被剪枝，显然这样是低效率的。</p>
<p>leaf-wise的分裂方式是这样的，在每一次分裂的时候，比较当前所有叶子节点，选择能够使得分裂增益最大的那个叶子节点进行分裂。这样可以避免上述level-wise的缺点，提升效率。</p>
</li>
<li><p>histogram算法</p>
<p>这里对比的是xgboost的预排序算法。lightgbm的作者认为，由于xgboost的数据的存储方式是非线性的，即使进行了预排序，那么无论是在存储上还是在扫描效率上仍然是不尽人意的。histogram算法可以提升这方面的效率。<img src="/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-02 17-50-30屏幕截图.png" alt="2020-05-02 17-50-30屏幕截图"></p>
</li>
</ul>
<p>算法的思想大体如下：</p>
<p>１、遍历当前树的所有叶子节点。</p>
<p>２、遍历每个叶子节点的每个特征，对每个特征建立histogram.</p>
<p>3、分桶策略，通过超参数指定桶的个数，bin_max. min(bin_max,len(特征j))，当特征值的取值个数小于超参指定的值时,对每一个特征值进行分桶。分桶依据没看到，盲猜可能是xgboost的分桶标准（使用2阶导作为权重）。</p>
<p>４、把数据装桶（不需要排序），计算桶内的一阶导数和二阶导和。</p>
<p>５、以桶序号指代数据，然后遍历分桶，找到最佳的分裂点。</p>
<p>这里用到的主要思想是用histogram来代替原始数据，将原始数据的index映射到了相应的桶内，这样做的好处有几个：</p>
<p>​    - 避免了排序，这样读取数据的时候就是线性的了。</p>
<p>​    - 它将我们需要的梯度和单独保存了起来，单个hist所需要的空间用１个字节就可以了。（对比预排所需要的空间）减小了８倍</p>
<p>​    - 分裂搜索的时候用的是桶序，而不是原始数据序，因此搜索范围要小的多，速度提升。</p>
<p>​    - 分桶虽然可能会导致找到的分裂点不是最优的，但是理论和实际都表示分桶达到一定数量对性能的影响微乎其微。</p>
<ul>
<li><p>Need only <code>O(2 * #non_zero_data)</code> to construct histogram for sparse features</p>
</li>
<li><p>对于类别特征进行了优化</p>
<p>传统的对待类别特征的方法是进行one-hot编码，但是这样做对于树模型来讲有些缺点：</p>
<p>It is common to represent categorical features with one-hot encoding, but this approach is suboptimal for tree learners. Particularly for high-cardinality categorical features, a tree built on one-hot features tends to be unbalanced and needs to grow very deep to achieve good accuracy.</p>
<p>意思大致就是这个方案对于基数稍微高点的特征是不友好的，会造成数据不平衡并且会导致树越来越深。</p>
<p>lightgbm对此进行了优化，不再使用one-hot编码，而是对特征值进行排序，至于排序的标准则是(<code>sum_gradient / sum_hessian</code> ?)虽然不知道为什么要这样排，但是经过排序后的复杂度会变低（原最优分裂的时候是2^(k-1)-1,经过排序以后的复杂度变为了k*log(k)）。</p>
<p>The basic idea is to sort the categories according to the training objective at each split. More specifically, LightGBM sorts the histogram (for a categorical feature) according to its accumulated values (<code>sum_gradient / sum_hessian</code>) and then finds the best split on the sorted histogram. </p>
</li>
<li><p>再有一些优化就是并行方面的了：特征并行，数据并行，Voting Parallel。（看官方解读吧）</p>
</li>
</ul>
<h1 id="Catboost"><a href="#Catboost" class="headerlink" title="Catboost"></a>Catboost</h1><p>catboost完全指南  - 知乎（解读的不错） <a href="https://zhuanlan.zhihu.com/p/102570430" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/102570430</a>　</p>
<h3 id="对该基数类别特征的处理"><a href="#对该基数类别特征的处理" class="headerlink" title="对该基数类别特征的处理"></a>对该基数类别特征的处理</h3><p>这个写起来好长，不过当增强一遍记忆吧。</p>
<p><strong>Target statistics</strong></p>
<p>目标编码的思路很简单，就是将特征值映射为对应类别的期望。</p>
<script type="math/tex; mode=display">
\hat{x}_K^i = E(y|x^i == x_k^i)</script><p>i表示第i个特征，k表示第k个特征值。</p>
<p>特别是对那些对应样本少的特征值不友好。</p>
<p><strong>Greedy　TS</strong></p>
<script type="math/tex; mode=display">
\hat{x}_k^i = \frac{\sum_{j=1}^n \mathbf{1}\{x_j^i=x_k^i\}y_i+ap}{\sum_{j=1}^n \mathbf{1}\{x_j^i =x_k^i\}+a}</script><p>where a &gt; 0 is a parameter. A common setting for p is the average target value in the dataset. (对于二分类来说就是y先验)</p>
<p>这样编码容易发生leaky。</p>
<p><strong>Holdout TS</strong></p>
<p>思想就是将数据集进行分割，利用一部分数据集进行greedy TS编码，剩下的一部分用于训练，但这样会导致训练集不够用。</p>
<p><strong>Leave-one-out TS</strong></p>
<p>留一法编码。原文是这样说的</p>
<p>At first glance, a leave-one-out technique might work well: take $D _k = D/x_k$for training examples $x_k$  and $D_k = D$ for test ones.</p>
<p>翻译没错的话就是，利用x_k训练，其余为测试集。</p>
<script type="math/tex; mode=display">
\bar{x}_k^i = \frac{n^+-y_k+\alpha p}{n-1+\alpha}</script><p>但这样仍然有问题，对于特征$x_k^i = A$ for all samples(这里想表达的意思是对于所有的样本，该特征取值都为Ａ)，$n^+$表示y=1的样本个数，那么$t=\frac{n^+-0.5+\alpha p}{n-1+\alpha}$,可以将其二分，所以依然存在泄露。</p>
<p><strong>Ordered TS</strong></p>
<p>ordered TS的思想是这样的，对样本进行n轮shuffle，对每一轮shuffle过得数据集，对于样本$x_i$，都用前$i-1$个样本进行TS.</p>
<p> <a href="https://zhuanlan.zhihu.com/p/102570430" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/102570430</a>　介绍的不错。</p>
<p><strong>怎么样去理解这个prediction shift？</strong></p>
<p>文章谈到，训练数据的$g^t(x_k,y)|x_k$偏移了$g^t(x,y)|x$.我理解的原因是，对于训练数据点$x_k$，这个梯度$g^t(x_k,y_k)$本身在训练的时候就已经利用了数据$x_k,y_k$的信息，因为每棵树的构建都使用了全部的数据。而对于测试集而言，对于数据点$x$，它的梯度$g^t(x,y)$在构建树的过程中没有用到数据点x。因此说产生了偏斜。并且作者也说了，对于ＴＳ这个偏斜产生的原因是数据泄露，正好吻合。</p>
<p>那么怎么去解决这个问题呢，如果构建树的过程中为了防止偏斜而不使用数据点,那么为了照顾每个数据点的偏斜，就没有数据点可用了。catboost提出了一种解决方法，大概意思就是说，对于数据点$x_k,y_k$,我使用在构建过程中使用一种替代方案。不使用点$x_k,y_k$的真实值，而是使用前k-1个数据点的对$x_k,y_k$ 的预测值.</p>
<p><img src="/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-07 18-32-27屏幕截图.png" alt="2020-05-07 18-32-27屏幕截图"></p>
<p>算法中的$m_i$是support function，作用是使用前i-1个样本用来预测样本$x_ｉ$的值，对于梯度boosting就是残差。对于不同的shuffle，对应了不同的$m_{r}(i)$，不过看到它的更新方式，我很怀疑他只是存储了每一轮关于样本$x_k$的残差值。注意下面算法的Model =Ordered部分。</p>
<p><img src="/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-08 09-50-23屏幕截图.png" alt="2020-05-08 09-50-23屏幕截图"></p>
<p>算法２是catboost树构建算法，使用的是对称树（关键词：obvious tree），这种树的特点就是树的同一层的分裂使用的是相同标准。</p>
<p>Term oblivious means that the same splitting criterion is used across an entire level of the　tree。</p>
<p><strong>如何计算最终模型的叶子节点值</strong></p>
<p>显然，boosting树的最终模型的叶子节点一般为</p>
<script type="math/tex; mode=display">
\sum_{k=1}^I\sum_{j=1}^T \alpha b_j^k</script><p>$b_j^k$是叶子里面样本点的label的平均值。catboost也遵循了这样的形式，不同的是catboost在初始化的时候共shuffle了s+1次，训练的时候使用${r_1,r_2,r_3,…r_s}$，测试的时候使用$r_0$.就是说$b_{r_o,j}^k$的叶子节点是按照$r_0$序来计算的。并且对测试集类别特征编码的时候，我们使用的是全部数据集。</p>
<p><strong>类别特征的合并使用</strong></p>
<p>catboost引入了类别特征合并，而且合并方式采用greedy。特征合并就是指将特征两两合并，比方一个特征country: CN,US,UN。另一个特征music:pop,rock,rap.那么合并后就是new: CN_pop,US_rock,UN_rap.</p>
<p>greedy是指</p>
<p>Namely, for each split of atree, CatBoost combines (concatenates) all categorical features (and their combinations) already used<br>for previous splits in the current tree with all categorical features in the dataset. Combinations are　converted to TS on the fly.</p>
<p><strong>其他需要注意的地方</strong></p>
<p>使用了贝叶斯采样，避免过拟合。对于问题：对于每个shuffle靠前的几个样本，预测他们的时候由于使用的样本过少，容易产生高方差。解决方法是在计算损失的时候，倾向于丢掉前几个样本的梯度。</p>
<p>进行优化过得catboost算法以及buildtree</p>
<p><img src="/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-08 10-18-30屏幕截图.png" alt="2020-05-08 10-18-30屏幕截图"></p>
<p><img src="/2020/06/12/xgboost,lightgbm%E5%92%8Ccatboost/2020-05-08 10-18-43屏幕截图.png" alt="2020-05-08 10-18-43屏幕截图"></p>

      
    </div>
    
    
    
    
    <div>
      
        
      
    </div>

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ML/" rel="tag"><i class="fa fa-tag"></i>ML</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/06/12/%E5%86%B3%E7%AD%96%E6%A0%91/" rel="next" title="决策树">
                <i class="fa fa-chevron-left"></i> 决策树
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/06/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="prev" title="强化学习一些笔记">
                强化学习一些笔记 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">qingfengbangzuo</p>
              <p class="site-description motion-element" itemprop="description">世界上只有一种英雄主义，那就是看清生活的真相之后依然热爱生活。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives%7C%7Carchive">
              
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/qingfengbangzuo" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="cqupt_lixz@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-globe"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                推荐阅读
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://ife.baidu.com/" title="百度前端技术学院" target="_blank">百度前端技术学院</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Xgboost"><span class="nav-number">1.</span> <span class="nav-text">Xgboost</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数"><span class="nav-number">1.0.1.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#树分裂过程"><span class="nav-number">1.0.2.</span> <span class="nav-text">树分裂过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据处理"><span class="nav-number">1.0.3.</span> <span class="nav-text">数据处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#防止过拟合措施"><span class="nav-number">1.0.4.</span> <span class="nav-text">防止过拟合措施</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其他的一些实现"><span class="nav-number">1.0.5.</span> <span class="nav-text">其他的一些实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lightgbm"><span class="nav-number">2.</span> <span class="nav-text">Lightgbm</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Catboost"><span class="nav-number">3.</span> <span class="nav-text">Catboost</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#对该基数类别特征的处理"><span class="nav-number">3.0.1.</span> <span class="nav-text">对该基数类别特征的处理</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">qingfengbangzuo</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">27.2k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  





  
  



  
  



  



  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="https://github.com/jjandxa/canvas_sphere"></script>
  

  
  
    <script type="text/javascript" src="https://github.com/zproo/canvas-ribbon"></script>
  

  
  
    <script id="ribbon" type="text/javascript" size="300" alpha="0.6"  zIndex="-1" src="https://github.com/ethantw/Han"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  


  
  <script src="/live2d-widget/autoload.js"></script>
  
</body>
</html>
