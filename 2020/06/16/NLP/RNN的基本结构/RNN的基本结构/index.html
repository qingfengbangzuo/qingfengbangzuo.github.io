<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="NLP," />





  <link rel="alternate" href="/atom.xml" title="Qingfengbangzuo" type="application/atom+xml" />






<meta name="description" content="RNN的基本结构 上图是RNN的基本单元和基本结构。 可以看出RNN（Recurrent Neuron Network）是递归神经网络的一种。不过，RNN还有一种表示形式如下，这种形式和上述形式是等价的，是按时间步长展开的表示。   循环神经网络or递归神经网络引用博客：https:&#x2F;&#x2F;blog.csdn.net&#x2F;zhangjiali12011&#x2F;article&#x2F;details&#x2F;90045978 很">
<meta property="og:type" content="article">
<meta property="og:title" content="RNN的基本结构">
<meta property="og:url" content="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/index.html">
<meta property="og:site_name" content="Qingfengbangzuo">
<meta property="og:description" content="RNN的基本结构 上图是RNN的基本单元和基本结构。 可以看出RNN（Recurrent Neuron Network）是递归神经网络的一种。不过，RNN还有一种表示形式如下，这种形式和上述形式是等价的，是按时间步长展开的表示。   循环神经网络or递归神经网络引用博客：https:&#x2F;&#x2F;blog.csdn.net&#x2F;zhangjiali12011&#x2F;article&#x2F;details&#x2F;90045978 很">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%841.png">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%842.PNG">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/transducer.PNG">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/BIRNN.PNG">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/biRNN2.PNG">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/Multi-layerRNN.PNG">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/gate.PNG">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/LSTM%E7%BB%93%E6%9E%84%E5%9B%BE.PNG">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/peephole_conenction.PNG">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/variant2.PNG">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/GRU.PNG">
<meta property="article:published_time" content="2020-06-15T16:00:00.000Z">
<meta property="article:modified_time" content="2020-06-28T07:50:42.548Z">
<meta property="article:author" content="qingfengbangzuo">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%841.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":20,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN的基本结构/RNN的基本结构/"/>





  <title>RNN的基本结构 | Qingfengbangzuo</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    
    <a href="https://your-url" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Qingfengbangzuo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">私人博客</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="qingfengbangzuo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qingfengbangzuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">RNN的基本结构</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-16T00:00:00+08:00">
                2020-06-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  4.1k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  15
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="RNN的基本结构"><a href="#RNN的基本结构" class="headerlink" title="RNN的基本结构"></a>RNN的基本结构</h3><p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN的基本结构1.png" alt="RNN的基本结构"></p>
<p>上图是RNN的基本单元和基本结构。</p>
<p>可以看出RNN（Recurrent Neuron Network）是递归神经网络的一种。不过，RNN还有一种表示形式如下，这种形式和上述形式是等价的，是按时间步长展开的表示。</p>
<p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN基本结构2.PNG" alt="RNN基本结构2"></p>
<hr>
<h3 id="循环神经网络or递归神经网络"><a href="#循环神经网络or递归神经网络" class="headerlink" title="循环神经网络or递归神经网络"></a>循环神经网络or递归神经网络</h3><p>引用博客：<a href="https://blog.csdn.net/zhangjiali12011/article/details/90045978" target="_blank" rel="noopener">https://blog.csdn.net/zhangjiali12011/article/details/90045978</a></p>
<p>很明显，它俩名字就是不一样的，循环神经网络是Recurrent Neural Network，递归神经网络是Recursive Neural Network。</p>
<p>当然循环神经网络也确实可以归类到递归神经网络，从广义上说，递归神经网络分为结构递归神经网络和时间递归神经网络。从狭义上说，递归神经网络通常指结构递归神经网络，而时间递归神经网络则称为循环神经网络。</p>
<p>两者最主要的差别就在于Recurrent Neural Network是在时间维度展开，Recursive Neural Network在空间维度展开。一个很重要的改型LSTM也是基于Recurrent Neural Network改进的。</p>
<hr>
<p>图中为$x_i$代表时间步$i$的输入，即序列中第$i$个词。$s_i$代表时间步$i$的状态，$s_{i-1}$代表上一个时间步的状态，$s_0$是 一个初始状态，一般被设置为0. 状态更新函数为$s_i = R(s_{i-1},x_i|\theta)$,也称作激活函数。输出函数为$y_i$是时间步$i$的输出，这个输出不是必须的。输出函数$y_i = O(s_i|\theta)$,$\theta$是整个RNN网络的参数，包括了$\theta_R$和$\theta_O$.</p>
<p>下面对应一下各个变量和参数的维度：</p>
<script type="math/tex; mode=display">
\begin{align}&s_i = tanh(\theta_ss_{i-1}+\theta_xx_i+b_a)\\&y_i = softmax(\theta_ys_i+b_y) \\&x_i: (n_x,m)\\&s_i: (n_a,m)\\&\theta_x:(n_a,n_x)\\&\theta_s:(n_a,n_a)\\&\theta_y:(n_y,n_a)\\&b_a:(n_a,1)\\&b_y:(n_y,1)\end{align}</script><h3 id="RNN解决了什么？"><a href="#RNN解决了什么？" class="headerlink" title="RNN解决了什么？"></a>RNN解决了什么？</h3><p>回答这个问题其实挺难，因为截至到目前为止，人们依然无法很好地解释RNN为何能取得优异地表现。但是RNN的一些特性是明显的，比如说，它的每一步都继承了上一步的状态，这使得它的当前状态对过去状态产生了依赖，这个性质是符合语言特征的。其次它对位置十分敏感，不同的位置导致了不同的更新过程，这个主要体现在非线性激活函数的作用上。对比语法的特征，可以发现RNN的这个性质也是符合语言特征的，因为对于主流语言英语、汉语，词性决定了单词出现的相对位置，尽管这个位置有时候是捉摸不定的，但是其仍然具有一定的规律可循。</p>
<p>那么RNN到底解决了什么？</p>
<p>要想回答这个需要先看一下CNN解决了什么，CNN解决的问题很直接，就是CBOW严重忽略了词序，导致在进行一些分类预测任务的时候出现了问题。而CNN可以通过卷积+池化的操作使得模型对局部的顺序变得敏感。通俗一点说就是，CBOW只能捕捉到单个词的特征，而CNN可以捕捉词组的特征，这个特征包含了统计特征和语义方面的特征。CNN是不完美的，因为它更多的是对局部的序列顺序敏感，而不是整个句子。比方说“not good”不能说成是”good not”，但是至于”not good”放在什么句子的什么地方，CNN是对此是较疲软的。RNN解决了这个问题。</p>
<p>RNN是对CNN的扩展，因为RNN可以捕捉全局（整个句子）的特征，原因是它整个句子的词序都是敏感的，更因为它的每一个状态都是建立在前一个状态之上的。这就使得当前词对它之前出现过的所有词都构成了隐形的依赖。</p>
<p>看两个最基本的RNN结构。</p>
<h4 id="CBOW-RNN"><a href="#CBOW-RNN" class="headerlink" title="CBOW-RNN"></a>CBOW-RNN</h4><script type="math/tex; mode=display">
s_i = R_{CBOW}(x_i,s_{i-1}) = s_{i-1}+x_i\\y_i = O_{CBOW}(s_i) = s_i</script><p>对应一下个参数维度：</p>
<script type="math/tex; mode=display">
\begin{align}&x_i: (n_x,m)\\&s_i: (n_x,m)\\&y_i:(n_x,m)\end{align}</script><p>这是CBOW的RNN版本，可以看到这个网络对于序列顺序是不敏感的，因为顺序的变换不会导致更新过程的变换（忽略$y_i$无论怎么加结果都不变）。</p>
<h4 id="Simple-RNN-S-RNN"><a href="#Simple-RNN-S-RNN" class="headerlink" title="Simple RNN(S-RNN)"></a>Simple RNN(S-RNN)</h4><script type="math/tex; mode=display">
s_i  = R_{SRNN}(x_i,s_{i-1}) = g(\theta_ss_{i-1}+\theta_xx_i+b_a)\\y_i = O_{SRNN}(s_i) = s_i</script><p>对应一下个参数维度：</p>
<script type="math/tex; mode=display">
\begin{align}&x_i: (n_x,m)\\&s_i: (n_a,m)\\&\theta_x:(n_a,n_x)\\&\theta_s:(n_a,n_a)\\&b_a:(n_a,1)\\&y_i:(n_a,m)\end{align}</script><p>激活函数g一般使用tanh或者Relu。这个网络是可能是最简单的循环神经网络之一了，我们可以拿这个网络作为一个Baseline来学习。SRNN相较于CBOW，只多了一层非线性激活层，但就是这一层可以使得网络对于输入顺序变得极为敏感，因为不同的输入顺序会导致参数的更新过程不同。</p>
<h3 id="Acceptor、Encoder、Transducer"><a href="#Acceptor、Encoder、Transducer" class="headerlink" title="Acceptor、Encoder、Transducer"></a>Acceptor、Encoder、Transducer</h3><p>以上是RNN三种形式的使用类比。</p>
<p>对于第一种”Acceptor”，RNN只关注最后一个状态的输出，那么整个RNN结构就相当于一个训练器，它的输出结果就是最后一个时间步对应的$y_i$。倘若我们可以对每一个输出进行标签化，那么就可以将其视为一个有监督的学习过程。</p>
<p>对于第二种“Encoder”,顾名思义这种方式就是将RNN视为一种编码器，将RNN的最后一个时间步对应的输出视为编码结果，由于输出的维度是可定制的，因此，我们可以将RNN对我们的sample进行了重新编码。之后，可以将编码结果伙同其他特征一起送到更深层的网络进行训练。</p>
<p>一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">an extractive document summarization system may first run over the document with an RNN, resulting in a vector yn summarizing the entire document. Then, yn will be used together with other features in order to select the sentences to be included in the summarization.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>对于第三种”Transducer”,形式更为我们熟知，它是指在训练过程中存储每一个时间步对应的输出求损失，最后将所有的损失综合到一起反馈给网络。一个熟知的模型就是RNN可以作为语言模型，因为它的第i个时间步的输出可以被视为给定前i-1个上下文得到的概率分布。</p>
<p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/transducer.PNG" alt="transducer"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Using RNNs as transducers allows us to relax the Markov assumption</span></span><br><span class="line"><span class="string">that is traditionally taken in language models and HMM taggers, and</span></span><br><span class="line"><span class="string">condition on the entire prediction history.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>到这里可以稍微总结一些RNN的特性：</p>
<p><strong>1、对序列顺序敏感</strong></p>
<p><strong>2、训练采用后向传播</strong></p>
<p><strong>3、输出维度可定制化，输入维度不敏感</strong></p>
<p><strong>4、由于结构是循环(递归)的，因此所有的单元共用一套参数。</strong></p>
<p><strong>5、RNN的输入可以是One-hot</strong></p>
<h3 id="Bidirection-RNN（BI-RNN）"><a href="#Bidirection-RNN（BI-RNN）" class="headerlink" title="Bidirection RNN（BI-RNN）"></a>Bidirection RNN（BI-RNN）</h3><p>双向RNN解决的问题是什么？</p>
<p>在n-grma模型里面，我们为了捕捉词向量的语义上的特征，因此使用了滑动窗口，既捕捉目标词前k个单词，也捕捉目标词的后k个单词。但在RNN的基础结构中，我们可以看到模型只捕捉到了时间步i的历史依赖，对于未来可能出现的依赖是乏力的。因此，使用双向RNN可以缓解这个问题，顾名思义双向RNN就是使用两个RNN来从不同的方向对时间步i进行扫描。这样，使得得到到的时间步i的状态既依赖于过去也依赖于未来。</p>
<p>就像RNN放宽了马尔科夫假设，使得模型可以捕捉过去任意长度的词与目标词的依赖关系（马尔科夫是前k个词），biRNN放宽了窗口长度的假设，使得模型可以捕捉任意窗口大小的词与目标词的依赖关系。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Much like the RNN relaxes the Markov assumption and allows looking arbitrarily back into the past, the biRNN relaxes the fixed window size assumption, allowing to look arbitrarily far at both the past and the futurewithin the sequence.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/BIRNN.PNG" alt="BIRNN"></p>
<p>由于采用了双向的RNN扫描，因此在时间步状态的表示上会进行一些更改。$S_i = [s^f_i,s^b_i]$, $y_i = [O^f(s_i^f);O^b(s_i^b)]$.</p>
<p>定义biRNN的形式为：</p>
<script type="math/tex; mode=display">
biRNN(x_{1:n},i) = y_i = [RNN^f(x_{1:n});RNN^b(x_{n:i})]</script><p>这里可以是一个合并操作。使用sum的话，没有concat更能反映编码捕捉到的信息。</p>
<p>另一种形式的BIRNN</p>
<p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/biRNN2.PNG" alt="biRNN2"></p>
<p>biRNN被广泛应用在各种任务中，其中在sequence tagging中表现优异。</p>
<h3 id="Multi-layer-RNN"><a href="#Multi-layer-RNN" class="headerlink" title="Multi-layer RNN"></a>Multi-layer RNN</h3><p>和CNN一样，RNN也是可以堆叠的，称为深度RNN或者多层RNN。研究发现，多层RNN在效果上好于单层RNN，但是理论支撑的工作还没有完成。</p>
<p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/Multi-layerRNN.PNG" alt="Multi-layerRNN"></p>
<p><strong>开下脑洞：</strong>为甚么2层的会比一层的有效？</p>
<p>要回答这个问题需要知道RNN干了一件什么事。</p>
<script type="math/tex; mode=display">
s_i = tanh(\theta_ss_{i-1}+\theta_xx_i+b_a)\\y_i = softmax(\theta_ys_i+b_y) \\</script><p>对于非线性激活函数，我一直将其视为是一种特征抽取方式，就是将一个简单的特征变位一个比较复杂但是更好地特征，或者说对原始特征进行了重新解决，解读过后变得更容易被接受了。这可能是所有表示学习的一种特性，比如神经网络就是通过一轮又一轮地非线性变换然后得到一个比较不错的表达。那么对于RNN来讲，单层的RNN相当于对特征$(s_i,x_i)$进行了一次解读，多层的RNN相当于对特征进行了多次的解读。那么多次的解读一定比单次或者原始特征更好么，这个也不是一概而论的，比如核变换将数据映射到不同的维度观察到的分布特性也是不同的，至于映射到哪一维更好，或许是个超参数。我将这个对比用于非线性变换，因为非线性变换与核映射有异曲同工之妙。</p>
<p>如果将RNN视为对时间步特征的非线性解读，那么解读后的特征就是对时间步对应词的一个特征变换。通常来说通过这样的变化带来的收益是正的，因为原始的词特征表示没有考虑到全局依赖的问题。而变换后的特征是一种目标词和上下文(全局)的混合表示，更强更壮。多层RNN是对这个混合过程进行了更多次的抽取，按照上面的解释，结果一般总能表现地更好。</p>
<h3 id="门控结构"><a href="#门控结构" class="headerlink" title="门控结构"></a>门控结构</h3><p>S-RNN在训练时遭遇了梯度消失的问题，导致了实际应用时效果大打折扣。</p>
<p>梯度消失会导致参数更新无法传播到前面几层，就是越靠近输入端越难以被更新，或者更新速度很慢。这样还附带了一个现象，就是由于梯度传播不过来，导致RNN很难去捕捉long-range的依赖。（前面分析的是RNN有能力捕捉过去和未来的全局依赖，这里提出的门控结构更像是为了解决工程问题）</p>
<p>门控结构可以缓解这个问题。门控结构的基本思想：</p>
<p>如果将RNN视为一种有着有限大小资源计算硬件，计算单元R读进输入$x_{i+1}$,和当前状态$s_i$,然后对他们进行某种计算操作。并且将计算结果写入内存，替换掉旧的状态$s_i$。从这个角度看SRNN的问题是，内存的介入是不受控制的，也就是说在每一步计算中全部内存状态被读入，然后又被更新。通俗来讲，就是在计算时间步i的时候，我们是将先前所有的状态信息都读入进去了，那些读进去的状态信息只有一部分是真正对目标有依赖关系的，其余的相当于噪声。很自然的一个想法就是能不能将那些对目标词没有关系的状态不读入目标词时间步的计算过程，只读入那些有依赖关系的状态。如果能够只读入有依赖关系的状态，那么在反向传播的过程就意味着不需要每个状态依次更新，而只更新那些被读入的状态，这可以大大缓解梯度消失的问题。想要达到这样的目的，可以采取一种门控机制：</p>
<p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/gate.PNG" alt="gate"></p>
<p>就是在每一步读入的时候或者是说在计算时间步$s’$的时候，由一个门控单元控制读入的状态。</p>
<p>这种机制的一个特点是，假设状态变量$s_i$有$n_a$维，每次更新并不是对所有的维度进行更新，而是根据门控单元g选择一些维度进行更新。可以暂且认为这些被更新的维度是与当前状态有关的。</p>
<p>接下来的问题变为了如何设计门控单元g。很显然，门控单元是时间步的函数g = f(t)。它所时间步的不同而改变，且不应该由人来指定。典型的硬编码门控单元：$g \in \{0,1\}$无法进行梯度下降，因此可以采用一种软门控单元来既达到门控的效果，又与时间步联系起来。</p>
<p>一种方法是放宽g的取值范围，令$g\in R^n$,且对$g$施加一sigmod函数$\delta(g)$。这里的$\delta$函数有一个好处，就是当g趋于均匀分布时，它对于大部分g值都能映射为一个接近$\{0,1\}$的值，只有当g取接近于0的数时，才会被映射为0到1之间的值，因此，为了避免这种现象，在实际使用时，可以将接近于0的g值舍去，只留下那些绝对值较大的g值。这样，采用软门控，可以使得求梯度了。</p>
<p><strong>现在只留下如何构造g了。</strong></p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>LSTM的全拼是 The Long Short-Term Memory architecture.被 Hochreiter and Schmidhuber 于1997年提出。是一种门控结构的神经网络。</p>
<p>LSTM将状态向量$s_i$分为了两部分，一部分被称为“memory cell”。另一部分被称为”working memory”。”memory cell”用于存贮状态和”error gradients”.可以通过”dierentiable gating components”控制。在整个模型框架下有三个门控i,f,o，分别对应输入，隐含层的更新和输出。</p>
<script type="math/tex; mode=display">
\begin{align}
&s_i = R_{LSTM}(s_{i-1},x_j) = [c_j;h_j]\\
&c_j = f\odot c_{j-1}+i\odot z\\
&h_j = o \odot tanh(c_j)\\
&i = \delta(x_j W^{xi}+h_{j-1}W^{hi})\\
&f = \delta(x_j W^{xf}+h_{j-1}W^{hf})\\
&o = \delta(x_jW^{x0}+h_{j-1}W^{h0})\\
&z  = tanh(x_jW^{xz}+h_{j-1}W^{hz})\\
&y_i = O_{LSTM}(s_j) = h_j\\
&s_j \in R^{2·d_h},x_i\in R^{d_x},c_j,h_j,i,f,o,z\in R^{dh},W^{xo}\in R^{d_x,d_h},W^{ho}\in R^{d_h,d_h}
\end{align}</script><p>$c_j$表示”memory cell”，$h_j$表示working memory。两个合并到一块表示状态$s_j$。</p>
<p>注意这里关于g的构造，</p>
<script type="math/tex; mode=display">
g_i = x_jW^{xi}+h_{j-1}W^{hi}</script><p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/LSTM结构图.PNG" alt="LSTM结构图"></p>
<p>水平线是状态。</p>
<p>关于LSTM的变种，一种是Gers和Schmidhuber于2000年提出的增加了窥视孔连接。</p>
<p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/peephole_conenction.PNG" alt="peephole_conenction"></p>
<p>另一种变种是使用了配对遗忘门和输入门，与之前分别决定遗忘与添加信息不同，我们同事决定两只。只有当我们需要输入一些内容的时候我们才选择忘记。只有当早前信息系被忘记之后我们才会输入。</p>
<p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/variant2.PNG" alt="variant2"></p>
<h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p>GRU没有将状态分割成两部分。</p>
<script type="math/tex; mode=display">
\begin{align}
&s_j = R_{GRU}(s_{j-1},x_j) = (1-z)\odot s_{j-1}+z \odot \tilde{s_j}\\
&z = \delta(x_jW^{xz}+s_{j-1}W^{sz})\\
&r = \delta(x_jW^{xr}+s_{j-1}W^{sr})\\
&\tilde{s_j} = tanh(x_jW^{xs}+(r\odot s_{j-1})W^{sg})\\
&y_j = O_{GRU}(s_j) = s_j
\end{align}</script><p>这里只使用了两个门，一个门r用来控制介入的先前状态$s_{j-1}$并且计算$\tilde{s_j}$。状态更新使用到了先前的状态$s_{j-1}$和计算值$\tilde{s_j}$，使用门$z$来控制比例。</p>
<p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/GRU.PNG" alt="GRU"></p>

      
    </div>
    
    
    
    
    <div>
      
        
      
    </div>

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i>NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/06/15/tensorflow2.0%E4%BB%A5%E4%B8%8Acpu%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95/" rel="next" title="tensorflow2.0以上cpu版本安装方法">
                <i class="fa fa-chevron-left"></i> tensorflow2.0以上cpu版本安装方法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/06/16/NLP/Glove/GloVe/" rel="prev" title="Word GloVe">
                Word GloVe <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">qingfengbangzuo</p>
              <p class="site-description motion-element" itemprop="description">世界上只有一种英雄主义，那就是看清生活的真相之后依然热爱生活。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives%7C%7Carchive">
              
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/qingfengbangzuo" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="cqupt_lixz@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-globe"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                推荐阅读
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://ife.baidu.com/" title="百度前端技术学院" target="_blank">百度前端技术学院</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN的基本结构"><span class="nav-number">1.</span> <span class="nav-text">RNN的基本结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#循环神经网络or递归神经网络"><span class="nav-number">2.</span> <span class="nav-text">循环神经网络or递归神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN解决了什么？"><span class="nav-number">3.</span> <span class="nav-text">RNN解决了什么？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CBOW-RNN"><span class="nav-number">3.1.</span> <span class="nav-text">CBOW-RNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Simple-RNN-S-RNN"><span class="nav-number">3.2.</span> <span class="nav-text">Simple RNN(S-RNN)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Acceptor、Encoder、Transducer"><span class="nav-number">4.</span> <span class="nav-text">Acceptor、Encoder、Transducer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bidirection-RNN（BI-RNN）"><span class="nav-number">5.</span> <span class="nav-text">Bidirection RNN（BI-RNN）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-layer-RNN"><span class="nav-number">6.</span> <span class="nav-text">Multi-layer RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#门控结构"><span class="nav-number">7.</span> <span class="nav-text">门控结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM"><span class="nav-number">8.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GRU"><span class="nav-number">9.</span> <span class="nav-text">GRU</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">qingfengbangzuo</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">40.1k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  





  
  



  
  



  



  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="https://github.com/jjandxa/canvas_sphere"></script>
  

  
  
    <script type="text/javascript" src="https://github.com/zproo/canvas-ribbon"></script>
  

  
  
    <script id="ribbon" type="text/javascript" size="300" alpha="0.6"  zIndex="-1" src="https://github.com/ethantw/Han"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  


  
  <script src="/live2d-widget/autoload.js"></script>
  
</body>
</html>
