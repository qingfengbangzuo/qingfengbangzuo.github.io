<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="NLP," />





  <link rel="alternate" href="/atom.xml" title="Qingfengbangzuo" type="application/atom+xml" />






<meta name="description" content="Attention Mechanism思想：人眼观察事物时，有一个聚焦机制，即只注意和目标物相关的部分，而忽略其他和目标物弱相关的部分。这种机制可以被引进Encoder-Decoder框架。 经典的encoder-decoder模型，编码器和解码器中间的耦合靠上下文向量$C$. 前面讨论过仅依靠上下文向量$C$维持的耦合关系是很弱的。Attention机制带的一个重大改变就是，编码-解码器之间的耦">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention Mechanism">
<meta property="og:url" content="https://qingfengbangzuo.github.io/2020/06/20/NLP/Seq2Seq/Attention/index.html">
<meta property="og:site_name" content="Qingfengbangzuo">
<meta property="og:description" content="Attention Mechanism思想：人眼观察事物时，有一个聚焦机制，即只注意和目标物相关的部分，而忽略其他和目标物弱相关的部分。这种机制可以被引进Encoder-Decoder框架。 经典的encoder-decoder模型，编码器和解码器中间的耦合靠上下文向量$C$. 前面讨论过仅依靠上下文向量$C$维持的耦合关系是很弱的。Attention机制带的一个重大改变就是，编码-解码器之间的耦">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/20/NLP/Seq2Seq/Attention/Attention.PNG">
<meta property="article:published_time" content="2020-06-19T16:00:00.000Z">
<meta property="article:modified_time" content="2020-06-29T09:29:27.915Z">
<meta property="article:author" content="qingfengbangzuo">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qingfengbangzuo.github.io/2020/06/20/NLP/Seq2Seq/Attention/Attention.PNG">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":20,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://qingfengbangzuo.github.io/2020/06/20/NLP/Seq2Seq/Attention/"/>





  <title>Attention Mechanism | Qingfengbangzuo</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    
    <a href="https://your-url" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Qingfengbangzuo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">私人博客</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://qingfengbangzuo.github.io/2020/06/20/NLP/Seq2Seq/Attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="qingfengbangzuo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qingfengbangzuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Attention Mechanism</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-20T00:00:00+08:00">
                2020-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.3k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  5
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h2><p>思想：人眼观察事物时，有一个聚焦机制，即只注意和目标物相关的部分，而忽略其他和目标物弱相关的部分。这种机制可以被引进Encoder-Decoder框架。</p>
<p>经典的encoder-decoder模型，编码器和解码器中间的耦合靠上下文向量$C$. 前面讨论过仅依靠上下文向量$C$维持的耦合关系是很弱的。Attention机制带的一个重大改变就是，编码-解码器之间的耦合不再使用单向量$C$. 而是使用编码器在每一个时间步产生的上下文$c_{i:n}$. </p>
<script type="math/tex; mode=display">
c_{1:n} = ENC(x_{1:n}) = biRNN(x_{1:n})</script><p>这里的编码器使用的是双向RNN，因为要对时间步t进行编码，因此，考虑时间步t过去和未来是有必要的。</p>
<p>编码器产生上下文$c_i$的数学过程：</p>
<script type="math/tex; mode=display">
s^f_i = R(s^f_{i-1},x_t)\\
s^b_i = R(s^b_{n-i+1},x_t)\\
c_i  = O([s^f_i,s^b_i])</script><p>得到输入序列每一个时间步的上下文编码，接下来考虑解码过程，Attention带来的第二个改变是，解码过程中每一个时间步所使用的上下文向量$c_j$不再是固定的，而是依据当前状态和前面得到的$c_{1:n}$构造出来的。构造过程如下：</p>
<script type="math/tex; mode=display">
c_j = attend(c_{1:n},\hat{t}_{1:j})\\</script><p>式中$c_j$表示解码过程的第$j$个时间步所需要使用的上下文信息，这信息由$c_{1;n}$和$\hat{t}_{i:j}$（状态）构造， attend()是构造函数，是一个可训练(可以做梯度)的带参函数。下面是原版的soft attention机制：</p>
<script type="math/tex; mode=display">
c_j = \sum_{i=1}^n a^{j}_{[i]}·c_i\\</script><p>$c_i$表示的是原序列中关于时间步$i$的上下文，$a^j_{[i]}$是该时间步上下文对应的权重，也就是说，解码过程中使用的上下文是原序列对应上下文的加权和。那么这个权重是如何得到的呢？原版的soft attention机制是经过一个MLP层训练：</p>
<script type="math/tex; mode=display">
\bar{\alpha}_j = \bar{\alpha}^j_{[1]}...\bar{\alpha}^j_{[n]} = MLP([s_j;c_1]),...MLP([s_j;c_n])</script><p>式中的$s_j$是指解码过程中，时间步$j$对应的状态，$c_i$仍然是原序列对应的编码后的上下文，经过MLP层训练之后，我们可以得到一组非规范化的向量$\bar{\alpha}_j = \bar{\alpha}^j_{[1]}…\bar{\alpha}^j_{[n]}$, 然后通过softmax可以将其转化为和为1的概率分布。</p>
<script type="math/tex; mode=display">
a_j = softmax(\bar{\alpha}^j_{[1]}...\bar{\alpha}^j_{[n]})</script><p>如何解释上面获得权重的过程呢？</p>
<p>对于机器翻译来说，MLP过程可以认为是在当前解码状态$s_i$（捕捉近期生成的词）和每一个原序列对应的编码后的上下文$c_i$之间软对齐（关联度）的计算过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">In the context of machine translation, one can think of MLPatt as computing a soft alignment between the current decoder state sj (capturing the recently produced foreign words) and each of the source sentence components ci.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>完整的Attention机制的计算过程如下：</p>
<script type="math/tex; mode=display">
\begin{align}
&attend(c_{1:n},\hat{t}_{1:j}) = c_j\\
&c_j = \sum_{i=1}^n \alpha^j_{[i]}·c_i\\
&\alpha^j =softmax(\bar{\alpha}^j_{[1]},...,\bar{\alpha^j_{[n]}})\\
&\bar{\alpha}^j_{[i]} = MLP([s_j;c_i])
\end{align}</script><p>完整的基于attention的encoder-decoder模型数学表达如下：</p>
<script type="math/tex; mode=display">
\begin{align}
&p(t_{j+1}= k|\hat{t}_{1:j},x_{1:n}) = f(O_{dec}(s_{j+1})\\
&s_{j+1} = R_{dec}(s_j,[\hat{t}_j;c^j])\\
&c^j = \sum_{i=1}^n \alpha^j_{[i]}·c_i\\
&c_{1:n} = biRNN_{enc}(x_{1:n})\\
&\alpha^j =softmax(\bar{\alpha}^j_{[1]},...,\hat{\alpha}^j_{[n]})\\
&\bar{\alpha}^j_{[i]} = MLP([s_j;c_i])\\
&\hat{t}_j \approx p(t_j|\hat{t}_{1:j-1},x_{1:n})\\
&f(z) = softmax(MLP^{out}(z))\\
& MLP([s_j;c_i]) = v· tanh([s_j;c_i]U+b)
\end{align}</script><p><img src="/2020/06/20/NLP/Seq2Seq/Attention/Attention.PNG" alt="Attention"></p>
<h2 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h2><p>为什么要使用BiRNN来训练得到一个上下文$c_{1:n}$而不直接使用原文$x_{1:n}$呢？</p>
<p>其实是可以使用原文来进行attention机制的计算的，但是我们可以从$c_{1:n}$中得到更多 , 受限BiRNN训练得到的$c_i$不仅对应了序列中对应的$x_i$，它还包含了$x_i$ 的上下文。其次，通过使用一个经过训练的表示$c_i$，可以将编码和解码过程绑定在一起（训练），这样可以使得网络去学习那些对解码过程有用的编码方式，而这种信息一般不会直接体现在原文里。</p>
<p>这里说的有点绕，其实意思很简单，通过将编码和解码参数绑定在一起训练，使得编码得到的上下文$c_{1:n}$对解码过程是有用的，原始的输入向量表达的信息是有限且固定的，通过一层特征提取，可以使得训练效果更好。</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">For example, the biRNN encoder may learn to encode the position of xi within the sequence, and the decoder could use this information to access the elements in order, or learn to pay more attention to elements in the beginning of the sequence then to elements at its end.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h2><p>当不采用Attention的时候，训练的复杂度取决于编码的长度和解码的长度，（不考虑softmax的计算）$O(m+n)$。</p>
<p>当采用Attention的时候，由于针对每一个解码过程都需要训练一个上下文$c_j$，训练的复杂度为$O(m \times n)$。</p>

      
    </div>
    
    
    
    
    <div>
      
        
      
    </div>

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i>NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/06/18/NLP/Seq2Seq/Seq2Seq/" rel="next" title="Seq2Seq">
                <i class="fa fa-chevron-left"></i> Seq2Seq
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/06/20/NLP/%E4%BB%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%88%B0CNN/%E4%BB%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%88%B0CNN/" rel="prev" title="从词嵌入到CNN">
                从词嵌入到CNN <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">qingfengbangzuo</p>
              <p class="site-description motion-element" itemprop="description">世界上只有一种英雄主义，那就是看清生活的真相之后依然热爱生活。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives%7C%7Carchive">
              
                  <span class="site-state-item-count">30</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/qingfengbangzuo" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="cqupt_lixz@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-globe"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                推荐阅读
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://ife.baidu.com/" title="百度前端技术学院" target="_blank">百度前端技术学院</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention-Mechanism"><span class="nav-number">1.</span> <span class="nav-text">Attention Mechanism</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#问题："><span class="nav-number">2.</span> <span class="nav-text">问题：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#复杂度"><span class="nav-number">3.</span> <span class="nav-text">复杂度</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">qingfengbangzuo</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">59.4k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  





  
  



  
  



  



  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="https://github.com/jjandxa/canvas_sphere"></script>
  

  
  
    <script type="text/javascript" src="https://github.com/zproo/canvas-ribbon"></script>
  

  
  
    <script id="ribbon" type="text/javascript" size="300" alpha="0.6"  zIndex="-1" src="https://github.com/ethantw/Han"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  


  
  <script src="/live2d-widget/autoload.js"></script>
  
</body>
</html>
