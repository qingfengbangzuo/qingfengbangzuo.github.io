<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="ML," />





  <link rel="alternate" href="/atom.xml" title="Qingfengbangzuo" type="application/atom+xml" />






<meta name="description" content="为什么要使用神经网络，神经网络的优势？ ​    首先神经网络针对的问题一般是非线性假设，对与线性模型，要解决非线性假设的一般思路是多项式扩充，但是这样会造成特征爆炸，显然就不是很好用了。神经网络在解决非线性假设方面表现比较好。 神经网络中的梯度爆炸和梯度消失1、产生梯度爆炸和梯度消失的原因对于神经网络可以有如下的表示方式：  F(X) &#x3D; f^{l}(W^{l}f^{l-1}(W^{l-1}f^">
<meta property="og:type" content="article">
<meta property="og:title" content="一些问题">
<meta property="og:url" content="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/index.html">
<meta property="og:site_name" content="Qingfengbangzuo">
<meta property="og:description" content="为什么要使用神经网络，神经网络的优势？ ​    首先神经网络针对的问题一般是非线性假设，对与线性模型，要解决非线性假设的一般思路是多项式扩充，但是这样会造成特征爆炸，显然就不是很好用了。神经网络在解决非线性假设方面表现比较好。 神经网络中的梯度爆炸和梯度消失1、产生梯度爆炸和梯度消失的原因对于神经网络可以有如下的表示方式：  F(X) &#x3D; f^{l}(W^{l}f^{l-1}(W^{l-1}f^">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-995ba930f6f2c5dd1ca4ddeb10661666_b.jpg">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-db9d7889d408a1a13d49be058c797f33_b.jpg">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-f52ca25ffd6829ee2dfd849c256119b3_r.jpg">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-cf7fedfa18be02756aea0f57f383d897_b.jpg">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-8c3ec7fba2049e933c9fc3178fd3219d_b.jpg">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/equation.svg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/%E6%89%B9%E6%B3%A8%202020-06-02%20150911.png">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/%E6%8D%95%E8%8E%B7.PNG">
<meta property="og:image" content="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/%E6%8D%95%E8%8E%B72.PNG">
<meta property="article:published_time" content="2020-06-20T16:00:00.000Z">
<meta property="article:modified_time" content="2020-07-01T07:29:39.713Z">
<meta property="article:author" content="qingfengbangzuo">
<meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-995ba930f6f2c5dd1ca4ddeb10661666_b.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":20,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://qingfengbangzuo.github.io/2020/06/21/机器学习/神经网路/经典神经网络/一些问题/"/>





  <title>一些问题 | Qingfengbangzuo</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    
    <a href="https://your-url" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Qingfengbangzuo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">私人博客</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="qingfengbangzuo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qingfengbangzuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">一些问题</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-21T00:00:00+08:00">
                2020-06-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index">
                    <span itemprop="name">神经网络</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  4.8k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  17
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>为什么要使用神经网络，神经网络的优势？</p>
<p>​    首先神经网络针对的问题一般是非线性假设，对与线性模型，要解决非线性假设的一般思路是多项式扩充，但是这样会造成特征爆炸，显然就不是很好用了。神经网络在解决非线性假设方面表现比较好。</p>
<h3 id="神经网络中的梯度爆炸和梯度消失"><a href="#神经网络中的梯度爆炸和梯度消失" class="headerlink" title="神经网络中的梯度爆炸和梯度消失"></a>神经网络中的梯度爆炸和梯度消失</h3><h5 id="1、产生梯度爆炸和梯度消失的原因"><a href="#1、产生梯度爆炸和梯度消失的原因" class="headerlink" title="1、产生梯度爆炸和梯度消失的原因"></a>1、产生梯度爆炸和梯度消失的原因</h5><p>对于神经网络可以有如下的表示方式：</p>
<script type="math/tex; mode=display">
F(X) = f^{l}(W^{l}f^{l-1}(W^{l-1}f^{l-2}(W^{l-2}(...)+b^{l-2})+b^{l-1})+b^{l})</script><p>$f^{l}(z)$表示非线性激活函数，sigmod, tanh,Relu等</p>
<p>参数更新方式</p>
<script type="math/tex; mode=display">
w: w-\Delta w \\
\Delta w = \frac{\partial Loss}{\partial w}= \frac{\partial Loss}{\partial f^{l}}\frac{\partial f^{l}}{\partial f^{l-1}}...\frac{\partial f^{1}}{\partial w}</script><p>当$\frac{\partial f^{l}}{\partial f^{l-1}}$ &lt; 1的时候，随着层数的增加，$\Delta w$就会呈现指数级的下降，而当上式的梯度大于1的时候，随着层数的增加，$\Delta w$就会呈现指数级上升。这样的现象被称为梯度消失和梯度爆炸。<strong>表现形式为，随着层数的增加，靠近输出层的权重被更新的很快，而靠近输入层的权重几乎没有更新。</strong></p>
<h5 id="2、解决梯度爆炸和梯度消失的方法"><a href="#2、解决梯度爆炸和梯度消失的方法" class="headerlink" title="2、解决梯度爆炸和梯度消失的方法"></a>2、解决梯度爆炸和梯度消失的方法</h5><p>(1) 选择好的激活函数</p>
<p>sigmod函数的导数图如下：</p>
<p>$sigmod(x) = \frac{1}{1+e^{-x}}$</p>
<p><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-995ba930f6f2c5dd1ca4ddeb10661666_b.jpg" alt="v2-995ba930f6f2c5dd1ca4ddeb10661666_b"></p>
<p>可见它的梯度最大也不会超过0.25，因此会出现梯度消失。</p>
<p>tanh函数的导数图：</p>
<p>$tanh(x) = \frac{e^x-x^{-x}}{e^x+e^{-x}}$</p>
<p><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-db9d7889d408a1a13d49be058c797f33_b.jpg" alt="v2-db9d7889d408a1a13d49be058c797f33_b"></p>
<p>可见tanh相对sigmod要好很多，可以避免梯度爆炸，但是导数依然不大于1.</p>
<p>Relu</p>
<p>$Relu(x) = max(0,x) = (0, if \quad x<0)or (x,if \quad x>0) $</0)or></p>
<p><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-f52ca25ffd6829ee2dfd849c256119b3_r.jpg" alt="v2-f52ca25ffd6829ee2dfd849c256119b3_r"></p>
<p>relu函数的导数大于0的时候为1，这样使用relu函数就不会出现梯度爆炸和梯度消失。但是relu同样会使得某些神经元失活，因为它将某些小于输出小于0的神将元置为了0.</p>
<p>ads:</p>
<ul>
<li>解决了梯度消失和梯度爆炸</li>
<li>计算方便，计算速度快</li>
<li>加速了网络的训练</li>
</ul>
<p>dis:</p>
<ul>
<li>负数部分为0，会使得某些神经元失活。</li>
<li>输出不以0为中心。（输出为0有很多好处，比如方便做梯度下降。）</li>
</ul>
<p>leakyRelu</p>
<p>$leakyRelu(x) = (x,if \quad x&gt;0) or (x*k,if\quad x&lt;0)$，k一般取0.1或者0.2.</p>
<p><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-cf7fedfa18be02756aea0f57f383d897_b.jpg" alt="v2-cf7fedfa18be02756aea0f57f383d897_b"></p>
<p>leakyRelu继承了relu的优点，改善了其部分缺点。</p>
<p>elu:</p>
<p>$elu(x) = (x,if \quad x&gt;0)or(\alpha(e^x-1),otherwise) $<img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-8c3ec7fba2049e933c9fc3178fd3219d_b.jpg" alt="v2-8c3ec7fba2049e933c9fc3178fd3219d_b"></p>
<p>(2)正则化和梯度剪枝可以防止部分梯度爆炸</p>
<p>梯度剪枝：设置一个阈值，一旦梯度超过这个阈值，就将其限制在这个范围之内。</p>
<p>正则化：l1,l2。</p>
<p>正则项可以缓和梯度爆炸的原因是，（网上给的）一旦发生梯度爆炸，权重参数会变得特别大，因此限制权重规模可以缓解梯度爆炸。</p>
<p>另外，Ng给出的解释是，假设激活函数采用线性函数，那么只要权重矩阵稍微大于0，就会随着层数的增长使得输出值爆炸。从这个角度看，貌似梯度爆炸和权重值大小之间存在一定的关联。</p>
<script type="math/tex; mode=display">
F(X) = W^{1}W^{2}W^{3}W^{4}...W^{l}X</script><p>(3) batchnorm</p>
<p>batchnorm(batch normalization)是指对隐含层的的输出做标准化处理。（这里不清楚是对每一层都做标准化还是批量做标准化）</p>
<p>原理：</p>
<p>前项传播：</p>
<script type="math/tex; mode=display">
J = f_2(w_2f_1(w_1x+b_1)+b_2)</script><p>反向传播：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w_1} = \frac{\partial f_2(w_2f_1(w_1x+b_1)+b_2)}{\partial w_2f_1(w_1x+b_1)+b_2}\frac{\partial (w_2f_1(w_1x+b_1)+b_2)}{\partial (f_1(w_1x+b_1))}\frac{\partial f_1(w_1x+b_1)}{\partial(w_1x+b_1)}\frac{\partial (w_1x+b_1) }{\partial w_1} \\
=f'_2 ·w_2·f'_1·x</script><p>注意这里是对<script type="math/tex">w_1</script>求偏导。</p>
<p>从这里可以看出，影响梯度梯度大小的<strong>除了激活函数还有隐含层的权重值</strong>。这也就解释了前面通过正则化可以缓解梯度爆炸的原因。</p>
<p>batchnorm通过对隐含层的输出做标准化，这样使得通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到接近均值为0方差为1的标准正太分布，即严重偏离的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，使得让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</p>
<p>参考tanh函数的导数图，当数据处在0左右的时候，它的梯度近似为1。</p>
<p>batchnorm关于对$Z^{[l]}$进行归一化还是对$a^{[l]}$进行归一化在业界还存在争论，Ng说普遍认为对$Z^{[l]}$进行归一化的比较多。(z = wx+b)，也就是说在激活函数发生作用之前进行归一化操作。方法如下：</p>
<script type="math/tex; mode=display">
u = \frac{1}{m}\sum_{i=1}^{m}z_i^{[l]}\\
\delta^2 = \frac{1}{m}\sum_{i=1}^m (z^{[l]}_i - u)^2\\

z^{(i)}_{norm} = \frac{z^{(i)}-u}{\sqrt{\delta^2+\epsilon}}\\

z^{[l]}_i = \gamma z^{(i)}_{norm}+\beta</script><p>$\beta_1和\beta_2$是两个超参，m是minibatch的最小样本数，用来控制归一化的程度。这里的$z_i^{[l]}$是指每个样本经过第l层的第i个神经元得出的值。</p>
<p>方法(2)和(3)都是保证数据的导数尽可能的在导数尽可能在1附近。只不过是通过两个角度来达到这个目的，一个是调整激活函数的结构，一个调整数据结构适应激活函数。</p>
<p>为什么说BN有效？</p>
<p>”BN相当于弱化了浅层和深层网络之间的耦合，使得每一层网络都能独立训练。“</p>
<p>对于不加BN的网络来讲，浅层网络的变动（参数的变动）会使得深层网络的输入也发生大的变动，进而造成模型的不稳定。而如果加上BN以后，浅层网络依然在变动，但是无论怎么变动都被限制在了一个均值为0，方差为1的范围内，这相当于增强了模型的鲁棒性。总结一下就是，加上BN以后，后层网络对于浅层网络的变化不那么敏感了。</p>
<p>BN具有部分正则化的能力，因为是使用了minibatch计算的均值和方差（不是使用全部数据），因此会引入一点噪声，正式这些噪声起到了正则化的作用。并且，minibatch的数量越少，正则化越强。这个特性也适用于dropout。但是不要讲BN用来作为正则化的工具，可以将BN和dropout一起使用。</p>
<p>(4)初始化权重</p>
<p>不能完全解决但是相当大程度上缓解。</p>
<script type="math/tex; mode=display">
z = w_1 x_1+w_2x_2+w_3x_3+...+w_nx_n</script><p>如果n很大，那么一般我们希望$w_i$都很小。对于初始化，我们可以通过这样的方式使得$w_i$都差不多大</p>
<script type="math/tex; mode=display">
w^{[l]} = np.random.rand(shape)*np.sqrt(\frac{1}{n^{l-1}})</script><p>Ng指出，如果使用relu这样的激活函数，那么使用$np.sqrt(\frac{2}{n^{l-1}})$是比较合适的。如果是使用tanh那么使用$np.sqrt(\frac{1}{n^{l-1}})$或者$np.sqrt(\frac{2}{n^{l-1}+n^{l}})$.</p>
<p>通常这样初始化权重，会对缓解梯度爆炸有不错的效果。</p>
<p>(5)残差结构。</p>
<p><strong>残差结构</strong>说起残差的话，不得不提这篇论文了：Deep Residual Learning for Image Recognition，关于这篇论文的解读，可以参考机器学习算法全栈工程师知乎专栏文章链接：<a href="https://zhuanlan.zhihu.com/p/31852747这里只简单介绍残差如何解决梯度的问题。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31852747这里只简单介绍残差如何解决梯度的问题。</a></p>
<p>待研究。</p>
<p>(6) LSTM</p>
<p>待研究。</p>
<h3 id="神经网络中防止过拟合的方法"><a href="#神经网络中防止过拟合的方法" class="headerlink" title="神经网络中防止过拟合的方法"></a>神经网络中防止过拟合的方法</h3><p>1、正则化l1,l2. （这个和线性回归中的l1,l2解释相同，从shrinking 的角度理解）</p>
<p>2、dropout</p>
<p>dropout的思想是对于参数比较多的隐含层,($w_l : (n^l,n^{l-1})$)，可以给其设置一个较小的keep_prob，比如0.5，对于参数较多的隐含层，可以给其设置一个大的keep_prob，比如0.7或者0.8。keep_prob表示激活概率，如果随机数大于这一概率即不进行激活。</p>
<p>当前Dropout被大量利用于全连接网络，而且一般认为设置为0.5或者0.3，而在卷积网络隐藏层中由于卷积自身的稀疏化以及稀疏化的ReLu函数的大量使用等原因，Dropout策略在卷积网络隐藏层中使用较少。</p>
<p>流程：</p>
<p>（1）首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图3中虚线为部分临时被删除的神经元）</p>
<p>（2） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。</p>
<p>（3）然后继续重复这一过程：</p>
<ul>
<li>恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）</li>
<li>从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。</li>
<li>对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。</li>
</ul>
<p>不断重复这一过程</p>
<p><strong>为什么说dropout可以防止过拟合？</strong></p>
<p><strong>（1）取平均的作用：</strong> 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</p>
<p><strong>（2）减少神经元之间复杂的共适应关系：</strong> 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。</p>
<p>（3）<strong>Dropout类似于性别在生物进化中的角色：</strong>物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。</p>
<p>(4) Ng给出的一种解释为：由于每一轮都都会随机去掉一些特征，使得模型不敢给给个神经元太多的权重，因为下一次它可能就被去掉了。因此，模型会给所有神经元相对小的权重，在这一点上表现像L2。</p>
<p>那么Dropout为什么需要进行缩放呢？</p>
<p>因为我们训练的时候会随机的丢弃一些神经元，但是预测的时候就没办法随机丢弃了。如果丢弃一些神经元，这会带来结果不稳定的问题，也就是给定一个测试数据，有时候输出a有时候输出b，结果不稳定，这是实际系统不能接受的，用户可能认为模型预测不准。那么一种”补偿“的方案就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的。比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是px+(1-p)0=px。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望。</p>
<p>3、early stoping早停策略防止过拟合</p>
<p>随着迭代次数的增加，训练集损失会单调递减，而验证集则会在损失下降一段时间后上升，我们选择那个两者相差最小的迭代次数时停止迭代。但是Ng解释说这样做在一定程度上能够缓解过拟合，但是却不是最理想的缓解过拟合的方法，它的缺点是将过拟合和减小损失作为一个问题考虑了。</p>
<p>这通常不是好的。</p>
<p>在xgboost里面也有一个早停参数。这个参数是根据我们传入的指标，比如auc和验证集，使得验证集变化不大的情况下提前停止迭代和这里早停不是很一样。</p>
<p>4、增添数据集</p>
<p>图片翻转，数字变形等等。</p>
<h3 id="神经网络中训练加速方法"><a href="#神经网络中训练加速方法" class="headerlink" title="神经网络中训练加速方法"></a>神经网络中训练加速方法</h3><h5 id="1、mini-batch"><a href="#1、mini-batch" class="headerlink" title="1、mini-batch"></a>1、mini-batch</h5><p>将数据集分成m小份，每一份记为$x^{\{i\}}$</p>
<p>for i  to m:   //用完m份记为一epoch</p>
<p>（用$x^{\{i\}},y^{\{i\}}$做一步梯度下降）</p>
<p>forward prop on $x^{\{i\}}$</p>
<script type="math/tex; mode=display">
Z^{[1]} = W^{[1]}X^{\{i\}}+b^{[1]}\\
A^{[1]} = g^{[1]}(Z^{[1]})\\
...
A^{[l]} = g^{[l]}(Z^{[l]})</script><p>compute cost </p>
<script type="math/tex; mode=display">
J = \frac{1}{n(i)}\sum_{j}^{n(i)}L(\hat{y}^{(j)},y^{(j)})+\lambda/（2*n(i)） \sum_{l}||w^{[l]}||^2_2</script><p>backward prop on $x^{\{i\}}$</p>
<script type="math/tex; mode=display">
w^{[l]} = w^{[l]}-\alpha dw^{[l]}\\
b^{[l]} = b^{[l]} - \alpha db^{[l]}</script><h5 id="2、动量梯度下降和RMSprop-以及Adam"><a href="#2、动量梯度下降和RMSprop-以及Adam" class="headerlink" title="2、动量梯度下降和RMSprop,以及Adam"></a>2、动量梯度下降和RMSprop,以及Adam</h5><p>首先需要引入指数平滑的概念（非常有用）</p>
<script type="math/tex; mode=display">
v_{t+1} = \beta v_t+(1-\beta)s_{t+1}</script><p> <img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/equation.svg" alt="equation"></p>
<p>可以看出，在指数平滑法中，所有先前的观测值都对当前的平滑值产生了影响，但它们所起的作用随着参数 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 的幂的增大而逐渐减小。那些相对较早的观测值所起的作用相对较小。同时，称α为<strong>记忆衰减因子</strong>可能更合适——因为α的值越大，模型对历史数据“遗忘”的就越快。从某种程度来说，指数平滑法就像是拥有无限记忆（平滑窗口足够大）且权值呈指数级递减的移动平均法。一次指数平滑所得的计算结果可以在数据集及范围之外进行扩展，因此也就可以用来进行预测。</p>
<p>对于$\beta$值得理解，给定$\beta$值，一般相当于过去$\frac{1}{1-\beta}$个样本进行平均。</p>
<p>什么意思呢，就是给定一堆样本（2维），样本的分布可能近似服从某一分布，通过指数平滑后，<img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/批注 2020-06-02 150911.png" alt="批注 2020-06-02 150911">样本在垂直方向上的波动会变弱。</p>
<p>指数加权平均的参数修正。</p>
<p>问题的提出，对于</p>
<script type="math/tex; mode=display">
v_{t+1} = \beta v_t+(1-\beta)s_{t+1}</script><p>初始化$v_0$为0，那么对于第一天</p>
<script type="math/tex; mode=display">
v_1 = 0.98*v_0+0.02\theta_1\\
v_2 = 0.98*v_1 + 0.02\theta_2</script><p>由于$v_0$等于0，假设$\theta$等于40，那么对于第一天以及第二天的平均都是严重脱离真实值的。因此要引入参数修正。</p>
<p>解决 的方法为：</p>
<p>u(t) = $\frac{v_t}{1-\beta^t}=\frac{\beta v_{t-1}+(1-\beta)\theta_{t}}{1-\beta^t}$</p>
<h3 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h3><p>鞍点是指在高维空间中的某方向梯度为0的点，但不是局部最优点。</p>
<p>因为形状酷似马鞍，因此得名。鞍点才是高维空间中常见的局部最优点。<img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/捕获.PNG" alt="捕获"></p>
<p>关于为什么在高维空间中更容易碰到一个鞍点而非左图那样的局部最优点，Ng给出的解释为，在一个高维空间中一个最优点要满足各个方向上都是凸的，这其实很难，更常见的是在某一个纬度或者某几个纬度共同作用下的方向上是凸的，而在其他方向是凹的，也就是鞍点。</p>
<p>鞍点是降低学习速率的重要原因。实际上是鞍点所有的停滞区。停滞区是指导数长时间接近于零的一段区域。由于导数接近于零，因此平面呈现出一个很平的面，算法会在这个平面上缓慢的挪动去找到那个最低点，然后才能离开这个平面。<img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/捕获2.PNG" alt="捕获2"></p>
<p>也就是说，坡度过缓导致算法收敛速度变慢了。</p>
<p>Ng的观点是：当我们训练一个大型的网络时，只要将初始损失函数定义在一个较高的点上，其实是不太可能进入局部最优。而真正影响收敛速度的是鞍点的存在。</p>
<p>解决的方法就是使用像动量梯度下降或者Adam这样的加速算法。加速离开停滞区的速度。</p>
<h3 id="神经网络调参原则"><a href="#神经网络调参原则" class="headerlink" title="神经网络调参原则"></a>神经网络调参原则</h3><p>1、固定超参范围，使用随机搜索。</p>
<p>2、在随机搜索的基础上固定一个更小的范围进行gridsearch。</p>
<p>3、参数的调试优先顺序，Ng说是学习率最高，其次是像Adam或者动量梯度下降的参数 和网络隐含层神经元的个数，再然后是网络的层数等等。（仅限于指导意义）</p>
<h3 id="正交化"><a href="#正交化" class="headerlink" title="正交化"></a>正交化</h3><p>主要思想：一个参数只保证一个功能。这样对于调参比较方便。</p>
<p>例子：电视机、汽车</p>

      
    </div>
    
    
    
    
    <div>
      
        
      
    </div>

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ML/" rel="tag"><i class="fa fa-tag"></i>ML</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%90%86%E8%A7%A3/" rel="next" title="对神经网络模型的一个小理解">
                <i class="fa fa-chevron-left"></i> 对神经网络模型的一个小理解
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E6%AD%A3%E5%88%99%E5%8C%96/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E6%AD%A3%E5%88%99%E5%8C%96/" rel="prev" title="如何理解正则化">
                如何理解正则化 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">qingfengbangzuo</p>
              <p class="site-description motion-element" itemprop="description">世界上只有一种英雄主义，那就是看清生活的真相之后依然热爱生活。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives%7C%7Carchive">
              
                  <span class="site-state-item-count">28</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/qingfengbangzuo" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="cqupt_lixz@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-globe"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                推荐阅读
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://ife.baidu.com/" title="百度前端技术学院" target="_blank">百度前端技术学院</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络中的梯度爆炸和梯度消失"><span class="nav-number">1.</span> <span class="nav-text">神经网络中的梯度爆炸和梯度消失</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1、产生梯度爆炸和梯度消失的原因"><span class="nav-number">1.0.1.</span> <span class="nav-text">1、产生梯度爆炸和梯度消失的原因</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2、解决梯度爆炸和梯度消失的方法"><span class="nav-number">1.0.2.</span> <span class="nav-text">2、解决梯度爆炸和梯度消失的方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络中防止过拟合的方法"><span class="nav-number">2.</span> <span class="nav-text">神经网络中防止过拟合的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络中训练加速方法"><span class="nav-number">3.</span> <span class="nav-text">神经网络中训练加速方法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1、mini-batch"><span class="nav-number">3.0.1.</span> <span class="nav-text">1、mini-batch</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2、动量梯度下降和RMSprop-以及Adam"><span class="nav-number">3.0.2.</span> <span class="nav-text">2、动量梯度下降和RMSprop,以及Adam</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#鞍点"><span class="nav-number">4.</span> <span class="nav-text">鞍点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络调参原则"><span class="nav-number">5.</span> <span class="nav-text">神经网络调参原则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正交化"><span class="nav-number">6.</span> <span class="nav-text">正交化</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">qingfengbangzuo</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">55.6k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  





  
  



  
  



  



  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="https://github.com/jjandxa/canvas_sphere"></script>
  

  
  
    <script type="text/javascript" src="https://github.com/zproo/canvas-ribbon"></script>
  

  
  
    <script id="ribbon" type="text/javascript" size="300" alpha="0.6"  zIndex="-1" src="https://github.com/ethantw/Han"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  


  
  <script src="/live2d-widget/autoload.js"></script>
  
</body>
</html>
