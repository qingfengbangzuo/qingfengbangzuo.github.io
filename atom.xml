<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Qingfengbangzuo</title>
  
  <subtitle>私人博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://qingfengbangzuo.github.io/"/>
  <updated>2020-07-03T02:59:33.090Z</updated>
  <id>https://qingfengbangzuo.github.io/</id>
  
  <author>
    <name>qingfengbangzuo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>高斯过程2</title>
    <link href="https://qingfengbangzuo.github.io/2020/07/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B2/"/>
    <id>https://qingfengbangzuo.github.io/2020/07/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B2/</id>
    <published>2020-07-03T02:58:40.089Z</published>
    <updated>2020-07-03T02:59:33.090Z</updated>
    
    <content type="html"><![CDATA[<h2 id="再谈高斯过程"><a href="#再谈高斯过程" class="headerlink" title="再谈高斯过程"></a>再谈高斯过程</h2><p>经过这几天的学习，对高斯过程和核函数又有了一定程度的理解，特此记录。</p><p>核函数的定义</p><script type="math/tex; mode=display">k(x,x') = \phi(x)^T\phi(x')</script><p>这其实是一种相似度计算方式的定义。当采用高斯核的时候：</p><script type="math/tex; mode=display">k(x,x') = exp(-||x-x'||^2/2\delta^2)</script><p>这种相似度计算方法就更加直观了。</p><p>再来看高斯过程：</p><p>一般地，假设存在任意线性函数</p><script type="math/tex; mode=display">y = w^T\phi(x)\\</script><p>假设</p><script type="math/tex; mode=display">p(w) = N(w|0,\alpha^{-1} \mathbf{I})</script><p>那么$$</p><script type="math/tex; mode=display">E[y] = \phi(x)E[w] = 0\\cov[y] =\alpha^{-1}· \phi(x)^2</script><p>假设输入空间是空间轴上的连续动作，每一个样本点代表一个空间点（按大小排序），且彼此之间相互独立（这个其实不应该由样本点x来保证，而是由w来保证，因为我们对x没有任何假设，但是对w我们假设了先验分布为高斯，但是无论怎么说都至少存在一个假设，每个空间节点的y的分布是相互独立的），那么整个样本就可以视为是一个高斯过程。</p><script type="math/tex; mode=display">\mathbf{y} = \mathbf{\Phi w }</script><p>$\mathbf{y} = [y(x_1),y(x_2),y(x_3),…y(x_m)]$，因此该高斯过程服从一个多元高斯分布，这个元可以是任意的，甚至是无限的。但是这样要弄清楚，高斯过程和多元高斯分布本质上是不同的概念，一般来说多元高斯分布的变量之间没有先后顺序的概念（每个都是相互独立的），而高斯过程不一样，它是在时间轴或者空间轴上的序列过程。这里的$y$是 一元高斯分布的高斯过程，也可以存在多元高斯分布的高斯过程，这里说的不恰当，但是方便理解。由于是一元高斯过程，那么只要解决了高斯过程的序列表示问题，就可以将其视为一个多元高斯分布了。（似乎这里使用联合分布来解释更恰当）</p><script type="math/tex; mode=display">E[\mathbf{y}] = \Phi E[\mathbf{w}] = 0\\cov[y] = E[\mathbf{yy^T}] = \mathbf{\Phi E[ww^T]\Phi^T} = \mathbf{\alpha^{-1}\Phi \Phi^T = K}</script><p>上面的高斯过程的序列表示问题对于函数来讲，是这样的一个问题，就是相邻的x值应该对应一个差不多的y（注意x是连续的），反映在分布中就是相邻时间步的协方差应该相对较大。那么上述已经给出了协方差矩阵的表示方式，就是格拉姆矩阵$K$,其中的元素$k(x,x’)$用来衡量不同时间步之间的协方差。这里就回应了一开说到的核函数是衡量相似度的一种计算方式。</p><p>上述表示的最大好处就是，核函数的方式不是固定的，只要是合理的核函数，上式都可以成立。</p><p>但是也应当注意到，选择不同的核函数对应的$\mathbf{y}$的分布式不一样的，这要看你所要拟合的函数的性质。</p><h2 id="高斯过程用来拟合函数"><a href="#高斯过程用来拟合函数" class="headerlink" title="高斯过程用来拟合函数"></a>高斯过程用来拟合函数</h2><p>对于高斯过程来讲，由于每个点都是一个高斯分布，因此任意k个节点的分布就是联合高斯分布。</p><p>联合高斯分布的采样过程可以拟合任意一条函数曲线（线性和非线性）。这里的一个主要原因是高斯分布的分布范围是整个实数空间。也就是说高斯过程中每个点的取值范围都是真个实数空间，那么对于任何一条存在于实数空间中的函数，高斯过程（联合高斯分布）都有能力近似，形状由超参控制（$\alpha$）。</p><p>10个采样点：</p><p><img src="/2020/07/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B2/10节点.jpg" alt="10节点"></p><p>100个采样点：</p><p><img src="/2020/07/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B2/100节点.jpg" alt="100节点"></p><h2 id="高斯过程用作预测"><a href="#高斯过程用作预测" class="headerlink" title="高斯过程用作预测"></a>高斯过程用作预测</h2><p><img src="/2020/07/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B2/高斯过程用于预测.PNG" alt="高斯过程用于预测"></p><p>摘自： <a href="https://zhuanlan.zhihu.com/p/73832253" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/73832253</a></p><h2 id="回顾高斯过程"><a href="#回顾高斯过程" class="headerlink" title="回顾高斯过程"></a>回顾高斯过程</h2><p>高斯过程是一种非参模型，直观地看，高斯过程是使用样本数据来预测非样本数据，在预测过程中使用到了所有样本。高斯过程同样是生成模型，因为它是通过对样本数据建模，从而学习到了整个样本数据的分布规律，然后作为先验知识去预测未知样本。因此它具有生成模型的一系列缺点：</p><p>1、模型复杂</p><p>2、计算效率比较低（主要是核函数，易受维度的限制）</p><p>3、不稀疏，使用了所有样本/特征信息来进行预测</p><p>优点：</p><p>自适应性，对于在线学习，样本的数量是随时间变化的，因此自适应学习非常重要，而高斯过程由于是通过所有样本来预测新数据，因此它是一种在线学习方式。</p><p>总体来说，高斯过程虽然理论上很完美，但是在却对大数据不是很适用。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;再谈高斯过程&quot;&gt;&lt;a href=&quot;#再谈高斯过程&quot; class=&quot;headerlink&quot; title=&quot;再谈高斯过程&quot;&gt;&lt;/a&gt;再谈高斯过程&lt;/h2&gt;&lt;p&gt;经过这几天的学习，对高斯过程和核函数又有了一定程度的理解，特此记录。&lt;/p&gt;
&lt;p&gt;核函数的定义&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/"/>
    
      <category term="概率统计模型" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="ML" scheme="https://qingfengbangzuo.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>kd树</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN/kd%E6%A0%91/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN/kd%E6%A0%91/</id>
    <published>2020-06-20T16:00:00.000Z</published>
    <updated>2020-07-01T07:16:11.993Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/v_july_v/article/details/8203674" target="_blank" rel="noopener">https://blog.csdn.net/v_july_v/article/details/8203674</a></p><p>kd树是一颗二叉平衡树：</p><p><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN/kd%E6%A0%91/kdtree.jpg" alt="kdtree"></p><p>它的思想其实挺简单，N个数据点散布在k维空间，kd树首先找到数据方差最大的维度，这一步叫做找到split域。然后在根据该维度上的值对数据进行排序，并且确定中值。这一步叫做确定node-data域。</p><p>根据上面的一次划分，将数据空间分为了两部分，每一部分都是一个子空间。然后在子空间上递归的进行上述划分，知道子空间中只包含一个数据点。</p><p><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN/kd%E6%A0%91/数据结构.jpg" alt="数据结构"><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN/kd%E6%A0%91/alg.jpg" alt="alg"><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/KNN/kd%E6%A0%91/流程图.jpg" alt="流程图"></p><p>kd树搜索算法：</p><p>输入：以构造的kd树，目标点x；<br>输出：x 的最近邻<br>算法步骤如下：</p><p>在kd树种找出包含目标点x的叶结点：从根结点出发，递归地向下搜索kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点，直到子结点为叶结点为止。<br>以此叶结点为“当前最近点”。<br>递归的向上回溯，在每个结点进行以下操作：<br>（a）如果该结点保存的实例点比当前最近点距离目标点更近，则更新“当前最近点”，也就是说以该实例点为“当前最近点”。<br>（b）当前最近点一定存在于该结点一个子结点对应的区域，检查子结点的父结点的另一子结点对应的区域是否有更近的点。具体做法是，检查另一子结点对应的区域是否以目标点位球心，以目标点与“当前最近点”间的距离为半径的圆或超球体相交：<br>如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点，接着，继续递归地进行最近邻搜索；<br>如果不相交，向上回溯。<br>当回退到根结点时，搜索结束，最后的“当前最近点”即为x 的最近邻点。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/v_july_v/article/details/8203674&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/v_july_v/article/de
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/"/>
    
      <category term="KNN" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/KNN/"/>
    
    
      <category term="ML" scheme="https://qingfengbangzuo.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>如何理解正则化</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E6%AD%A3%E5%88%99%E5%8C%96/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E6%AD%A3%E5%88%99%E5%8C%96/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E6%AD%A3%E5%88%99%E5%8C%96/</id>
    <published>2020-06-20T16:00:00.000Z</published>
    <updated>2020-07-01T07:13:37.604Z</updated>
    
    <content type="html"><![CDATA[<p><strong>正则化可以抑制模型的复杂度，从而达到抑制过拟合的目的。</strong></p><p>常见的正则化方式就两种，以线性回归为例：</p><p>L2:</p><script type="math/tex; mode=display">J = \frac{1}{N}\sum_{i=1}^N(y^{(i)}-wx^{(i)})^2 + \lambda\sum_{j=1}^J|w|^2</script><p>L1:</p><script type="math/tex; mode=display">J = \frac{1}{N}\sum_{i=1}^N(y^{(i)}-wx^{(i)})^2 + \gamma\sum_{j=1}^J|w|</script><h3 id="第一种理解："><a href="#第一种理解：" class="headerlink" title="第一种理解："></a>第一种理解：</h3><p>假设$J= 2$，那么正则项的分布如下所示：</p><p><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E6%AD%A3%E5%88%99%E5%8C%96/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E6%AD%A3%E5%88%99%E5%8C%96/L1_L2.PNG" alt="L1_L2"></p><p>右上方为损失函数的变化，左下方为正则化项的分布，两者的交点为参数的解，可以看到，L1正则化项的参数比L2正则化项参数有更大的概率被置为0.因此使用L1的参数分布相较L2具有更强的稀疏性。</p><h3 id="第二种解释："><a href="#第二种解释：" class="headerlink" title="第二种解释："></a>第二种解释：</h3><p>从概率分布的角度解释，L1正则化和L2正则化分别对应了拉普拉斯分布和高斯分布。</p><p><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E6%AD%A3%E5%88%99%E5%8C%96/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E6%AD%A3%E5%88%99%E5%8C%96/拉普拉斯分布.png" alt="拉普拉斯分布"></p><p>​                                                                拉普拉斯分布</p><p>拉普拉斯分布：</p><script type="math/tex; mode=display">f(x|u,b) = \frac{1}{2b}exp(-\frac{|x-u|}{b})</script><p>高斯分布：</p><script type="math/tex; mode=display">f(x|u,\delta) = \frac{1}{\sqrt{2\pi}\delta}exp(-\frac{|x-u|^2}{2\delta^2})</script><p>若将均方误差看做是最大似然概率的log版本，那么对两个分布取Log变换得到：</p><p>$log f_1 =logf(x|u,b) = log\frac{1}{2b}-\frac{|x-u|}{b}$</p><p>$log f_2 =logf(x|u,b) = log\frac{1}{\sqrt{2\pi}\delta}-\frac{|x-u|^2}{2\delta^2}$</p><p>可见L1为u=0,b = 1的拉普拉斯分布的对数版本。L2为u=0,$\delta^2 = 1$的高斯分布的 对数版本（忽略常数）。那么使用正则化的目标函数中就相当于分别加入了参数服从拉普拉斯分布和高斯分布的先验假设。</p><p>至于为什么L1比L2更容易获得稀疏解，是因为拉普拉斯分布的图像中得到非零值得概率比高斯分布要小的多。</p><h3 id="第三种解释"><a href="#第三种解释" class="headerlink" title="第三种解释"></a>第三种解释</h3><p>对目标函数进行梯度下降，可以得到附加正则化的梯度更新公式：</p><p>L2:</p><script type="math/tex; mode=display">w_{k+1} = (1-\alpha \lambda)w_k-\eta y^{(i)}x^{(i)}</script><p>L1:</p><script type="math/tex; mode=display">w_{k+1} = w_k - \eta y^{(i)}x^{(i)}-\eta\gamma</script><p>观察上式有这样一个规律，相比不加正则化的时候的参数$w’_{k+1}$，加了正则化的效果是$w’_{k+1}$被按比例$\alpha \lambda$稀释了，而对于加了L1正则化，$w’_{k+1}$被按固定值稀释了。对于大的$w’_{k+1}$L2对参数的稀释程度要高于L1，对于小的的$w’_{k+1}$，L1对于参数的稀释程度要高于L2.</p><p>另外，当$w_k$大的时候，附加了$L2$的梯度下降的更快，因为是成比例下降的，当$w_k$较小的时候，附加了$L1$的梯度下降更快，因为是按照一个固定值下降的，那么正因为是这样，$L1$在$w_k$较小时，对$w_k$的抑制更强烈，也更稀疏。</p><p><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E6%AD%A3%E5%88%99%E5%8C%96/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E6%AD%A3%E5%88%99%E5%8C%96/L1.PNG" alt="L1" style="zoom:50%;"><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E6%AD%A3%E5%88%99%E5%8C%96/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E6%AD%A3%E5%88%99%E5%8C%96/L2.PNG" alt="L2" style="zoom:50%;"></p><p>​                                L1                                                                    L2</p><h3 id="第四种解释"><a href="#第四种解释" class="headerlink" title="第四种解释"></a>第四种解释</h3><p>从参数分布的角度来探讨一下：</p><script type="math/tex; mode=display">t = y(w,x)+\epsilon</script><p>$t$是标签，$y(w,x) = w^Tx$是预测函数，$\epsilon$是误差且服从$N(0,\beta^2)$的高斯分布。</p><script type="math/tex; mode=display">P(t = t_i|w,x_i,\beta)  =  N(w^Tx_i,\beta^2)</script><p>$P(t|W,X,\beta) $是关于标签$t$的条件分布.它是一服从均值为$W^TX$，方差为$\beta^2$的高斯分布。</p><p>似然函数$P(\mathbf{t}|w,x,\beta)$ :  $\mathbf{t} = [t_1,t_2,…,t_n]^T$</p><script type="math/tex; mode=display">P(\mathbf{t}|w,x,\beta) = \prod_{i=1}^NN(w^Tx_i,\beta^2)</script><p>现在对参数$w$建模：</p><p>假设$w$先验分布服从均值为0，方差为1的高斯分布$p(w) =  N(0,\alpha^{-1}I）$。那么后验分布有：</p><script type="math/tex; mode=display">p(w|\mathbf{t})  = \frac{p(\mathbf{t}|w)p(w)}{p(\mathbf{t})}</script><p>忽略$p(\mathbf{t})$（视为一已知条件，不影响对参数后验分布的估计），$p(\mathbf{t}|w)$和$p(w)$都是高斯分布，那么两各高斯相乘，$p(w|\mathbf{t})$也是高斯分布。根据上面的$p(\mathbf{t}|w)$和$p(w)$的式子，可以采用最大后验估计得到对数版本的估计似然公式：</p><script type="math/tex; mode=display">\mathbf{ln} p(w|\mathbf{t}) = -\frac{\beta}{2}\sum_{i=1}^N(t_i-w^Tx_i)^2-\frac{\alpha}{2}w^Tw+const</script><p>从这里可见，当参数采用高斯假设的时候，式子中出现了正则项$w^Tw$.而参数$\alpha$正好可以作为正则化控制参数，抑制参数的散布程度。</p><p>拉普拉斯分布也可通过这样的方式推导出来，因为无论是拉普拉斯分布还是高斯分布，他们都属于指数分布簇，拥有通用的表达方式。</p><script type="math/tex; mode=display">p(w|\alpha) = [\frac{q}{2}(\frac{\alpha}{2})^{1/q}\frac{1}{\Gamma(1/q)}]^Mexp(-\frac{\alpha}{2}\sum_{j=1}^M|w_j|^q)</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;正则化可以抑制模型的复杂度，从而达到抑制过拟合的目的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;常见的正则化方式就两种，以线性回归为例：&lt;/p&gt;
&lt;p&gt;L2:&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
J = \frac{
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/"/>
    
      <category term="线性模型" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="ML" scheme="https://qingfengbangzuo.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>一些问题</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/</id>
    <published>2020-06-20T16:00:00.000Z</published>
    <updated>2020-07-01T07:29:39.713Z</updated>
    
    <content type="html"><![CDATA[<p>为什么要使用神经网络，神经网络的优势？</p><p>​    首先神经网络针对的问题一般是非线性假设，对与线性模型，要解决非线性假设的一般思路是多项式扩充，但是这样会造成特征爆炸，显然就不是很好用了。神经网络在解决非线性假设方面表现比较好。</p><h3 id="神经网络中的梯度爆炸和梯度消失"><a href="#神经网络中的梯度爆炸和梯度消失" class="headerlink" title="神经网络中的梯度爆炸和梯度消失"></a>神经网络中的梯度爆炸和梯度消失</h3><h5 id="1、产生梯度爆炸和梯度消失的原因"><a href="#1、产生梯度爆炸和梯度消失的原因" class="headerlink" title="1、产生梯度爆炸和梯度消失的原因"></a>1、产生梯度爆炸和梯度消失的原因</h5><p>对于神经网络可以有如下的表示方式：</p><script type="math/tex; mode=display">F(X) = f^{l}(W^{l}f^{l-1}(W^{l-1}f^{l-2}(W^{l-2}(...)+b^{l-2})+b^{l-1})+b^{l})</script><p>$f^{l}(z)$表示非线性激活函数，sigmod, tanh,Relu等</p><p>参数更新方式</p><script type="math/tex; mode=display">w: w-\Delta w \\\Delta w = \frac{\partial Loss}{\partial w}= \frac{\partial Loss}{\partial f^{l}}\frac{\partial f^{l}}{\partial f^{l-1}}...\frac{\partial f^{1}}{\partial w}</script><p>当$\frac{\partial f^{l}}{\partial f^{l-1}}$ &lt; 1的时候，随着层数的增加，$\Delta w$就会呈现指数级的下降，而当上式的梯度大于1的时候，随着层数的增加，$\Delta w$就会呈现指数级上升。这样的现象被称为梯度消失和梯度爆炸。<strong>表现形式为，随着层数的增加，靠近输出层的权重被更新的很快，而靠近输入层的权重几乎没有更新。</strong></p><h5 id="2、解决梯度爆炸和梯度消失的方法"><a href="#2、解决梯度爆炸和梯度消失的方法" class="headerlink" title="2、解决梯度爆炸和梯度消失的方法"></a>2、解决梯度爆炸和梯度消失的方法</h5><p>(1) 选择好的激活函数</p><p>sigmod函数的导数图如下：</p><p>$sigmod(x) = \frac{1}{1+e^{-x}}$</p><p><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-995ba930f6f2c5dd1ca4ddeb10661666_b.jpg" alt="v2-995ba930f6f2c5dd1ca4ddeb10661666_b"></p><p>可见它的梯度最大也不会超过0.25，因此会出现梯度消失。</p><p>tanh函数的导数图：</p><p>$tanh(x) = \frac{e^x-x^{-x}}{e^x+e^{-x}}$</p><p><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-db9d7889d408a1a13d49be058c797f33_b.jpg" alt="v2-db9d7889d408a1a13d49be058c797f33_b"></p><p>可见tanh相对sigmod要好很多，可以避免梯度爆炸，但是导数依然不大于1.</p><p>Relu</p><p>$Relu(x) = max(0,x) = (0, if \quad x<0)or (x,if \quad x>0) $</0)or></p><p><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-f52ca25ffd6829ee2dfd849c256119b3_r.jpg" alt="v2-f52ca25ffd6829ee2dfd849c256119b3_r"></p><p>relu函数的导数大于0的时候为1，这样使用relu函数就不会出现梯度爆炸和梯度消失。但是relu同样会使得某些神经元失活，因为它将某些小于输出小于0的神将元置为了0.</p><p>ads:</p><ul><li>解决了梯度消失和梯度爆炸</li><li>计算方便，计算速度快</li><li>加速了网络的训练</li></ul><p>dis:</p><ul><li>负数部分为0，会使得某些神经元失活。</li><li>输出不以0为中心。（输出为0有很多好处，比如方便做梯度下降。）</li></ul><p>leakyRelu</p><p>$leakyRelu(x) = (x,if \quad x&gt;0) or (x*k,if\quad x&lt;0)$，k一般取0.1或者0.2.</p><p><img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-cf7fedfa18be02756aea0f57f383d897_b.jpg" alt="v2-cf7fedfa18be02756aea0f57f383d897_b"></p><p>leakyRelu继承了relu的优点，改善了其部分缺点。</p><p>elu:</p><p>$elu(x) = (x,if \quad x&gt;0)or(\alpha(e^x-1),otherwise) $<img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/v2-8c3ec7fba2049e933c9fc3178fd3219d_b.jpg" alt="v2-8c3ec7fba2049e933c9fc3178fd3219d_b"></p><p>(2)正则化和梯度剪枝可以防止部分梯度爆炸</p><p>梯度剪枝：设置一个阈值，一旦梯度超过这个阈值，就将其限制在这个范围之内。</p><p>正则化：l1,l2。</p><p>正则项可以缓和梯度爆炸的原因是，（网上给的）一旦发生梯度爆炸，权重参数会变得特别大，因此限制权重规模可以缓解梯度爆炸。</p><p>另外，Ng给出的解释是，假设激活函数采用线性函数，那么只要权重矩阵稍微大于0，就会随着层数的增长使得输出值爆炸。从这个角度看，貌似梯度爆炸和权重值大小之间存在一定的关联。</p><script type="math/tex; mode=display">F(X) = W^{1}W^{2}W^{3}W^{4}...W^{l}X</script><p>(3) batchnorm</p><p>batchnorm(batch normalization)是指对隐含层的的输出做标准化处理。（这里不清楚是对每一层都做标准化还是批量做标准化）</p><p>原理：</p><p>前项传播：</p><script type="math/tex; mode=display">J = f_2(w_2f_1(w_1x+b_1)+b_2)</script><p>反向传播：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial w_1} = \frac{\partial f_2(w_2f_1(w_1x+b_1)+b_2)}{\partial w_2f_1(w_1x+b_1)+b_2}\frac{\partial (w_2f_1(w_1x+b_1)+b_2)}{\partial (f_1(w_1x+b_1))}\frac{\partial f_1(w_1x+b_1)}{\partial(w_1x+b_1)}\frac{\partial (w_1x+b_1) }{\partial w_1} \\=f'_2 ·w_2·f'_1·x</script><p>注意这里是对<script type="math/tex">w_1</script>求偏导。</p><p>从这里可以看出，影响梯度梯度大小的<strong>除了激活函数还有隐含层的权重值</strong>。这也就解释了前面通过正则化可以缓解梯度爆炸的原因。</p><p>batchnorm通过对隐含层的输出做标准化，这样使得通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到接近均值为0方差为1的标准正太分布，即严重偏离的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，使得让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</p><p>参考tanh函数的导数图，当数据处在0左右的时候，它的梯度近似为1。</p><p>batchnorm关于对$Z^{[l]}$进行归一化还是对$a^{[l]}$进行归一化在业界还存在争论，Ng说普遍认为对$Z^{[l]}$进行归一化的比较多。(z = wx+b)，也就是说在激活函数发生作用之前进行归一化操作。方法如下：</p><script type="math/tex; mode=display">u = \frac{1}{m}\sum_{i=1}^{m}z_i^{[l]}\\\delta^2 = \frac{1}{m}\sum_{i=1}^m (z^{[l]}_i - u)^2\\z^{(i)}_{norm} = \frac{z^{(i)}-u}{\sqrt{\delta^2+\epsilon}}\\z^{[l]}_i = \gamma z^{(i)}_{norm}+\beta</script><p>$\beta_1和\beta_2$是两个超参，m是minibatch的最小样本数，用来控制归一化的程度。这里的$z_i^{[l]}$是指每个样本经过第l层的第i个神经元得出的值。</p><p>方法(2)和(3)都是保证数据的导数尽可能的在导数尽可能在1附近。只不过是通过两个角度来达到这个目的，一个是调整激活函数的结构，一个调整数据结构适应激活函数。</p><p>为什么说BN有效？</p><p>”BN相当于弱化了浅层和深层网络之间的耦合，使得每一层网络都能独立训练。“</p><p>对于不加BN的网络来讲，浅层网络的变动（参数的变动）会使得深层网络的输入也发生大的变动，进而造成模型的不稳定。而如果加上BN以后，浅层网络依然在变动，但是无论怎么变动都被限制在了一个均值为0，方差为1的范围内，这相当于增强了模型的鲁棒性。总结一下就是，加上BN以后，后层网络对于浅层网络的变化不那么敏感了。</p><p>BN具有部分正则化的能力，因为是使用了minibatch计算的均值和方差（不是使用全部数据），因此会引入一点噪声，正式这些噪声起到了正则化的作用。并且，minibatch的数量越少，正则化越强。这个特性也适用于dropout。但是不要讲BN用来作为正则化的工具，可以将BN和dropout一起使用。</p><p>(4)初始化权重</p><p>不能完全解决但是相当大程度上缓解。</p><script type="math/tex; mode=display">z = w_1 x_1+w_2x_2+w_3x_3+...+w_nx_n</script><p>如果n很大，那么一般我们希望$w_i$都很小。对于初始化，我们可以通过这样的方式使得$w_i$都差不多大</p><script type="math/tex; mode=display">w^{[l]} = np.random.rand(shape)*np.sqrt(\frac{1}{n^{l-1}})</script><p>Ng指出，如果使用relu这样的激活函数，那么使用$np.sqrt(\frac{2}{n^{l-1}})$是比较合适的。如果是使用tanh那么使用$np.sqrt(\frac{1}{n^{l-1}})$或者$np.sqrt(\frac{2}{n^{l-1}+n^{l}})$.</p><p>通常这样初始化权重，会对缓解梯度爆炸有不错的效果。</p><p>(5)残差结构。</p><p><strong>残差结构</strong>说起残差的话，不得不提这篇论文了：Deep Residual Learning for Image Recognition，关于这篇论文的解读，可以参考机器学习算法全栈工程师知乎专栏文章链接：<a href="https://zhuanlan.zhihu.com/p/31852747这里只简单介绍残差如何解决梯度的问题。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31852747这里只简单介绍残差如何解决梯度的问题。</a></p><p>待研究。</p><p>(6) LSTM</p><p>待研究。</p><h3 id="神经网络中防止过拟合的方法"><a href="#神经网络中防止过拟合的方法" class="headerlink" title="神经网络中防止过拟合的方法"></a>神经网络中防止过拟合的方法</h3><p>1、正则化l1,l2. （这个和线性回归中的l1,l2解释相同，从shrinking 的角度理解）</p><p>2、dropout</p><p>dropout的思想是对于参数比较多的隐含层,($w_l : (n^l,n^{l-1})$)，可以给其设置一个较小的keep_prob，比如0.5，对于参数较多的隐含层，可以给其设置一个大的keep_prob，比如0.7或者0.8。keep_prob表示激活概率，如果随机数大于这一概率即不进行激活。</p><p>当前Dropout被大量利用于全连接网络，而且一般认为设置为0.5或者0.3，而在卷积网络隐藏层中由于卷积自身的稀疏化以及稀疏化的ReLu函数的大量使用等原因，Dropout策略在卷积网络隐藏层中使用较少。</p><p>流程：</p><p>（1）首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图3中虚线为部分临时被删除的神经元）</p><p>（2） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。</p><p>（3）然后继续重复这一过程：</p><ul><li>恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）</li><li>从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。</li><li>对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。</li></ul><p>不断重复这一过程</p><p><strong>为什么说dropout可以防止过拟合？</strong></p><p><strong>（1）取平均的作用：</strong> 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</p><p><strong>（2）减少神经元之间复杂的共适应关系：</strong> 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。</p><p>（3）<strong>Dropout类似于性别在生物进化中的角色：</strong>物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。</p><p>(4) Ng给出的一种解释为：由于每一轮都都会随机去掉一些特征，使得模型不敢给给个神经元太多的权重，因为下一次它可能就被去掉了。因此，模型会给所有神经元相对小的权重，在这一点上表现像L2。</p><p>那么Dropout为什么需要进行缩放呢？</p><p>因为我们训练的时候会随机的丢弃一些神经元，但是预测的时候就没办法随机丢弃了。如果丢弃一些神经元，这会带来结果不稳定的问题，也就是给定一个测试数据，有时候输出a有时候输出b，结果不稳定，这是实际系统不能接受的，用户可能认为模型预测不准。那么一种”补偿“的方案就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的。比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是px+(1-p)0=px。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望。</p><p>3、early stoping早停策略防止过拟合</p><p>随着迭代次数的增加，训练集损失会单调递减，而验证集则会在损失下降一段时间后上升，我们选择那个两者相差最小的迭代次数时停止迭代。但是Ng解释说这样做在一定程度上能够缓解过拟合，但是却不是最理想的缓解过拟合的方法，它的缺点是将过拟合和减小损失作为一个问题考虑了。</p><p>这通常不是好的。</p><p>在xgboost里面也有一个早停参数。这个参数是根据我们传入的指标，比如auc和验证集，使得验证集变化不大的情况下提前停止迭代和这里早停不是很一样。</p><p>4、增添数据集</p><p>图片翻转，数字变形等等。</p><h3 id="神经网络中训练加速方法"><a href="#神经网络中训练加速方法" class="headerlink" title="神经网络中训练加速方法"></a>神经网络中训练加速方法</h3><h5 id="1、mini-batch"><a href="#1、mini-batch" class="headerlink" title="1、mini-batch"></a>1、mini-batch</h5><p>将数据集分成m小份，每一份记为$x^{\{i\}}$</p><p>for i  to m:   //用完m份记为一epoch</p><p>（用$x^{\{i\}},y^{\{i\}}$做一步梯度下降）</p><p>forward prop on $x^{\{i\}}$</p><script type="math/tex; mode=display">Z^{[1]} = W^{[1]}X^{\{i\}}+b^{[1]}\\A^{[1]} = g^{[1]}(Z^{[1]})\\...A^{[l]} = g^{[l]}(Z^{[l]})</script><p>compute cost </p><script type="math/tex; mode=display">J = \frac{1}{n(i)}\sum_{j}^{n(i)}L(\hat{y}^{(j)},y^{(j)})+\lambda/（2*n(i)） \sum_{l}||w^{[l]}||^2_2</script><p>backward prop on $x^{\{i\}}$</p><script type="math/tex; mode=display">w^{[l]} = w^{[l]}-\alpha dw^{[l]}\\b^{[l]} = b^{[l]} - \alpha db^{[l]}</script><h5 id="2、动量梯度下降和RMSprop-以及Adam"><a href="#2、动量梯度下降和RMSprop-以及Adam" class="headerlink" title="2、动量梯度下降和RMSprop,以及Adam"></a>2、动量梯度下降和RMSprop,以及Adam</h5><p>首先需要引入指数平滑的概念（非常有用）</p><script type="math/tex; mode=display">v_{t+1} = \beta v_t+(1-\beta)s_{t+1}</script><p> <img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/equation.svg" alt="equation"></p><p>可以看出，在指数平滑法中，所有先前的观测值都对当前的平滑值产生了影响，但它们所起的作用随着参数 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 的幂的增大而逐渐减小。那些相对较早的观测值所起的作用相对较小。同时，称α为<strong>记忆衰减因子</strong>可能更合适——因为α的值越大，模型对历史数据“遗忘”的就越快。从某种程度来说，指数平滑法就像是拥有无限记忆（平滑窗口足够大）且权值呈指数级递减的移动平均法。一次指数平滑所得的计算结果可以在数据集及范围之外进行扩展，因此也就可以用来进行预测。</p><p>对于$\beta$值得理解，给定$\beta$值，一般相当于过去$\frac{1}{1-\beta}$个样本进行平均。</p><p>什么意思呢，就是给定一堆样本（2维），样本的分布可能近似服从某一分布，通过指数平滑后，<img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/批注 2020-06-02 150911.png" alt="批注 2020-06-02 150911">样本在垂直方向上的波动会变弱。</p><p>指数加权平均的参数修正。</p><p>问题的提出，对于</p><script type="math/tex; mode=display">v_{t+1} = \beta v_t+(1-\beta)s_{t+1}</script><p>初始化$v_0$为0，那么对于第一天</p><script type="math/tex; mode=display">v_1 = 0.98*v_0+0.02\theta_1\\v_2 = 0.98*v_1 + 0.02\theta_2</script><p>由于$v_0$等于0，假设$\theta$等于40，那么对于第一天以及第二天的平均都是严重脱离真实值的。因此要引入参数修正。</p><p>解决 的方法为：</p><p>u(t) = $\frac{v_t}{1-\beta^t}=\frac{\beta v_{t-1}+(1-\beta)\theta_{t}}{1-\beta^t}$</p><h3 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h3><p>鞍点是指在高维空间中的某方向梯度为0的点，但不是局部最优点。</p><p>因为形状酷似马鞍，因此得名。鞍点才是高维空间中常见的局部最优点。<img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/捕获.PNG" alt="捕获"></p><p>关于为什么在高维空间中更容易碰到一个鞍点而非左图那样的局部最优点，Ng给出的解释为，在一个高维空间中一个最优点要满足各个方向上都是凸的，这其实很难，更常见的是在某一个纬度或者某几个纬度共同作用下的方向上是凸的，而在其他方向是凹的，也就是鞍点。</p><p>鞍点是降低学习速率的重要原因。实际上是鞍点所有的停滞区。停滞区是指导数长时间接近于零的一段区域。由于导数接近于零，因此平面呈现出一个很平的面，算法会在这个平面上缓慢的挪动去找到那个最低点，然后才能离开这个平面。<img src="/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/捕获2.PNG" alt="捕获2"></p><p>也就是说，坡度过缓导致算法收敛速度变慢了。</p><p>Ng的观点是：当我们训练一个大型的网络时，只要将初始损失函数定义在一个较高的点上，其实是不太可能进入局部最优。而真正影响收敛速度的是鞍点的存在。</p><p>解决的方法就是使用像动量梯度下降或者Adam这样的加速算法。加速离开停滞区的速度。</p><h3 id="神经网络调参原则"><a href="#神经网络调参原则" class="headerlink" title="神经网络调参原则"></a>神经网络调参原则</h3><p>1、固定超参范围，使用随机搜索。</p><p>2、在随机搜索的基础上固定一个更小的范围进行gridsearch。</p><p>3、参数的调试优先顺序，Ng说是学习率最高，其次是像Adam或者动量梯度下降的参数 和网络隐含层神经元的个数，再然后是网络的层数等等。（仅限于指导意义）</p><h3 id="正交化"><a href="#正交化" class="headerlink" title="正交化"></a>正交化</h3><p>主要思想：一个参数只保证一个功能。这样对于调参比较方便。</p><p>例子：电视机、汽车</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;为什么要使用神经网络，神经网络的优势？&lt;/p&gt;
&lt;p&gt;​    首先神经网络针对的问题一般是非线性假设，对与线性模型，要解决非线性假设的一般思路是多项式扩充，但是这样会造成特征爆炸，显然就不是很好用了。神经网络在解决非线性假设方面表现比较好。&lt;/p&gt;
&lt;h3 id=&quot;神经网
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/"/>
    
      <category term="神经网络" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="ML" scheme="https://qingfengbangzuo.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>对神经网络模型的一个小理解</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2020-06-20T16:00:00.000Z</published>
    <updated>2020-07-05T11:38:51.132Z</updated>
    
    <content type="html"><![CDATA[<h3 id="为什么使用非线性激活函数"><a href="#为什么使用非线性激活函数" class="headerlink" title="为什么使用非线性激活函数"></a>为什么使用非线性激活函数</h3><p>观点来自书籍《Neural_Network_Methods_for_Natural_Language_Processing_by_Yoav_Goldberg》.</p><p>假设一个XOR(异或)问题。</p><p>XOR(0,0)  =  0</p><p>XOR(1,0)  = 1</p><p>XOR(0,1)  = 1</p><p>XOR(1,1)  = 0</p><p>如果我们作图可以发现，无法找到一条直线将这两类点正确划分。也就是说线性模型对于XOR问题是无能为力的，将问题扩展到其他的非线性可分的情况，线性模型的缺点就暴露出来了，主要原因是线性模型的假设太强（要求数据线性可分）。</p><p>怎么办呢？</p><p>方法1是通过一种basic function: $\phi(\mathbf{X})$ ,进行坐标映射，对于XOR问题有$\phi(\mathbf{X}) = [x_1*x_2,x_1+x_2]$</p><p>这样处理过后，问题变为了线性可分。此时可使用线性模型进行划分。</p><p>​                                        $\hat{y} = \phi(\mathbf{X})W+\mathbf{b}$    </p><p>这里映射是一种维度不变的映射，但是通常找到一种这样的映射很难，一般地做法是将原始数据映射到高维空间，也就是kernel method. 关于核策略就不展开了。</p><p>方法2是训练一个非线性函数 ，对参数线性模型进行非线性映射，注意不是对数据映射，而是对模型进行映射。这个非线性函数成为</p><p>trainable nonlinear mapping function.</p><script type="math/tex; mode=display">\hat{y} = \phi(X)W+b\\\phi(X) = g'(XW'+b')</script><p>比方说，对于上面的XOR问题，可以找到一个g’ = max(0,x)并且W’ = (1,1,</p><p>1,1), b’ = (-1,0). 可以得到相同的$（x_1*x_2,x_1+x_2）$ .</p><p>似曾相识？这不就是神经网络的激活函数麽？</p><p>总结一下，使用激活函数可以达到将非线性问题转换为线性问题的效果，这是另一种形式的 非线性 -&gt; 线性 映射。</p><p>大胆揣测：</p><p>同样，像XOR问题这样找到一个合适的非线性函数是困难的，因此我们知道在神经网络中使用的都是通用的激活函数，sigmoid, tanh, relu等。那为什么神经网络依然有效呢？一个猜测是如果将神经网络视为一种非线性到线性的映射方式，层数或许可以弥补固定映射方式的不足。不断地叠加网络层数或许可以不断地逼近线性模型。</p><h3 id="为什么要使用深度神经网络"><a href="#为什么要使用深度神经网络" class="headerlink" title="为什么要使用深度神经网络"></a>为什么要使用深度神经网络</h3><p>Telgarsky  [2016]指出，多层有限大小(layer)的网络结构不能通过少层的网络结构来近似，除非这些层的大小是指数大的。</p><p>ndeed, Telgarsky [2016] show that there exist neural networks with many layers of bounded size that cannot be approximated by networks with fewer layers unless these layers are exponentially large.  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;为什么使用非线性激活函数&quot;&gt;&lt;a href=&quot;#为什么使用非线性激活函数&quot; class=&quot;headerlink&quot; title=&quot;为什么使用非线性激活函数&quot;&gt;&lt;/a&gt;为什么使用非线性激活函数&lt;/h3&gt;&lt;p&gt;观点来自书籍《Neural_Network_Methods_
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/"/>
    
      <category term="神经网络" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="ML" scheme="https://qingfengbangzuo.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>从词嵌入到CNN</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/20/NLP/%E4%BB%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%88%B0CNN/%E4%BB%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%88%B0CNN/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/20/NLP/%E4%BB%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%88%B0CNN/%E4%BB%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%88%B0CNN/</id>
    <published>2020-06-19T16:00:00.000Z</published>
    <updated>2020-06-28T07:37:24.846Z</updated>
    
    <content type="html"><![CDATA[<p>内容：</p><p>1、介绍了为什么需要引进CNN，CNN解决了那些问题。</p><p>2、介绍了CNN在NLP中的基本结构。</p><p>3、介绍了层次卷积的基本结构</p><h3 id="为什么需要CNN？"><a href="#为什么需要CNN？" class="headerlink" title="为什么需要CNN？"></a>为什么需要CNN？</h3><p>传统的词嵌入算法word2vec、Glove等等训练得到的向量都是基于“词”的。虽然它在训练的过程中使用到了上下文的语义信息或者统计方面的信息，但是依然面临着一些问题。、</p><p><strong>问题1：</strong>比如，在包含主观情感的句子中，形容词往往拥有比其他词性的单词包含更多的信息。在主题分类任务中，特殊的名词、代词相比其他词性的词语往往包含跟多的信息。这就产生了新的需求，我们希望找到某种“方法”能够根据不同的任务对一些关键词进行区别对待，或者说我们希望挖掘出那些包含更多信息量的词。</p><p><strong>问题2：</strong>在基于“词”的level上来训练任务往往会遇到一些问题，因为带有关键信息的可能不仅仅是单个词，而往往是一些词组（k-grams)，虽然CBOW考虑到了n-grams的信息，但是它的一个比较大的缺点是忽略了词序，从而造成一个比较大的问题：“it was not good, it was actually quite bad” and “it was not bad, it was actually quite good ”会给予一个相同的表示。但是这两句尽管很相似，但是在情感上完全是相反的。这意味着ngrams相比于bag-of-words可能包含了更多的信息。</p><p>一个很自然的解决方法是，扩充Embedding矩阵，给kgrams和word相同的表示，效果就是嵌入矩阵中不仅包含单个词的表示向量，同样包含词组（kgrams）的表示向量。然后就可以使用传统的词嵌入训练方法来进行训练。不过这样做的缺点有这样几点：1、扩充后的词嵌入矩阵可能会发生维度爆炸，因为对于一个$10^5$到$10^6$的语料库来讲，扩充带来的后果是词嵌入矩阵维度的指数级增长，同时带来了更大的稀疏性。2、由于矩阵的扩充会导致存储的消耗变得巨大。3、对(k-grams)的伸缩性不好。4、不同享统计信息，对于词组”very good”和“quite good”，它们两个词在嵌入矩阵中是独立的两个”词“，尽管它们拥有一个相同的单词“good”，但是它们在训练过程中被视为是不相关的，因此给定其中一个无法推断另一个。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">the embedding of “quite good” and “very good” are completely independent of one another, so if the learner saw only one of them during training, it will not be able to deduce anything about the other based on its component words .</span><br></pre></td></tr></table></figure><p>CNN被发现能够很好的缓解以上问题。无论是在CV领域还是NLP领域，CNN结构都可以被视为是一种特征抽取结构。Convolution  + Pooling 能够有效地从句子中抽出关键信息，这个关键信息是一个k-grams。因此CNN能够带来的几点好处是：1、不需要对词嵌入矩阵进行修改，从而避免了扩张带来的存储消耗。2、可以抽取k-grams信息，并且使得其中的统计信息可以被共享。3、伸缩性好，可以将k从1扩张到N（句子长度），只需要增加线性复杂度。4、输出维度可控，可以指定输出维度为任意大小。</p><h3 id="CNN结构-使用1D卷积"><a href="#CNN结构-使用1D卷积" class="headerlink" title="CNN结构(使用1D卷积)"></a>CNN结构(使用1D卷积)</h3><p><img src="/2020/06/20/NLP/%E4%BB%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%88%B0CNN/%E4%BB%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%88%B0CNN/image-20200626143733720.png" alt="image-20200626143733720"></p><p>如图所示是k=2，nfilter = 1时的CNN结构，可以看出卷积层就做了一件事儿，就是对一个大小为2的窗口滚动操作。这个操作如下：</p><script type="math/tex; mode=display">\begin{align}&对于序列&W_{1:n} &= {w_1,w_2,w_3,...,w_n}\\&定义一个concatenation操作：&x_i &= C(w_i:w_{i+k-1}); \quad x_i\in R^{k·d_{emd}}\\&g是一个非线性函数(tanh) &p^k_i &= g(x_i^T·u)\quad p^k_i \in R \quad u\in R^{k·d_{emd}} \end{align}</script><p>式中的i表示窗口的index。对于<script type="math/tex">nfilter = l</script>的情况，可以将参数矩阵$u$扩展成$U \quad Dim_U = (k·d_{emd},l)$.</p><p>得到 结果为</p><script type="math/tex; mode=display">P^k_i  = g(x_i^TU+b) \quad P^k_i \in R^{(1,l)}</script><p>假设sentence的长度为N，有</p><script type="math/tex; mode=display">P^k = g(X^TU+b) \quad P^k\in R^{(N-k+1,l)}</script><h4 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h4><p>由于卷积操作会使得维度下降（N个词的句子,有N-k+1个长度为k的窗口），padding是指对句子的前后进行插入一定数量的空白词，使得维度变化在一个可控的范围。上述操作没有进行padding，称为narrow convolution.还有一种称为wide convolution，就是在两边各插入k-1个词。这样得到的维度为N+k-1。</p><p>padding还有一个作用是提高两边词的检测度。</p><h4 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h4><p>对于一个窗口大小来讲，池化操作是指对filter过后的$(1,l)$维向量进行一个伸缩操作。常用的两种方法是max-pooling和average-pooling。顾名思义，max-pooling就是取$(1,l)$维向量中最大的那个，作为结果。average-pooling就是去$(1,l)$维向量的平均值作为结果。</p><p>pooling操作背后的含义：网上有不少介绍pooling的作用的博客，这里就不展开了，浅谈自己的一些认识。</p><p>要想了解pooling干了什么需要先知道卷积层做了什么，从效果上来看，卷积层无非是把句子中所有的n-grams特征都提取了出来。提取出来的这些特征经过一个concatenation操作和一个非线性函数映射，变为了一个值。这种操作类似于特征变换，将多个特征进行了一个非线性组合，然后得到了一个新的表示。这种思想应用到n-gram提取上显得再正常不过，也就是说我们可以将这个值看做是n-gram的一个表示，至于是哪方面的表示，这个是非线性变化的参数决定的，可能是词性可能是频率，也可能是其他。那么我们通过$l$个的filter得到了$l$个n-grams的不同表示，我们当然可以将这些表示作为一个向量直接扔进后面的训练过程，但是为了简化训练，我们总是希望根据这$l$个中总结出一个最好的表示来进行后续的训练，这样一来可以减少训练的复杂度，二来可以使得表示更加的鲁棒。那么常用的两种方法是max和average。这两种表示方法效果因人而异，不同任务表现出来的效果也不尽相同。</p><p>还有一种取法是k-max取法。就是取max最大的前k个表示最为最终的表示，但是在取值的过程中需要注意各个对应值的相对位置不变。</p><p><img src="/2020/06/20/NLP/%E4%BB%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%88%B0CNN/%E4%BB%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%88%B0CNN/k-max.PNG" alt="k-max"></p><p>max-pooling的值为[9,8,5].2-max的值为[[9,6,3],[7,8,5]].</p><h3 id="HIERARCHICAL-CONVOLUTIONS"><a href="#HIERARCHICAL-CONVOLUTIONS" class="headerlink" title="HIERARCHICAL CONVOLUTIONS"></a>HIERARCHICAL CONVOLUTIONS</h3><p>hierarchical convolutions是对CNN机构的一种扩展。目的和对图像进行多重卷积操作一样，都是为了能够提取更有效的表示。</p><script type="math/tex; mode=display">p_{1:m} = CONV^k_{U,b}(w_{1:n})\\p_i = g(C(w_{i:i+k-1})·U+b)\\m = n-k+1 \quad or \quad n+k-1\\--------------\\p^1_{1:m_1} = CONV^{k_1}_{U^1,b^1}(w_{1:n})\\p^2_{1:m_2} = CONV^{K_2}_{U^2,b^2}(p^1_{1:m_1})\\...\\p^r_{1:m_r} = CONV^{k_r}_{U^r,b^r}(p^{r-1}_{1:m_{r-1}})</script><p>效果 是$p_{1:m_r}^r$捕捉了一个递增的窗口。</p><p><img src="/2020/06/20/NLP/%E4%BB%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%88%B0CNN/%E4%BB%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%88%B0CNN/层次卷积.PNG" alt="层次卷积"></p><p>这个和CV中多层卷积目的一致，都是为了提取更有效的特征。我们假设窗口为k=2,经过第一层卷积提取出的特征表示的是2个相邻词的词组表示，再经过一层卷积之后，变为了三个相邻词的词组表示。</p><p>当然，也可以直接初始化窗口大小为3使用一层卷积来获得类似的结果，但是从效果来看，多层卷积的效果总是要好过使用单层卷积。</p><h4 id="Stride"><a href="#Stride" class="headerlink" title="Stride"></a>Stride</h4><p>步长是指在窗口滑动的过程中的滑动间隔，不做修改时默认为1，当然也可以为设为2。不过增加步长的效果是会得到一个更小的卷积层输出（行变少了）。</p><h4 id="dilated-convolution"><a href="#dilated-convolution" class="headerlink" title="dilated convolution"></a>dilated convolution</h4><p>是指在层次卷积的基础上，每一层卷积层的步长都设为k-1.这样会使得窗口大小变为一个关于layer的指数增长的值。</p><h4 id="Parameter-Tying"><a href="#Parameter-Tying" class="headerlink" title="Parameter Tying"></a>Parameter Tying</h4><p>指的是每一层卷积层都使用相同的参数集U，b。这样会造成更多的参数共享，并且不需要预先设定卷积层的大小，最终的结果是经过多次卷积之后，一个sequence就会收敛为一个向量表示。</p><h4 id="skip-connection"><a href="#skip-connection" class="headerlink" title="skip-connection"></a>skip-connection</h4><p>指的是对于第i层卷积层，不止使用第i-1层卷积层得到的向量表示，还要使用第i-2层卷积层得到的表示。</p><p>卷积层数是一个<strong>超参数</strong>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;内容：&lt;/p&gt;
&lt;p&gt;1、介绍了为什么需要引进CNN，CNN解决了那些问题。&lt;/p&gt;
&lt;p&gt;2、介绍了CNN在NLP中的基本结构。&lt;/p&gt;
&lt;p&gt;3、介绍了层次卷积的基本结构&lt;/p&gt;
&lt;h3 id=&quot;为什么需要CNN？&quot;&gt;&lt;a href=&quot;#为什么需要CNN？&quot; class
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Attention Mechanism</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/20/NLP/Seq2Seq/Attention/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/20/NLP/Seq2Seq/Attention/</id>
    <published>2020-06-19T16:00:00.000Z</published>
    <updated>2020-06-29T09:29:27.915Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h2><p>思想：人眼观察事物时，有一个聚焦机制，即只注意和目标物相关的部分，而忽略其他和目标物弱相关的部分。这种机制可以被引进Encoder-Decoder框架。</p><p>经典的encoder-decoder模型，编码器和解码器中间的耦合靠上下文向量$C$. 前面讨论过仅依靠上下文向量$C$维持的耦合关系是很弱的。Attention机制带的一个重大改变就是，编码-解码器之间的耦合不再使用单向量$C$. 而是使用编码器在每一个时间步产生的上下文$c_{i:n}$. </p><script type="math/tex; mode=display">c_{1:n} = ENC(x_{1:n}) = biRNN(x_{1:n})</script><p>这里的编码器使用的是双向RNN，因为要对时间步t进行编码，因此，考虑时间步t过去和未来是有必要的。</p><p>编码器产生上下文$c_i$的数学过程：</p><script type="math/tex; mode=display">s^f_i = R(s^f_{i-1},x_t)\\s^b_i = R(s^b_{n-i+1},x_t)\\c_i  = O([s^f_i,s^b_i])</script><p>得到输入序列每一个时间步的上下文编码，接下来考虑解码过程，Attention带来的第二个改变是，解码过程中每一个时间步所使用的上下文向量$c_j$不再是固定的，而是依据当前状态和前面得到的$c_{1:n}$构造出来的。构造过程如下：</p><script type="math/tex; mode=display">c_j = attend(c_{1:n},\hat{t}_{1:j})\\</script><p>式中$c_j$表示解码过程的第$j$个时间步所需要使用的上下文信息，这信息由$c_{1;n}$和$\hat{t}_{i:j}$（状态）构造， attend()是构造函数，是一个可训练(可以做梯度)的带参函数。下面是原版的soft attention机制：</p><script type="math/tex; mode=display">c_j = \sum_{i=1}^n a^{j}_{[i]}·c_i\\</script><p>$c_i$表示的是原序列中关于时间步$i$的上下文，$a^j_{[i]}$是该时间步上下文对应的权重，也就是说，解码过程中使用的上下文是原序列对应上下文的加权和。那么这个权重是如何得到的呢？原版的soft attention机制是经过一个MLP层训练：</p><script type="math/tex; mode=display">\bar{\alpha}_j = \bar{\alpha}^j_{[1]}...\bar{\alpha}^j_{[n]} = MLP([s_j;c_1]),...MLP([s_j;c_n])</script><p>式中的$s_j$是指解码过程中，时间步$j$对应的状态，$c_i$仍然是原序列对应的编码后的上下文，经过MLP层训练之后，我们可以得到一组非规范化的向量$\bar{\alpha}_j = \bar{\alpha}^j_{[1]}…\bar{\alpha}^j_{[n]}$, 然后通过softmax可以将其转化为和为1的概率分布。</p><script type="math/tex; mode=display">a_j = softmax(\bar{\alpha}^j_{[1]}...\bar{\alpha}^j_{[n]})</script><p>如何解释上面获得权重的过程呢？</p><p>对于机器翻译来说，MLP过程可以认为是在当前解码状态$s_i$（捕捉近期生成的词）和每一个原序列对应的编码后的上下文$c_i$之间软对齐（关联度）的计算过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">In the context of machine translation, one can think of MLPatt as computing a soft alignment between the current decoder state sj (capturing the recently produced foreign words) and each of the source sentence components ci.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>完整的Attention机制的计算过程如下：</p><script type="math/tex; mode=display">\begin{align}&attend(c_{1:n},\hat{t}_{1:j}) = c_j\\&c_j = \sum_{i=1}^n \alpha^j_{[i]}·c_i\\&\alpha^j =softmax(\bar{\alpha}^j_{[1]},...,\bar{\alpha^j_{[n]}})\\&\bar{\alpha}^j_{[i]} = MLP([s_j;c_i])\end{align}</script><p>完整的基于attention的encoder-decoder模型数学表达如下：</p><script type="math/tex; mode=display">\begin{align}&p(t_{j+1}= k|\hat{t}_{1:j},x_{1:n}) = f(O_{dec}(s_{j+1})\\&s_{j+1} = R_{dec}(s_j,[\hat{t}_j;c^j])\\&c^j = \sum_{i=1}^n \alpha^j_{[i]}·c_i\\&c_{1:n} = biRNN_{enc}(x_{1:n})\\&\alpha^j =softmax(\bar{\alpha}^j_{[1]},...,\hat{\alpha}^j_{[n]})\\&\bar{\alpha}^j_{[i]} = MLP([s_j;c_i])\\&\hat{t}_j \approx p(t_j|\hat{t}_{1:j-1},x_{1:n})\\&f(z) = softmax(MLP^{out}(z))\\& MLP([s_j;c_i]) = v· tanh([s_j;c_i]U+b)\end{align}</script><p><img src="/2020/06/20/NLP/Seq2Seq/Attention/Attention.PNG" alt="Attention"></p><h2 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h2><p>为什么要使用BiRNN来训练得到一个上下文$c_{1:n}$而不直接使用原文$x_{1:n}$呢？</p><p>其实是可以使用原文来进行attention机制的计算的，但是我们可以从$c_{1:n}$中得到更多 , 受限BiRNN训练得到的$c_i$不仅对应了序列中对应的$x_i$，它还包含了$x_i$ 的上下文。其次，通过使用一个经过训练的表示$c_i$，可以将编码和解码过程绑定在一起（训练），这样可以使得网络去学习那些对解码过程有用的编码方式，而这种信息一般不会直接体现在原文里。</p><p>这里说的有点绕，其实意思很简单，通过将编码和解码参数绑定在一起训练，使得编码得到的上下文$c_{1:n}$对解码过程是有用的，原始的输入向量表达的信息是有限且固定的，通过一层特征提取，可以使得训练效果更好。</p><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">For example, the biRNN encoder may learn to encode the position of xi within the sequence, and the decoder could use this information to access the elements in order, or learn to pay more attention to elements in the beginning of the sequence then to elements at its end.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h2 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h2><p>当不采用Attention的时候，训练的复杂度取决于编码的长度和解码的长度，（不考虑softmax的计算）$O(m+n)$。</p><p>当采用Attention的时候，由于针对每一个解码过程都需要训练一个上下文$c_j$，训练的复杂度为$O(m \times n)$。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Attention-Mechanism&quot;&gt;&lt;a href=&quot;#Attention-Mechanism&quot; class=&quot;headerlink&quot; title=&quot;Attention Mechanism&quot;&gt;&lt;/a&gt;Attention Mechanism&lt;/h2&gt;&lt;p&gt;思想
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Seq2Seq</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/18/NLP/Seq2Seq/Seq2Seq/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/18/NLP/Seq2Seq/Seq2Seq/</id>
    <published>2020-06-17T16:00:00.000Z</published>
    <updated>2020-06-30T01:39:30.565Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序列生成模型"><a href="#序列生成模型" class="headerlink" title="序列生成模型"></a>序列生成模型</h2><p>一个序列生成模型一般要满足这样的假设：</p><p>当前值$t_i$的分布依赖于之前$1:t_{i-1}$个词。即$P(t_i|t_{1:i-1})$,在每一步中，得到当前值得分布，即得到$P(t_i=k|t_{1:i-1})$的概率值，然后将当前值作为预测下一个值得已知条件。</p><p>RNN满足序列生成模型的形式：</p><p><img src="/2020/06/18/NLP/Seq2Seq/Seq2Seq/RNN_seqgenerate.PNG" alt="RNN_seqgenerate"></p><p>当RNN作为transducer时，只需将时间步t时刻的输出连接到时间步t+1时的输入即可。在训练过程中，每轮训练开始都给予一个初始字符$<s>$,遇到结束字符$</s>$时结束。在时间步$t$时，输入有$s_{t-1}$和$y_{t-1}$，$s_{t-1}$是之前$t-1$个词的编码，$y_{t-1}$是上一个时间步的预测值，$P(y_t|s_{t-1},y_{t-1})$近似于使用前$t-1$个词作为条件，等同于假设$P(t_i|t_{1:t-1})$。$y_i$的结果一般为一个词分布，大小为字典长度，采用softmax来转换概率。$s_0$初始化为0，但是decoder模型中，该值被初始化为$s_0 = \delta({VC})$.</p><p>注：当$y_{t-1}$被当做下一时刻的输入时，该值选用$y_{t-1}$分布中概率值最大的那个值。</p><p><strong>teacher-forcing: </strong>是另外一种训练方法，这种训练方法和上述最大的不同在于每个时间步输入$y_{t-1}$不再是模型上一个时间步的预测结果，而是训练序列中上一个时间步对应的值，是标签。这样会导致一个现象，就是对于时间步t，它的输入$y_{t-1}$（真是的标签）在上一个时间步的预测概率很小，但是由于它是真实的标签，因此在下一时间步被作为输入。这种方式导致的后果就是，在测试集中，会出现不可预知的结果。</p><h2 id="Conditioned-generation-framework"><a href="#Conditioned-generation-framework" class="headerlink" title="Conditioned generation framework"></a>Conditioned generation framework</h2><p>在条件生成框架下，下一个token的产生依赖于两个，一个是过去生成的token，另一个是上下文。</p><script type="math/tex; mode=display">\tilde{t}_{j+1} \approx p(t_{j+1} = k|\tilde{t}_{1:j},c)</script><p>用RNN形式表示就是：</p><script type="math/tex; mode=display">p(t_{j+1}=k|\tilde{t}_{1:j},c) = f(O(s_{j+1}))\\s_{j+1} = R{s_j,[\tilde{t_j};c]}\\\tilde{t}_j\approx p(t_i|\tilde{t}_{1:j-1},c)</script><p>这就是条件生成框架：</p><p><img src="/2020/06/18/NLP/Seq2Seq/Seq2Seq/条件生成框架.PNG" alt="条件生成框架"></p><p>关于上下文c的解释如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">What kind of information can be encoded in the context c? Pretty much any data we can put our hands on during training, and that we find useful. For example, if we have a large corpus of news items categorized into different topics, we can treat the topic as a conditioning context. Our language model will then be able to generate texts conditioned on the topic. If we are interested in movie reviews, we can condition the generation on the genre of</span></span><br><span class="line"><span class="string">the movie, the rating of the review, and perhaps the geographic region of the author. We can then control these aspects when generating text. We can also condition on inferred properties, that we automatically extract from the text. For example, we can derive heuristics to tell us if a given sentence is written in first person, if it contains a passive-voice construction, and the level of vocabulary used in it. We can then use these aspects as conditioning context for training, and, later, for text generation.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>大体意思就是说，上下文C可以是一个类别信息，比方说电影的类别，文本的话题。或者我们抽象出来的内容，比如机器翻译中输入序列的编码。</p><h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p>encode-decoder框架是针对Seq2Seq问题提出来的,Seq2Seq是指在机器翻译、语音识别等领域需要将任意长度的输入序列转换为任意长度的输出序列。</p><p>RNN（LSTM\GRU）的trasnducer模式本身是符合编码解码模式的，它的输入就是一段序列，输出也是一段序列，只不过在该框架下的输入输出是对称的。</p><p>为了解决输入输出不对称的问题，诞生了encode-decoder框架，这个框架依托于上述描述的条件生成框架。观察上述条件生成框架，我们发现该框架本质是一个序列生成模型，而解码器的功能就是产生一个基于输入序列的转换编码。因此，基于此框架作为解码器是合理的，而条件生成框架中存在一个关于上下文的输入，这个东西可以由输入序列的编码来代替。这样逻辑上是自恰的，给定一个输入序列，编码器将其转换为合适的编码，这个编码后信息可能是句子的主题、句子的语态等等（深度学习的不可知论），依据此编码作为上下文（也可以理解为这个编码内部包含了我们需要预测的句子的指示信息或者上下文，它用于指示我们的解码器需要生成的句子需要在什么样的上下文中）可以作为生成句子的条件（约束）。</p><p><img src="/2020/06/18/NLP/Seq2Seq/Seq2Seq/encoder-decoder.PNG" alt="encoder-decoder"></p><p>编码器和解码器被绑定在一起训练，但是标签只作用于解码器，但是梯度可以经过解码器被传播回编码器。</p><p><img src="/2020/06/18/NLP/Seq2Seq/Seq2Seq/encoder-decoder_loss.PNG" alt="encoder-decoder_loss"></p><p>基本的训练框架就是上图表示的这样，编码器为一个RNN网络比如说LSTM或者GRU。</p><p>解码器也为一个RNN网络（LSTM、GRU、BiRNN、CNN）。</p><h2 id="数学解释"><a href="#数学解释" class="headerlink" title="数学解释"></a>数学解释</h2><p>对Seq2Seq建模：</p><script type="math/tex; mode=display">P(Y|X) = P(y_1,y_2,y_3,...y_t|X)</script><p>$X$为输入序列，$Y$为输出序列。问题的表述为给定任意长度的输入序列，预测出一个概率最大的输出序列。</p><script type="math/tex; mode=display">argmax_Y log(P(Y|X)) = argmax_Ylog(\prod_{i=1}^tP(y_i|y_{i-1},y_{i-2},...,y_1;X))\\=argmax_Y\sum_{i=1}^tlog(P(y_i|y_{i-1},y_{i-2},...,y_1;X))</script><p>观察$P(y_i|y_{i-1},y_{i-2},…,y_1;X)$,$X$为原文输入序列，对于encoder-decoder来说，$X$可以被编码器转化为全局编码$C$.</p><script type="math/tex; mode=display">C = ENC(X)</script><p>ENC为编码器，假设编码器采用LSTM，那么编码流程：</p><script type="math/tex; mode=display">\begin{align}&s_{t-1} = [c_{t-1};h_{t-1}]\\&i_t = \delta(w^{hi}h_{t-1}+w^{x_i}x_{t})\\&f_t = \delta(w^{hf}h_{t-1}+w^{xf}x_t)\\&o_t = \delta(w^{ho}h_{t-1}+w^{xo}x_t)\\&g_t = tanh(w^{hg}h_{t-1}+w^{xg}x_t)\\& c_t = f_t \odot c_{t-1}+i_t \odot g_t\\&h_t = o_t \odot tanh(c_t)\end{align}</script><p>注意上面过程中的两个tanh函数的位置。</p><p>当遍历完整个输入序列后，得到全局编码$h_T$.此时不能将全局编码$H_T$直接扔进解码器，需要做一步转换。</p><script type="math/tex; mode=display">C  = tanh(V·h_T+b_c)</script><p>这里的激活函数是tanh，有几个作用，一个作用是激活，另一个作用是将编码的值映射到x轴的两侧，这样做可以加速梯度下降，还有一个作用就是维度变换，将向量$h_T$变换成我们需要的维度。</p><p>此时得到的$C$可以作为上下文输入解码器了。对于解码过程</p><script type="math/tex; mode=display">h_t = R(h_{t-1},y_{t-1},C)\\y_t = softmax(O(h_t,y_{t-1},C))</script><p>另外，对于$h_0$的初始化：</p><script type="math/tex; mode=display">h_0 = tanh(V^{ch}C)</script><p>这里原文是采用双向的GRU，因此对于初始状态的定义为：</p><script type="math/tex; mode=display">h_0 = tanh(W_sh^b_1)</script><p>这样就可以将解码过程与序列生成模型连接起来了。</p><p>局限性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">encoder-decoder模型虽然非常经典，但是局限性也非常大。最大的局限性就在于编码和解码之间的唯一联系就是一个固定长度的语义向量C。也就是说，编码器要将整个序列的信息压缩进一个固定长度的向量中去。但是这样做有两个弊端，一是语义向量无法完全表示整个序列的信息，还有就是先输入的内容携带的信息会被后输入的信息稀释掉，或者说，被覆盖了。输入序列越长，这个现象就越严重。这就使得在解码的时候一开始就没有获得输入序列足够的信息， 那么解码的准确度自然也就要打个折扣了.</span></span><br><span class="line"><span class="string">https://zhuanlan.zhihu.com/p/48648001</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h3 id="一个值得思考的问题"><a href="#一个值得思考的问题" class="headerlink" title="一个值得思考的问题"></a>一个值得思考的问题</h3><p>面对不同语言之间的语义不同的问题，模型是如何解决的？模型真的可以自动学到不同语言之间的特性么？直到目前为止，所有基于词向量的模型的特征抽取本质上都是对token统计特征的抽取，这是否是最好的词表示方式？</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;序列生成模型&quot;&gt;&lt;a href=&quot;#序列生成模型&quot; class=&quot;headerlink&quot; title=&quot;序列生成模型&quot;&gt;&lt;/a&gt;序列生成模型&lt;/h2&gt;&lt;p&gt;一个序列生成模型一般要满足这样的假设：&lt;/p&gt;
&lt;p&gt;当前值$t_i$的分布依赖于之前$1:t_{i-1}$
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>RNN的基本结构</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/</id>
    <published>2020-06-15T16:00:00.000Z</published>
    <updated>2020-06-28T07:54:34.849Z</updated>
    
    <content type="html"><![CDATA[<h3 id="RNN的基本结构"><a href="#RNN的基本结构" class="headerlink" title="RNN的基本结构"></a>RNN的基本结构</h3><p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN的基本结构1.PNG" alt="RNN的基本结构1"></p><p>上图是RNN的基本单元和基本结构。</p><p>可以看出RNN（Recurrent Neuron Network）是递归神经网络的一种。不过，RNN还有一种表示形式如下，这种形式和上述形式是等价的，是按时间步长展开的表示。</p><p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN基本结构2.PNG" alt="RNN基本结构2"></p><hr><h3 id="循环神经网络or递归神经网络"><a href="#循环神经网络or递归神经网络" class="headerlink" title="循环神经网络or递归神经网络"></a>循环神经网络or递归神经网络</h3><p>引用博客：<a href="https://blog.csdn.net/zhangjiali12011/article/details/90045978" target="_blank" rel="noopener">https://blog.csdn.net/zhangjiali12011/article/details/90045978</a></p><p>很明显，它俩名字就是不一样的，循环神经网络是Recurrent Neural Network，递归神经网络是Recursive Neural Network。</p><p>当然循环神经网络也确实可以归类到递归神经网络，从广义上说，递归神经网络分为结构递归神经网络和时间递归神经网络。从狭义上说，递归神经网络通常指结构递归神经网络，而时间递归神经网络则称为循环神经网络。</p><p>两者最主要的差别就在于Recurrent Neural Network是在时间维度展开，Recursive Neural Network在空间维度展开。一个很重要的改型LSTM也是基于Recurrent Neural Network改进的。</p><hr><p>图中为$x_i$代表时间步$i$的输入，即序列中第$i$个词。$s_i$代表时间步$i$的状态，$s_{i-1}$代表上一个时间步的状态，$s_0$是 一个初始状态，一般被设置为0. 状态更新函数为$s_i = R(s_{i-1},x_i|\theta)$,也称作激活函数。输出函数为$y_i$是时间步$i$的输出，这个输出不是必须的。输出函数$y_i = O(s_i|\theta)$,$\theta$是整个RNN网络的参数，包括了$\theta_R$和$\theta_O$.</p><p>下面对应一下各个变量和参数的维度：</p><script type="math/tex; mode=display">\begin{align}&s_i = tanh(\theta_ss_{i-1}+\theta_xx_i+b_a)\\&y_i = softmax(\theta_ys_i+b_y) \\&x_i: (n_x,m)\\&s_i: (n_a,m)\\&\theta_x:(n_a,n_x)\\&\theta_s:(n_a,n_a)\\&\theta_y:(n_y,n_a)\\&b_a:(n_a,1)\\&b_y:(n_y,1)\end{align}</script><h3 id="RNN解决了什么？"><a href="#RNN解决了什么？" class="headerlink" title="RNN解决了什么？"></a>RNN解决了什么？</h3><p>回答这个问题其实挺难，因为截至到目前为止，人们依然无法很好地解释RNN为何能取得优异地表现。但是RNN的一些特性是明显的，比如说，它的每一步都继承了上一步的状态，这使得它的当前状态对过去状态产生了依赖，这个性质是符合语言特征的。其次它对位置十分敏感，不同的位置导致了不同的更新过程，这个主要体现在非线性激活函数的作用上。对比语法的特征，可以发现RNN的这个性质也是符合语言特征的，因为对于主流语言英语、汉语，词性决定了单词出现的相对位置，尽管这个位置有时候是捉摸不定的，但是其仍然具有一定的规律可循。</p><p>那么RNN到底解决了什么？</p><p>要想回答这个需要先看一下CNN解决了什么，CNN解决的问题很直接，就是CBOW严重忽略了词序，导致在进行一些分类预测任务的时候出现了问题。而CNN可以通过卷积+池化的操作使得模型对局部的顺序变得敏感。通俗一点说就是，CBOW只能捕捉到单个词的特征，而CNN可以捕捉词组的特征，这个特征包含了统计特征和语义方面的特征。CNN是不完美的，因为它更多的是对局部的序列顺序敏感，而不是整个句子。比方说“not good”不能说成是”good not”，但是至于”not good”放在什么句子的什么地方，CNN是对此是较疲软的。RNN解决了这个问题。</p><p>RNN是对CNN的扩展，因为RNN可以捕捉全局（整个句子）的特征，原因是它整个句子的词序都是敏感的，更因为它的每一个状态都是建立在前一个状态之上的。这就使得当前词对它之前出现过的所有词都构成了隐形的依赖。</p><p>看两个最基本的RNN结构。</p><h4 id="CBOW-RNN"><a href="#CBOW-RNN" class="headerlink" title="CBOW-RNN"></a>CBOW-RNN</h4><script type="math/tex; mode=display">s_i = R_{CBOW}(x_i,s_{i-1}) = s_{i-1}+x_i\\y_i = O_{CBOW}(s_i) = s_i</script><p>对应一下个参数维度：</p><script type="math/tex; mode=display">\begin{align}&x_i: (n_x,m)\\&s_i: (n_x,m)\\&y_i:(n_x,m)\end{align}</script><p>这是CBOW的RNN版本，可以看到这个网络对于序列顺序是不敏感的，因为顺序的变换不会导致更新过程的变换（忽略$y_i$无论怎么加结果都不变）。</p><h4 id="Simple-RNN-S-RNN"><a href="#Simple-RNN-S-RNN" class="headerlink" title="Simple RNN(S-RNN)"></a>Simple RNN(S-RNN)</h4><script type="math/tex; mode=display">s_i  = R_{SRNN}(x_i,s_{i-1}) = g(\theta_ss_{i-1}+\theta_xx_i+b_a)\\y_i = O_{SRNN}(s_i) = s_i</script><p>对应一下个参数维度：</p><script type="math/tex; mode=display">\begin{align}&x_i: (n_x,m)\\&s_i: (n_a,m)\\&\theta_x:(n_a,n_x)\\&\theta_s:(n_a,n_a)\\&b_a:(n_a,1)\\&y_i:(n_a,m)\end{align}</script><p>激活函数g一般使用tanh或者Relu。这个网络是可能是最简单的循环神经网络之一了，我们可以拿这个网络作为一个Baseline来学习。SRNN相较于CBOW，只多了一层非线性激活层，但就是这一层可以使得网络对于输入顺序变得极为敏感，因为不同的输入顺序会导致参数的更新过程不同。</p><h3 id="Acceptor、Encoder、Transducer"><a href="#Acceptor、Encoder、Transducer" class="headerlink" title="Acceptor、Encoder、Transducer"></a>Acceptor、Encoder、Transducer</h3><p>以上是RNN三种形式的使用类比。</p><p>对于第一种”Acceptor”，RNN只关注最后一个状态的输出，那么整个RNN结构就相当于一个训练器，它的输出结果就是最后一个时间步对应的$y_i$。倘若我们可以对每一个输出进行标签化，那么就可以将其视为一个有监督的学习过程。</p><p>对于第二种“Encoder”,顾名思义这种方式就是将RNN视为一种编码器，将RNN的最后一个时间步对应的输出视为编码结果，由于输出的维度是可定制的，因此，我们可以将RNN对我们的sample进行了重新编码。之后，可以将编码结果伙同其他特征一起送到更深层的网络进行训练。</p><p>一个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">an extractive document summarization system may first run over the document with an RNN, resulting in a vector yn summarizing the entire document. Then, yn will be used together with other features in order to select the sentences to be included in the summarization.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>对于第三种”Transducer”,形式更为我们熟知，它是指在训练过程中存储每一个时间步对应的输出求损失，最后将所有的损失综合到一起反馈给网络。一个熟知的模型就是RNN可以作为语言模型，因为它的第i个时间步的输出可以被视为给定前i-1个上下文得到的概率分布。</p><p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/transducer.PNG" alt="transducer"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Using RNNs as transducers allows us to relax the Markov assumption</span></span><br><span class="line"><span class="string">that is traditionally taken in language models and HMM taggers, and</span></span><br><span class="line"><span class="string">condition on the entire prediction history.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>到这里可以稍微总结一些RNN的特性：</p><p><strong>1、对序列顺序敏感</strong></p><p><strong>2、训练采用后向传播</strong></p><p><strong>3、输出维度可定制化，输入维度不敏感</strong></p><p><strong>4、由于结构是循环(递归)的，因此所有的单元共用一套参数。</strong></p><p><strong>5、RNN的输入可以是One-hot</strong></p><h3 id="Bidirection-RNN（BI-RNN）"><a href="#Bidirection-RNN（BI-RNN）" class="headerlink" title="Bidirection RNN（BI-RNN）"></a>Bidirection RNN（BI-RNN）</h3><p>双向RNN解决的问题是什么？</p><p>在n-grma模型里面，我们为了捕捉词向量的语义上的特征，因此使用了滑动窗口，既捕捉目标词前k个单词，也捕捉目标词的后k个单词。但在RNN的基础结构中，我们可以看到模型只捕捉到了时间步i的历史依赖，对于未来可能出现的依赖是乏力的。因此，使用双向RNN可以缓解这个问题，顾名思义双向RNN就是使用两个RNN来从不同的方向对时间步i进行扫描。这样，使得得到到的时间步i的状态既依赖于过去也依赖于未来。</p><p>就像RNN放宽了马尔科夫假设，使得模型可以捕捉过去任意长度的词与目标词的依赖关系（马尔科夫是前k个词），biRNN放宽了窗口长度的假设，使得模型可以捕捉任意窗口大小的词与目标词的依赖关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Much like the RNN relaxes the Markov assumption and allows looking arbitrarily back into the past, the biRNN relaxes the fixed window size assumption, allowing to look arbitrarily far at both the past and the futurewithin the sequence.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/BIRNN.PNG" alt="BIRNN"></p><p>由于采用了双向的RNN扫描，因此在时间步状态的表示上会进行一些更改。$S_i = [s^f_i,s^b_i]$, $y_i = [O^f(s_i^f);O^b(s_i^b)]$.</p><p>定义biRNN的形式为：</p><script type="math/tex; mode=display">biRNN(x_{1:n},i) = y_i = [RNN^f(x_{1:n});RNN^b(x_{n:i})]</script><p>这里可以是一个合并操作。使用sum的话，没有concat更能反映编码捕捉到的信息。</p><p>另一种形式的BIRNN</p><p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/biRNN2.PNG" alt="biRNN2"></p><p>biRNN被广泛应用在各种任务中，其中在sequence tagging中表现优异。</p><h3 id="Multi-layer-RNN"><a href="#Multi-layer-RNN" class="headerlink" title="Multi-layer RNN"></a>Multi-layer RNN</h3><p>和CNN一样，RNN也是可以堆叠的，称为深度RNN或者多层RNN。研究发现，多层RNN在效果上好于单层RNN，但是理论支撑的工作还没有完成。</p><p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/Multi-layerRNN.PNG" alt="Multi-layerRNN"></p><p><strong>开下脑洞：</strong>为甚么2层的会比一层的有效？</p><p>要回答这个问题需要知道RNN干了一件什么事。</p><script type="math/tex; mode=display">s_i = tanh(\theta_ss_{i-1}+\theta_xx_i+b_a)\\y_i = softmax(\theta_ys_i+b_y) \\</script><p>对于非线性激活函数，我一直将其视为是一种特征抽取方式，就是将一个简单的特征变位一个比较复杂但是更好地特征，或者说对原始特征进行了重新解决，解读过后变得更容易被接受了。这可能是所有表示学习的一种特性，比如神经网络就是通过一轮又一轮地非线性变换然后得到一个比较不错的表达。那么对于RNN来讲，单层的RNN相当于对特征$(s_i,x_i)$进行了一次解读，多层的RNN相当于对特征进行了多次的解读。那么多次的解读一定比单次或者原始特征更好么，这个也不是一概而论的，比如核变换将数据映射到不同的维度观察到的分布特性也是不同的，至于映射到哪一维更好，或许是个超参数。我将这个对比用于非线性变换，因为非线性变换与核映射有异曲同工之妙。</p><p>如果将RNN视为对时间步特征的非线性解读，那么解读后的特征就是对时间步对应词的一个特征变换。通常来说通过这样的变化带来的收益是正的，因为原始的词特征表示没有考虑到全局依赖的问题。而变换后的特征是一种目标词和上下文(全局)的混合表示，更强更壮。多层RNN是对这个混合过程进行了更多次的抽取，按照上面的解释，结果一般总能表现地更好。</p><h3 id="门控结构"><a href="#门控结构" class="headerlink" title="门控结构"></a>门控结构</h3><p>S-RNN在训练时遭遇了梯度消失的问题，导致了实际应用时效果大打折扣。</p><p>梯度消失会导致参数更新无法传播到前面几层，就是越靠近输入端越难以被更新，或者更新速度很慢。这样还附带了一个现象，就是由于梯度传播不过来，导致RNN很难去捕捉long-range的依赖。（前面分析的是RNN有能力捕捉过去和未来的全局依赖，这里提出的门控结构更像是为了解决工程问题）</p><p>门控结构可以缓解这个问题。门控结构的基本思想：</p><p>如果将RNN视为一种有着有限大小资源计算硬件，计算单元R读进输入$x_{i+1}$,和当前状态$s_i$,然后对他们进行某种计算操作。并且将计算结果写入内存，替换掉旧的状态$s_i$。从这个角度看SRNN的问题是，内存的介入是不受控制的，也就是说在每一步计算中全部内存状态被读入，然后又被更新。通俗来讲，就是在计算时间步i的时候，我们是将先前所有的状态信息都读入进去了，那些读进去的状态信息只有一部分是真正对目标有依赖关系的，其余的相当于噪声。很自然的一个想法就是能不能将那些对目标词没有关系的状态不读入目标词时间步的计算过程，只读入那些有依赖关系的状态。如果能够只读入有依赖关系的状态，那么在反向传播的过程就意味着不需要每个状态依次更新，而只更新那些被读入的状态，这可以大大缓解梯度消失的问题。想要达到这样的目的，可以采取一种门控机制：</p><p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/gate.PNG" alt="gate"></p><p>就是在每一步读入的时候或者是说在计算时间步$s’$的时候，由一个门控单元控制读入的状态。</p><p>这种机制的一个特点是，假设状态变量$s_i$有$n_a$维，每次更新并不是对所有的维度进行更新，而是根据门控单元g选择一些维度进行更新。可以暂且认为这些被更新的维度是与当前状态有关的。</p><p>接下来的问题变为了如何设计门控单元g。很显然，门控单元是时间步的函数g = f(t)。它所时间步的不同而改变，且不应该由人来指定。典型的硬编码门控单元：$g \in \{0,1\}$无法进行梯度下降，因此可以采用一种软门控单元来既达到门控的效果，又与时间步联系起来。</p><p>一种方法是放宽g的取值范围，令$g\in R^n$,且对$g$施加一sigmod函数$\delta(g)$。这里的$\delta$函数有一个好处，就是当g趋于均匀分布时，它对于大部分g值都能映射为一个接近$\{0,1\}$的值，只有当g取接近于0的数时，才会被映射为0到1之间的值，因此，为了避免这种现象，在实际使用时，可以将接近于0的g值舍去，只留下那些绝对值较大的g值。这样，采用软门控，可以使得求梯度了。</p><p><strong>现在只留下如何构造g了。</strong></p><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>LSTM的全拼是 The Long Short-Term Memory architecture.被 Hochreiter and Schmidhuber 于1997年提出。是一种门控结构的神经网络。</p><p>LSTM将状态向量$s_i$分为了两部分，一部分被称为“memory cell”。另一部分被称为”working memory”。”memory cell”用于存贮状态和”error gradients”.可以通过”dierentiable gating components”控制。在整个模型框架下有三个门控i,f,o，分别对应输入，隐含层的更新和输出。</p><script type="math/tex; mode=display">\begin{align}&s_i = R_{LSTM}(s_{i-1},x_j) = [c_j;h_j]\\&c_j = f\odot c_{j-1}+i\odot z\\&h_j = o \odot tanh(c_j)\\&i = \delta(x_j W^{xi}+h_{j-1}W^{hi})\\&f = \delta(x_j W^{xf}+h_{j-1}W^{hf})\\&o = \delta(x_jW^{x0}+h_{j-1}W^{h0})\\&z  = tanh(x_jW^{xz}+h_{j-1}W^{hz})\\&y_i = O_{LSTM}(s_j) = h_j\\&s_j \in R^{2·d_h},x_i\in R^{d_x},c_j,h_j,i,f,o,z\in R^{dh},W^{xo}\in R^{d_x,d_h},W^{ho}\in R^{d_h,d_h}\end{align}</script><p>$c_j$表示”memory cell”，$h_j$表示working memory。两个合并到一块表示状态$s_j$。</p><p>注意这里关于g的构造，</p><script type="math/tex; mode=display">g_i = x_jW^{xi}+h_{j-1}W^{hi}</script><p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/LSTM结构图.PNG" alt="LSTM结构图"></p><p>水平线是状态。</p><p>关于LSTM的变种，一种是Gers和Schmidhuber于2000年提出的增加了窥视孔连接。</p><p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/peephole_conenction.PNG" alt="peephole_conenction"></p><p>另一种变种是使用了配对遗忘门和输入门，与之前分别决定遗忘与添加信息不同，我们同事决定两只。只有当我们需要输入一些内容的时候我们才选择忘记。只有当早前信息系被忘记之后我们才会输入。</p><p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/variant2.PNG" alt="variant2"></p><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p>GRU没有将状态分割成两部分。</p><script type="math/tex; mode=display">\begin{align}&s_j = R_{GRU}(s_{j-1},x_j) = (1-z)\odot s_{j-1}+z \odot \tilde{s_j}\\&z = \delta(x_jW^{xz}+s_{j-1}W^{sz})\\&r = \delta(x_jW^{xr}+s_{j-1}W^{sr})\\&\tilde{s_j} = tanh(x_jW^{xs}+(r\odot s_{j-1})W^{sg})\\&y_j = O_{GRU}(s_j) = s_j\end{align}</script><p>这里只使用了两个门，一个门r用来控制介入的先前状态$s_{j-1}$并且计算$\tilde{s_j}$。状态更新使用到了先前的状态$s_{j-1}$和计算值$\tilde{s_j}$，使用门$z$来控制比例。</p><p><img src="/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/RNN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84/GRU.PNG" alt="GRU"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;RNN的基本结构&quot;&gt;&lt;a href=&quot;#RNN的基本结构&quot; class=&quot;headerlink&quot; title=&quot;RNN的基本结构&quot;&gt;&lt;/a&gt;RNN的基本结构&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/2020/06/16/NLP/RNN%E7%9A%84%E5%9F%B
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Word GloVe</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/16/NLP/Glove/GloVe/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/16/NLP/Glove/GloVe/</id>
    <published>2020-06-15T16:00:00.000Z</published>
    <updated>2020-06-22T12:56:07.254Z</updated>
    
    <content type="html"><![CDATA[<h2 id="GloVe-Global-Vectors-for-Word-Representation"><a href="#GloVe-Global-Vectors-for-Word-Representation" class="headerlink" title="GloVe: Global Vectors for Word Representation"></a>GloVe: Global Vectors for Word Representation</h2><p>​        这篇文章基于论文《GloVe: Global Vectors for Word Representation  》进行总结学习，如有疑惑，以原文为主。</p><h3 id="什么是Matrix-Factorization-和Local-Context-Window？以及各自的不足？"><a href="#什么是Matrix-Factorization-和Local-Context-Window？以及各自的不足？" class="headerlink" title="什么是Matrix Factorization 和Local Context Window？以及各自的不足？"></a>什么是Matrix Factorization 和Local Context Window？以及各自的不足？</h3><p>​        关于如何表示词向量有两种流派：一种是利用Distributional Hypothesis，另一种被称为“Distributed Representation”。两种流派在另一篇文章<a href="词嵌入算法.md">《词嵌入算法》</a>中被讨论过。</p><p>​        关于如何挖掘语言信息也有两种方向，一种是基于统计概率的，依据的假设是“不同phrase的分布不同”，方法是统计不同目标对的出现频率，借助统计意义表达信息。另一种是在频率统计的基础上加入了语境信息，是一种较为高级的挖掘方向，依据的假设是“不同语境携带了不同的信息”，方法是利用目标词的语境作为输入信息的一部分。这两种方向都有各自的依据，都能得到部分预料信息。        </p><p>​        Matrix Factorization的意思是Matrix指的是”共现矩阵“，关于共现矩阵，有几种形式，一种是”term-term”，矩阵的行列对应相应的words,矩阵的值为word-pair共同出现的次数。一种是“term-document”,矩阵的行代表words，列对应不同的document(我更愿意理解为context)，矩阵值对应word-document共同出现的次数。前者的代表是HAL(Hyperspace Analogue to Language)。后者的代表是LSA(Latent Semantic Analysis)。</p><p>​        利用共现矩阵统计的”全局信息“（global）来存储语料中的信息，然后对共现矩阵进行降维。降维的原因是共现矩阵是一个巨大的稀疏矩阵，处理起来不是很友好。这一步叫做“Factorization”。这种表示方法称为“distributional representation”，本质是利用了语料库的统计信息。</p><p>​        由于只用到了统计信息，因此该中表示方法存在的缺陷是明显的：</p><ul><li>无法表达词向量之间的关系(相似性)</li><li>共现矩阵随着语料库的扩增而扩增，而扩增的程度是指数的。</li><li>有很强的稀疏性，不符合语言的特性。</li><li>复杂度是O($|V|^2$)</li></ul><p>​        Local Context Window的典型代表是word2vec，是指利用滑动窗口遍历语料库，经过神经网络训练得到一种词向量表示方法，窗口的大小对于训练结果有影响。这种方法的缺点是</p><ul><li>没有利用语料库的统计信息、</li></ul><p>​        两种表示方法的共同点是最终的得到词向量的维度都不具备可解释性，虽然共现矩阵的每个维度都有意义，但是经过SVD之后，每个维度的含义也都变得混乱了。</p><p>​        GloVe的想法是既然从这两种角度都可以挖掘到词向量的部分信息，但是显然都挖掘到的信息都是不完整的，那么将这两种方式结合起来或许可以挖掘到更强的word representation.</p><h3 id="构建共现矩阵"><a href="#构建共现矩阵" class="headerlink" title="构建共现矩阵"></a>构建共现矩阵</h3><p>​        根据语料库（corpus）构建一个共现矩阵（Co-ocurrence Matrix）XX（什么是<a href="http://www.fanyeong.com/2017/10/10/word2vec/" target="_blank" rel="noopener">共现矩阵</a>？），<strong>矩阵中的每一个元素XijXij代表单词ii和上下文单词jj在特定大小的上下文窗口（context window）内共同出现的次数。</strong>一般而言，这个次数的最小单位是1，但是GloVe不这么认为：它根据两个单词在上下文窗口的距离dd，提出了一个衰减函数（decreasing weighting）：decay=1/ddecay=1/d用于计算权重，也就是说<strong>距离越远的两个单词所占总计数（total count）的权重越小</strong>。</p><blockquote><p>In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count.</p></blockquote><p>摘自：<a href="http://www.fanyeong.com/2018/02/19/glove-in-detail/" target="_blank" rel="noopener">http://www.fanyeong.com/2018/02/19/glove-in-detail/</a></p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>定义notation:</p><p>$X_{i,j}:$ 单词j出现在语境i下的频次。</p><p>$X_i=\sum_kX_{i,k} :$ 所有在语境i下出现的目标单词的总个数。</p><p>$P_{i,j} = P(j|i) = \frac{X_{i,j}}{X_i}:$单词j出现在语境i中的概率。</p><p> 作者定义了一个等式：</p><script type="math/tex; mode=display">F(w_i,w_j,w_k) = \frac{P_{i,k}}{P_{j,k}}</script><p>​        首先谈一下对这个等式的理解。我想作者的目的是希望通过一个函数将词向量和共现矩阵里的统计信息建立联系。因为从某种角度来说，soft-max也是通过某种方法将词向量与概率联系起来，两者具有相似的思想。或者说，作者认为一个比较好的向量表示应该能够反映其在预料库中的统计信息。</p><p>​        如果上式能够成立，那么要做的有两件事，一件是找到一个映射函数F,另一件是训练词向量的表示方式，等式右边的值可以通过共现矩阵得到，因此可以作为一个标签。</p><p>​        首先是确定函数F。下面的解释是借鉴了网上的一篇博客<a href="（十五）通俗易懂理解——Glove算法原理 - 梦里寻梦的文章 - 知乎 https://zhuanlan.zhihu.com/p/42073620">GloVe</a>：</p><p>​        1、$\frac{P_{i,k}}{P_{j,k}}$反映了词向量$(w_i,w_j,w_k)$之间的相似关系，如果单独考虑两个词向量之间的相似关系，那么可以用$|w_i-w_j|$，因此F的形式可以是：$F(w_i-w_j,w_k) = \frac{P_{i,k}}{P_{j,k}}$.</p><p>​        2、$\frac{P_{i,k}}{P_{j,k}}$是一个标量，而自变量是两个同维度的向量，因此可以将自变量进行内积，将自变量转为标量$F((w_i-w_j)^Tw_k) = \frac{P_{i,k}}{P_{j,k}}$,再做一步变换：$F(w_i^Tw_k-w_j^Tw_k) = \frac{P_{i,k}}{P_{j,k}}$.</p><p>​        3、到此为止，等式作为是差右边是商，可以将F取exp变换将差和商联系起来。</p><script type="math/tex; mode=display">exp(w_i^Tw_k-w_j^Tw_k) = \frac{exp(w_i^Tw_k)}{exp(w_j^Tw_k)} = \frac{P_{i,k}}{P_{j,k}}</script><p>​        4、现在只要让分子和分母分别相等就可以了</p><script type="math/tex; mode=display">exp(w_i^Tw_k) = P_{i,k}\\w_i^Tw_k = log(P_{i,k}) = log(X_{i,k})-log(X_i)\\w_j^Tw_k = P_{j,k} = log(X_{j,k})-log({X_i})</script><p>​        5、作为向量，交换$w_i$和$w_k$的顺序值是不变得，但是等式右边显然不适用这个性质，因此将模型引入两个偏置项：</p><script type="math/tex; mode=display">log(X_{i,k}) = w_i^Tw_k+b_i+b_k</script><p>这里是$log(X_i)$消失了？   其实是将$log(X_i)$放到了偏置项$b_i$内。</p><p>​        6、上面的等式在实际中只能近似，因此就有了代价函数(cost function):</p><script type="math/tex; mode=display">J = \sum_{ik}(w_i^Tw_k +b_i+b_k-logX_{ik})^2</script><p>​        7、最后为了提高高频词对损失函数的贡献程度，可以根据两个词共同出现的次数设计一个权重想来对代价函数每一项进行加权：</p><script type="math/tex; mode=display">J = \sum_{ik}f(X_{ik})(w_i^Tx_k+b_i+b_k-log(X_{i,k}))^2</script><p>​        8、权重函数f(x)</p><script type="math/tex; mode=display">\begin{align}f(x) = \{ \begin{array}{ll}(\frac{x}{x_{max}})^\alpha, if\quad x<x_{max}\\1, \quad otherwise\end{array}\end{align}</script><p>这里$\alpha$和word2vec一样取$\frac{3}{4}$</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>​        虽然很多人声称GloVe是一种无监督（unsupervised learing）的学习方式（因为它确实不需要人工标注label），但其实它还是有label的，这个label就是公式2中的log(Xij)log⁡(Xij)，而公式2中的向量ww和~ww~就是要不断更新/学习的参数，所以本质上它的训练方式跟监督学习的训练方法没什么不一样，都是基于梯度下降的。具体地，这篇论文里的实验是这么做的：<strong>采用了AdaGrad的梯度下降算法，对矩阵XX中的所有非零元素进行随机采样，学习曲率（learning rate）设为0.05，在vector size小于300的情况下迭代了50次，其他大小的vectors上迭代了100次，直至收敛。</strong>最终学习得到的是两个vector是ww和~ww~，因为XX是对称的（symmetric），所以从原理上讲ww和~ww~是也是对称的，他们唯一的区别是初始化的值不一样，而导致最终的值不一样。所以这两者其实是等价的，都可以当成最终的结果来使用。<strong>但是为了提高鲁棒性，我们最终会选择两者之和w+~ww+w~作为最终的vector（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）。</strong>在训练了400亿个token组成的语料后，得到的实验结果如下图所示：</p><p><img src="/2020/06/16/NLP/Glove/GloVe/glove.jpg" alt="glove"></p><p>这个图一共采用了三个指标：语义准确度，语法准确度以及总体准确度。那么我们不难发现Vector Dimension在300时能达到最佳，而context Windows size大致在6到10之间。</p><p>摘自：<a href="http://www.fanyeong.com/2018/02/19/glove-in-detail/" target="_blank" rel="noopener">http://www.fanyeong.com/2018/02/19/glove-in-detail/</a></p><h3 id="网上关于GloVe的一个问题："><a href="#网上关于GloVe的一个问题：" class="headerlink" title="网上关于GloVe的一个问题："></a>网上关于GloVe的一个问题：</h3><p> <a href="https://www.zhihu.com/question/292482891/answer/492247284" target="_blank" rel="noopener">https://www.zhihu.com/question/292482891/answer/492247284</a></p><p>GloVe的损失函数：</p><script type="math/tex; mode=display">loss = \sum_{w_i,w_j}(<v_i,\hat{v_j}>+b_i+\hat{b_j}-logX_{i,j})^2</script><p>在glove模型中，对目标词向量和上下文向量做了区分，并且最后将两组向量求和，得到词向量。模型中最大的问题在于参数$b_i,b_j$也是可训练的参数，这会带来一些问题：</p><script type="math/tex; mode=display">\sum_{w_i,w_j}(<v_i,\hat{v_j}>+b_i+b_j-logX_{i,j})^2 \\=\sum_{w_i,w_j}[<v_i+c,\hat{v_j}+c>+(b_i-<v_i,c>-\frac{|c|^2}{2})\\+(\hat{b_j}-<\hat{v_j},c>-\frac{|c|^2}{2})-logX_{i,j}]^2</script><p>就是说这个方程的解不止一个，对与一个解向量加上任意一个常数向量后，它还是这个损失函数的解，原因是偏置因子$b_i$和$\hat{b_j}$的存在。</p><p>有人在评论里提出使用正则化来解决这个问题。</p><p>也有人说对一次更新都使用标注化，把词向量的模限制在1.</p><h3 id="GloVe和其他模型-的联系"><a href="#GloVe和其他模型-的联系" class="headerlink" title="GloVe和其他模型 的联系"></a>GloVe和其他模型 的联系</h3><p>作者在论文中指出，soft-max模型也可以转换成Glove的形式。</p><script type="math/tex; mode=display">soft-max : Q_{ij} = \frac{exp(w_i^Tw_j)}{\sum_{k=1}^Vexp(w_i^Tw_k)}</script><p>那么损失函数可以写成：</p><script type="math/tex; mode=display">J = -\sum_{i\in corpus,j\in context(i)} logQ_{ij}</script><p>由于根据经验，高频词对于损失的贡献程度更大，因此：</p><script type="math/tex; mode=display">J = -\sum_{i=1}^V\sum_{j=1}^V X_{i,j} logQ_{i,j}</script><p>注意，这里将j的取值范围定义为了整个语料库，并且加上了权重$X_{ij}$，也就是说对于没有出现过的词对$X_{ij}$，这一项被置为了0。作者在原文说这样近似更有效率。</p><p>根据定义</p><script type="math/tex; mode=display">X_i = \sum_{k}X_{ik}\\P_{ij} = X_{ij}/X_i</script><script type="math/tex; mode=display">J = -\sum_{i=1}^VX_i\sum_{j=1}^VP_{ij}logQ_{ij} = \sum_{i=1}^VX_iH(P_i,Q_i)</script><p>这里的$H(P_i,Q_i)$是交叉熵。这个损失也就变为了某种形式的交叉熵损失。</p><p>交叉熵是 一种概率分布之间的距离衡量方式，但是它有一个不好的性质就是会对poorly events 分配不小的权重，因此为了解决这个问题，需要Q采用合适的标准化手段。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;GloVe-Global-Vectors-for-Word-Representation&quot;&gt;&lt;a href=&quot;#GloVe-Global-Vectors-for-Word-Representation&quot; class=&quot;headerlink&quot; title=&quot;GloV
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>语言模型</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/16/NLP/%E4%BB%80%E4%B9%88%E6%98%AF%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E4%BB%80%E4%B9%88%E6%98%AF%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/16/NLP/%E4%BB%80%E4%B9%88%E6%98%AF%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E4%BB%80%E4%B9%88%E6%98%AF%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-06-15T16:00:00.000Z</published>
    <updated>2020-06-16T11:28:37.972Z</updated>
    
    <content type="html"><![CDATA[<h3 id="当我们在讨论语言模型的时候，我们讨论的是什么？"><a href="#当我们在讨论语言模型的时候，我们讨论的是什么？" class="headerlink" title="当我们在讨论语言模型的时候，我们讨论的是什么？"></a>当我们在讨论语言模型的时候，我们讨论的是什么？</h3><p>​      从字面意思上理解，语言模型（language model）无非是对语言的一种建模。那么对语言建模的目的是什么？为什么要对语言进行建模呢？</p><p>​    回答这个不难，因为无论是机器翻译还是语音识别，所有的NLP领域都绕不开一个问题，那就是“到底什么是人话？”。一段文本，一段音频，我们将其称之为“信号”，是否符合人类语言特征，是否能够让人看得懂，听得懂？这个是所有NLP任务的基本问题，能够很好地回答了这个问题，才能够继续做下一步。</p><h3 id="为什么语言模型很重要？"><a href="#为什么语言模型很重要？" class="headerlink" title="为什么语言模型很重要？"></a>为什么语言模型很重要？</h3><p>有一个概念是这样讲的：</p><p> Noisy Channel Model:</p><p>机器翻译，语音识别，拼写纠错，OCR，密码破解等NLP任务都有一个相似之处，那就是问题的形式都是给定一段信号，然后预测在给定信号下的正确的输出。形式化描述：</p><script type="math/tex; mode=display">P(text|source) \propto P(source|text)P(text)</script><p>分母上的$P(source)$被省略了，因为对于给定的source，$P(source)$是固定的，我们只讨论在给定source情况下的text正确性的问题。</p><p>例如：</p><p>​    机器翻译： 假设英文翻译成中文。</p><script type="math/tex; mode=display">obj =argmax_{中文} P(中文|英文)</script><p>根据上面的NCM模型，可以有：</p><script type="math/tex; mode=display">obj = argmax P(英文|中文)P(中文)</script><p>​    模型$P(英文|中文)$负责中英文间的映射，这个最简单的情况下可以由一个字典完成，模型$P(中文)$就是语言模型， 用来控制输出符合语言特征。</p><p>​    比方说，对于“The brown fox skip over the lazy dog.”映射模型可以完成单词间的逐一映射，但是如何组合才符合中文的说法习性，就需要语言模型来控制。</p><h3 id="如何解决？"><a href="#如何解决？" class="headerlink" title="如何解决？"></a>如何解决？</h3><p>​    然而要解决这个问题并不简单，原因是多方面的：</p><ul><li><p>1、语言天生的歧义和多变性。多变性是指一种含义可以有很多种方式来说，也可以指一种说法可以有很多种含义。</p></li><li><p>2、语言的规则不好定义。(ill-defined).很难有一套规则把某种语言给定义好，所谓的定义好就是指语言的表达模式完全遵从某种规则。对于中文来讲，尽管我们有标准的语法，但是在现实中一句话可能是语法的各种组合，或者我们讲话写作不用按照标准的语法也可以表达意思。</p></li><li><p>3、语义是无穷尽的，不同的单词可以组成不同的句子，不同的句子又可以表达不同的意思，这个组合没有上限。更何况人类的语言库一直在更新。因此，训练样本无法穷尽所有的情况。</p></li><li><p>4、解析语义是一个组合问题。单词-&gt;句子-&gt;文章/新闻/事件。复杂度骤增。</p></li><li>5、最后一个没有理解到位，NLP问题具有sparseness(稀疏性)?</li></ul><p>业界对语言模型的一个定义是：</p><p>Assigning a probability to sentences or sequence of words in a language.</p><p>不仅要能够给一句话的出现概率，还要能够给出，给定语境下的目标单词的条件概率。</p><p>上述两个问题等价于一个问题。根据条件概率公式：</p><script type="math/tex; mode=display">p(w_{1:n}) = p(w_1)p(w_2|w_1)p(w_3|w_{1:2})...p(w_n|w_{1:n-1})\\p(w_{1:n-1}) = p(w_1)p(w_2|w_1)p(w_3|w_{1:2})...p(w_{n-1}|w_{1:n-2})\\p(w_n|w_{1:n-1}) = \frac{p(w_{1:n})}{p(w_{1:n-1})}</script><p>假设我们知道每个句子的概率，那我们也可以知道给定序列下每个词的出现概率。</p><h4 id="传统的方法是采用马尔科夫假设。"><a href="#传统的方法是采用马尔科夫假设。" class="headerlink" title="传统的方法是采用马尔科夫假设。"></a>传统的方法是采用马尔科夫假设。</h4><p>我们对后面一个问题应用马尔科夫假设，准确地说是kth order markov-assumption。</p><p>即目标单词的出现概率仅与之前出现的前k个单词有关。</p><script type="math/tex; mode=display">P(w_{i+1}|w_{1:i}) \approx P(w_{i+1}|w_{i-k:i})</script><p>根据上面的公式：</p><script type="math/tex; mode=display">P_{MLE} (w_{i+1} = m|w_{i-k:i}) = \frac{\#(w_{i-k:i+1})}{\#(w_{i-k:i})}</script><p>至此，我们就得到了一个baseline模型。</p><p>但是这个baseline模型有一个缺点，就是对于没有观察到的句子，这个概率是无穷大。因此，又有人在这个基础上做了平滑。</p><script type="math/tex; mode=display">p_{add-\alpha} (w_{i+1} =m|w_{i-k:i})=\frac{\#(w_{i-k:i+1})+\alpha}{\#(w_{i-k:i})+\alpha|V|}</script><p>但是传统的语言模型有很多问题：</p><p>​    最大的缺点是假设太强了，真实情况只能部分符合该假设，比如“喝水”，“吃饭”等等（k=1）。其它情况下则不符合，比如：“Tom喝了好几杯水，因为他太渴了。”，这里的“渴”明显与“喝水”这一动作相关，但是由于词序靠后，因此不能被模型很好的分辨出来。</p><p>​    另外，对于k的选择是含糊的，k=1好还是k=2好，k到底怎么选？这需要具体情况具体分析，并没有一个一劳永逸的方法。小的K值对于corpus训练是可以接受的，但是大的k值会导致内存不够用，对于一个含有V个单词的词典，可能的kgrams有$|V|^{k}$种，k每增加一都会导致指数级的增长（尽管有些kgram的组合在现实不会出现或者不符合语法）。</p><p>但是从历史的发展来看，应用这个假设诞生的模型，在某些场景下，还是可以得到不错的效果。</p><h4 id="业界目前比较火的构造方法是采用神经网络"><a href="#业界目前比较火的构造方法是采用神经网络" class="headerlink" title="业界目前比较火的构造方法是采用神经网络"></a>业界目前比较火的构造方法是采用神经网络</h4><p>​    神经网络天生具有高效的学习能力，03年bengio提出使用神经网络构造语言模型，取得了不错的效果。</p><p>​    后来业界提出了使用RNN网络来对时间序列建模之后，将神经网络在NLP领域的运用推上了顶峰。</p><p>​    再后来出现了GRU和LSTM（复兴）等等。</p><p>神经网络的优点：</p><p>​    非线性神经网络模型解决了传统模型的缺点，主要是k值的限制。在神经网络中，k值增加是线性的，而非指数的。所谓的线性是指用于参数数量成线性关系。通俗来讲就是，为了能够使用更多的前缀内容，我可以通过增加参数来达到这个目的。也避免了手动设置”bakkoff order”(这个概念理解的不是很好，)</p><p>​    其实神经网络不止这一个优点，神经网络代替传统模型的几个方面：</p><p>​    上述是一个方面。二是，神经网络不要手动的combine特征，模型会帮我们自动提取出有用的特征，这也是表示学习的一大特点。三是，神经网络解决了上下文的问题，传统的模型假设限制了模型利用下文的信息，而双向神经网络即可以利用上文信息，也可以利用下文信息。这使得模型预测更加符合实际情况。第四点是，门控神经网络的诞生解决了模型对单词出现位置（时间序列）的依赖问题，更容易挖掘关联信息。</p><p>​    这些技术总体来说都是逐步放宽模型的假设，使得模型更贴近现实。</p><p>神经网络的限制：</p><p>​    神经网络对于预测context中的某个词的出现概率成本相对传统模型比较高。但是泛化效果不错，但是某种场景下，这种泛化效果会带来不好的影响，书中给出了一个例子，比如预测“red house”的出现概率，传统模型可能会给一个比较低的概率，因为不常见。但是神经网络可能观察到了“black house,blue house”等等，因为基于它的泛化能力反而会给出一个比较高的概率，这是不希望看到的。因为在显示中，红房子确实不多见，但是白房子，黑色城堡可能比较多一点。</p><p>​    语言模型从传统过渡到神经网络经历了一个漫长的过程，而且并不是传统的一定就是不好的，在某些场景下，传统语言模型反而比神经网络表现地更好。</p><p>​    有没有更好地建模方法？不知道。神经网络说到底是基于统计概率的，不具备良好的可解释性。而基于规则的方法虽然具备良好的可解释性，但是效果却不尽人意。有没有能够兼顾两者的模型可能还需要一个数年的探索。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;当我们在讨论语言模型的时候，我们讨论的是什么？&quot;&gt;&lt;a href=&quot;#当我们在讨论语言模型的时候，我们讨论的是什么？&quot; class=&quot;headerlink&quot; title=&quot;当我们在讨论语言模型的时候，我们讨论的是什么？&quot;&gt;&lt;/a&gt;当我们在讨论语言模型的时候，我们讨
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Unigram,Bigram,Ngram</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/16/NLP/k-gram/Unigram,Bigram,Ngram/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/16/NLP/k-gram/Unigram,Bigram,Ngram/</id>
    <published>2020-06-15T16:00:00.000Z</published>
    <updated>2020-06-22T12:52:15.571Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Unigram-Bigram和N-gram"><a href="#Unigram-Bigram和N-gram" class="headerlink" title="Unigram, Bigram和N-gram"></a>Unigram, Bigram和N-gram</h2><p>​        这三种模型都是基于kth order markov-assumption下k分别取1,2,n时的情况，是传统模型中最基本的模型。下面主要分析一下这三种模型的不足，同时也是传统模型的不足。</p><h3 id="Unigram"><a href="#Unigram" class="headerlink" title="Unigram"></a>Unigram</h3><p>核心：将每个单词视为独立的个体.</p><p>那么：</p><script type="math/tex; mode=display">P(w_1,w_2,w_3,...,w_n) = p(w_1)p(w_2)p(w_3)...p(w_n)</script><p>上面就是我们的模型，训练过程就是统计每个单词$p(w_i)$的出现概率。</p><p>至于怎么得到每个单词的概率，就需要给定一个语料库，在语料库中计算每个单词的频率。</p><p>预测过程：</p><p>将需要预测的样本带到模型中，得到每个单词频率的乘积，即为句子出现的概率。</p><p>可以看出这种方法虽然简单，但是假设太强，它的假设是每个单词之间相互独立，即一句话中的单词A出现的概率是完全随机的，因为单词之间都彼此独立。显然这不合理。</p><h3 id="Bi-gram"><a href="#Bi-gram" class="headerlink" title="Bi-gram:"></a>Bi-gram:</h3><p>核心：每个单词的出现依赖前一个单词。</p><p>模型表示:</p><script type="math/tex; mode=display">P(w_1,w_2,w_3,...,w_n) = p(w_1)p(w_2|w_1)p(w_3|w_2)...p(w_{n-1}|w_n)</script><p>模型的训练过程就是统计如下概率：</p><script type="math/tex; mode=display">p(w_i|w_{i-1}) = \frac{p(w_i,w_{i-1})}{p(w_{i-1})} = \frac{$(w_{i-1},w_i)}{$w_{i-1}}</script><p>同样是在语料库中统计。</p><p>预测过程：</p><p>将需要预测的样本带到模型中，各个条件概率的的乘积，即为句子出现的概率。</p><p>该模型虽然在Unigram的基础上放宽了部分假设，即假设下一个单词的出现与上一个单词有相关性。但是假设依然很强，因为没有考虑语义上的相关，比方说”the man”，这两个单词之间的相关性可能并不像他表现出来的那么强。其次，模型也没有考虑超过2个词距的单词对目标词的影响。这些都是限制模型表现地因素。</p><h3 id="n-gram：假设n-3"><a href="#n-gram：假设n-3" class="headerlink" title="n-gram：假设n =3"></a>n-gram：假设n =3</h3><p>核心：每个目标词的出现都依赖于前N个出现的单词。</p><p>模型表示：</p><script type="math/tex; mode=display">P(w_1,w_2,w_3,...w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)P(w_4|w_2,w_3)...P(w_n|w_{n-1},w_{n-2})</script><p>模型训练过程就是统计如下概率：</p><script type="math/tex; mode=display">P(w_i|w_{i-k:i}) = \frac{P(w_i,w_{i-k:i})}{P(w_{i-k:i})} = \frac{$(w_i,w_{i-k:i})}{$w_{i-k:i}}</script><p>预测过程：</p><p>将需要预测的样本带到模型中，各个条件概率的的乘积，即为句子出现的概率。</p><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>​        可以看出Bi-gram和N-gram模型都是在Unigram的基础上逐渐放宽假设的结果。下面主要讨论三种模型的共同的一些问题：</p><p><strong>问题1</strong>：</p><p>​        一个显著的问题就是对于语料库中没有出现，而预测样本中出现了的单词，模型会表现出糟糕的预测结果。即对于语料库中没有出现的单词，模型的预测结果会取0。这显然是不符合实际情况的。</p><p>一个因此普遍的做法是对模型进行平滑处理。</p><p>additive smoothing ：</p><script type="math/tex; mode=display">P_{add-\alpha} (w_{i+1 }=m|w_{i-k:i}) = \frac{$(w_{i-k:i+1})+\alpha}{$(w_{i-k:i})+\alpha|V|}</script><p>$\alpha$取（0,1]之间的值。</p><hr><p>back-off smothing:</p><script type="math/tex; mode=display">P_{int}(w_{i+1} = m|w_{i-k:i}) = \lambda_{w_{i-k:i}}\frac{$(w_{i-k:i+1})}{$(w_{i-k:i})}+(1-\lambda_{w_{i-k:i}})p(w_{i+1}=m|w_{i-(k-i):i})</script><p>​        该平滑原理是利用其backoff进行退步统计，就是说对于$P(w_{i+1}|w_{i-k:i})$如果语料库中没有出现$w_{i-k:i}$，那么就使用$w_{i-k:i-1}$来代替。</p><p><strong>问题2：</strong></p><p>​        没有利用到下文的信息。有些目标词是依赖下文的，对上文依赖反而不强，但是马尔科夫假设限制了这一点。这也是限制传统模型表现的一个重要因素。</p><p><strong>问题3：</strong></p><p>​        无法衡量序列之间的相似性。比方说“我今天喝了很多水。”与“我今天喝水快喝饱了。”这两个句子都是在说“喝了很多水”。但是模型却没有衡量这两个序列相似性的能力。</p><p>​        一个解决方法是引入词向量。这个话题可以单独写一篇，关于如何衡量序列之间相似性。</p><p><strong>问题4：</strong></p><p>​        对于n的选择是一个trade-off问题。（当采用线性模型时，这个问题尤为严重。因为K每增加1，相应的假设空间就要增大$|V|^{k+1}$-$|V|^{k}$。这说明，参数也要增加这么多。）当然，这里是基于统计的方法来训练的，这样的训练方式对应于一个大的n，就可能会出现大量的0，因为要统计的序列越长，相当于统计n个词同时出现的频次，这个概率一般较小。相反n较小，最好是一个单词，在语料库中出现的频次会越大。</p><p><strong>问题5：</strong></p><p>​        对于统计的模型的一个天然的问题是缺乏泛化能力。比方说“我喝了杯水”和“我喝了一杯水”这两个序列的出现概率是不同的，而且很可能是很不相同的，因为单词$P(“一”|w_{i-k:i})$的概率可能很小也可能很大，总之一字之差，甚至意思上也没有变化，然而却可能得到相差很大的出现概率。</p><p><strong>问题6：</strong></p><p>​        对于有些文本，目标词的出现可能只依赖于它前面出现的第i-k个单词，像is,was等等。而对其他单词都没有依赖，这种情况下对它前面出现的连续n个单词建模可能是不如人意的。这种情况，传统模型也束手无策。</p><p>​        总结来说，基于马尔科夫假设的传统模型本质上是基于概率统计的，甚至没有真正意义上的训练过程。这种基于统计的模型具有各种各样的缺点，直到神经网络的引入，部分问题才得到了相应地改善。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Unigram-Bigram和N-gram&quot;&gt;&lt;a href=&quot;#Unigram-Bigram和N-gram&quot; class=&quot;headerlink&quot; title=&quot;Unigram, Bigram和N-gram&quot;&gt;&lt;/a&gt;Unigram, Bigram和N-gram
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Word Representation</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/16/NLP/%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA/Word%20Representation/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/16/NLP/%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA/Word%20Representation/</id>
    <published>2020-06-15T16:00:00.000Z</published>
    <updated>2020-06-22T12:53:57.555Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Word-Representation？"><a href="#Word-Representation？" class="headerlink" title="Word Representation？"></a>Word Representation？</h3><p>​        传统的基于概率统计的语言模型，Unigram,Bigram,N-gram没有涉及到word representation,因为他们是基于统计训练的，因此在输入端可以不做textual feature 到input feature的转换。</p><p>​        但对于神经网络，训练过程需要有明确的数学形式的输入，而不是文本，这就引发了一个新的问题，如何实现textual feature 到neural feature的转换？</p><p><strong>One-hot和Eembedding</strong></p><p>一种是One-hot表示方法，一种是Embedding表示方法.</p><p>​        One-hot表示是一种稀疏表示，是将整个词典做为 一个向量，对应位置上的单词为1，其余为0。Embedding是运用分布式假设，将词向量在高维空间嵌入(映射)到低维空间。使其在每个维度上的表示都不至于为0。（即由所有嵌入后的维度共同表出，相比One-hot信息由单一维度表出变为了信息分布在了各个维度上）</p><p>分布式假设：在相似的语境中的目标词，具有相似性，具有相似性的单词，word representation形式相似度应该较高。</p><p><strong>Unigram：</strong></p><p>​        传统的Word representation方法是One-hot,目前流行的表示方法是词嵌入。当然这个只是对单词（最小意义单位）而言。</p><p><strong>N-gram:</strong></p><p>​        我们将N-gram（n&gt;=2）引入，可以发现，输入变为了多个单词（上下文+目标词）。解决方法是combination，combination的方法其实有很多，比较常见的有两种，一种是直接相加，一种是直接叠加。前者的代表是CBOW和WCBOW.</p><script type="math/tex; mode=display">CBOW(w_1,w_2,w_3,...w_k) = \frac{1}{k}\sum_{i}v(w_i)\\WCBOW(w_1,w_2,w_3,...w_k) = \frac{1}{\sum_{i=1}^ka_i}\sum_{i}a_iv(w_i)</script><p>两种叠加方式对两种word representation方法都是适用的。</p><p>​        当使用One-hot表示方法时，一个句子的表示可以由该句子中所有单词的One-hot表示方法相加，即One-hot的CBOW版本。</p><p>比方说：“I like sweat candy.”</p><p>以上句子的表示方法为：$V(S) = O(“I”)+O(“like”)+O(“sweat”)+O(“candy”)$</p><p>O(w)是单词的One-hot表示。</p><p>​        但是这样的表示方法不能够很好地衡量句子之间的相似性，因为它忽略了单词位置（词序）的影响。</p><p>另一种combination方法是叠加法。</p><p>​        当采用window-based时，即考虑窗口内的目标单词的前后文。输入变为了2c个单词。那么叠加法的方法就是将目标词前后各c个单词叠加在一起。</p><p>比方说$\mathbf{(w-2;w-1;w+1;w+2)}$.也可以$\mathbf{(w_{-2}+w_{-1});(w_{+1}+w_{+2})}$.这种在输入到神经网络的时候一般通过一个投射层将特征叠加起来。</p><h3 id="One-hot为什么失败？"><a href="#One-hot为什么失败？" class="headerlink" title="One-hot为什么失败？"></a>One-hot为什么失败？</h3><p>One-hot表示方法在与词嵌入表示方法竞争时失败了。至于原因是多方面的：</p><p>问题1：One-hot表示方法虽然简单，但不够优雅。它的维度是整个字典的长度，因此限制了输入的维度也必须是字典的长度。但是字典的长度一般在10的5次方到10的6次方左右，这也意味输入向量的维度在10的5次到6次方左右，会引入大量的稀疏性，尽管计算机对稀疏矩阵的运算比较快，显然这种表示方法不够好。</p><p>问题2：One-hot是一种硬编码，它只在对应的位置上有信息，而在其他位置上没有信息，这就造成当计算两个词向量之间相似性的时候，会出现问题。因为彼此都是正交的。</p><p>问题3：One-hot抛弃了位置信息，当输入需要引入前后文的时候，One-hot的做法是词向量直接相加或者叠加，相加的后果是对句子中出现的词进行了统计，可以表达一部分频率上的相似性，但是无法表达语义上的相似性。叠加的后果是进一步引入了稀疏性。</p><p>分布式表示方法没有上述的种种缺陷，因此更容易被业界接受。</p><h3 id="Embedding算法"><a href="#Embedding算法" class="headerlink" title="Embedding算法"></a>Embedding算法</h3><p><strong>关于如何实现词嵌入有两个流派：</strong></p><h4 id="Distributional-Hypothesis"><a href="#Distributional-Hypothesis" class="headerlink" title="Distributional Hypothesis"></a>Distributional Hypothesis</h4><p>​        NLP community 认为出现在相同语境中的单词具有相同的意思。（这个被称为<strong>Distributional Hypothesis。</strong>）基于此，可以根据单词在不同语境中的分布作为词向量来表示单词。这样出现在相同语境中的单词就会有相似的表示了。具体的操作如下：</p><p>​        假设存在一个词典集合和一个”语境集合“，这两个集合都是可索引的，$w_i$表示第i个单词，$c_j$表示第j种语境。（这里的词典集合和语境集合可以从一个大的语料库中得到，可以想象两个集合都会比较大。)存在一个矩阵实体$M_{[i,j]}]$，它的每一行代表字典集合中的一个单词，每一列代表一个语境集合中的语境，每一个元素$f(i,j)$代表单词$w_i$和语境$c_j$的association，这种association一般是通过统计$\mathbf{$}(w_i,c_j)$在语料库中的出现频次，或者标准化以后的形式$\frac{\mathbf{$}(w_i,c_j)}{|D|}$.|D|表示语料库的大小，即：</p><script type="math/tex; mode=display">f(i,j) =\frac{$(w_i,c_j)}{|D|}</script><p>​        但是这种表示存在一个问题，就是对于更常出现的word-context具有更高的权重（频次），比如：假设语境为目标词的前一个单词，那么”the cat”和”a cat”就要比”cute cat”和”small cat”的权重高的多，尽管后面的事件携带了更多的信息。也就是说对于出现不是那么频繁但是更有用的word-context，这种表示方法会比加的不公平。为了克服这种缺陷，提出了Pointwise Mutual Information(PMI)：</p><script type="math/tex; mode=display">PMI(x,y) = log\frac{P(x,y)}{P(x),p(y)}</script><p>在word-contex场景下，PMI测量的是他们共同出现的频率除以他们各自单独出现的频率。</p><script type="math/tex; mode=display">f(w,c) = PMI(w,c) = log\frac{$(w,c)·|D|}{$(w)·$(c)}</script><p>​        PMI和TF-IDF的思想相似，采用PMI可以避免给出现频次大的word-context更多的权重，给出现频次小的word-context的更小的权重。但是引入PMI表示仍然有些问题，对于没有出现在语料库中的word-context对，在计算PMI是会出现log(0)的现象。</p><p>因此对PMI加以改进，提出PPMI(positive PMI):</p><script type="math/tex; mode=display">PPMI(w,c) = max(PMI(w,c),0)</script><p>将低频出现的word-context给忽略掉（赋值为0）。</p><p> PMI在实际运行中表现良好，但是也存在一个比较明显的缺点：</p><p><strong>那就是容易对rare event赋予比较高的权重，尤其是当两个事件发生的频次都很低，又恰好发生在了一起的时候。</strong></p><p>​        采用Distributional Hypothesis假设的一个问题是，字典集合和语境集合都很大，他们都是从一个大的语料库中得到的，因此造成实体矩阵M很大。并且由于很多word-context对不会出现在语料库中，这又会造成矩阵很稀疏。解决方法是对实体矩阵进行降维。可以采用经典的降维方法比如SVD.这样，通过降维后的实体矩阵维度会变得比较容易让人接受，同时每个词向量又可以被表出。通过SVD降维后的词向量维度大约在50&lt;d&lt;300之间。</p><p>​        到此为止，经典的采用Distributional Hypothesis假设的方法介绍完毕。由于向量的表示是基于统计，准确地说是基于word-context数量的表示，因此也被成为count-based method.</p><p>问题：语境集合如何构造？是选取目标的前一个词还是前n个词，还是用目标词的上下文？</p><h4 id="Distributed-Representation"><a href="#Distributed-Representation" class="headerlink" title="Distributed Representation"></a>Distributed Representation</h4><p>​        相对于NLP派推崇的Distributional Hypothesis，神经网络派则比较推崇Distributed Representations。那么什么是Distributed representation?和Distributional Hypothesis的不同是什么？</p><p>​        在分布式表示（Distributed representation）中，一个单词的意思被多个维度共同表出。每个维度没有具体的含义，即不具可解释性（对比count-based可见，每个维度对应一个确定的语境）。分布式表示（Distributed）的本质是给定一个meaning（word）,它可以被多个维度的combination捕捉（表示），或者给定一个维度，它捕捉了meaning的一个aspect。</p><p>​        打个比方：”国王”。“国王”的属性有哪些呢？首先国王是个人，而且一般是个男人，“男人”可以作为“国王”的一个属性。其次，“国王”一般是个成年人，因此“成年人”也可以作为“国王”的一个属性，再来，从词性的角度来讲，“国王”是一个名词，因此“名词”也可以作为“国王”的一个属性。这么表示出来“国王”可以由[“男人”，“成年人”，‘名词“]来表示，每个属性都是一个维度，每个维度对应一个数值，用数值表示出来就是”国王“的词向量。</p><p>​        但是对于分布式表示来说，每个维度不具有可解释性，即给定一个分布式表示的词向量，每个维度所表示的含义无法获知。但是这个并不影响我们使用它们，即使不知道它们每个维度的所代表的的含义，一旦给出两个词向量表示，我们仍然可以用标准的相似度计算方法来计算。</p><p>​        分布式表示的另一个特点是需要训练。先让我们来看一下词嵌入在神经网络中的形式。</p><script type="math/tex; mode=display">\hat{y} = P(w_i|w_{1:k}) = LM(w_{1:k}) = softmax(hW^2+b^2)\\h = g(xW^1+b^1)\\x = [v(w_1);v(w_2);...;v(w_k)]\\v(w) = \mathbf{E}_{[w]}\\w_i \in V ,\mathbf{E}\in R^{(|V|,d_w)},W^1 \in R^{k·d_w,d_{hid}},b^1 \in R^{d_{hid}},W^2 \in R^{d_{hid},V},b^2 \in R^{|V|}</script><p>​        在这个模型中一共有两套参数，一个是$\mathbf{E}$,另外一个是$\mathbf{[W^1,W^2]}$.其中$\mathbf{E}$是词嵌入矩阵，因此我们可以通过训练模型来得到词嵌入矩阵。可以看到$\mathbf{W^2}$的列和$\mathbf{E}$的行都是一种distributed representation表示。训练过程保证了词嵌入矩阵的值足够好，因为对于一个有着k-gram的输入，他们产生了正确的预测概率（对于指定的预测目标）。通过网络的训练过程，我们可以得到最终的词嵌入矩阵。这是大多数distributed representation的生成方式。</p><p>语言模型有两个功能需求，给定一个语境，推测目标词的分布。另一种：</p><p>the need to condition on contexts that can be combined using the chain-rule of probability to produce sentence level probability estimates  </p><p>这个我怕翻译不准，个人理解是给定一个词向量，利用链式法则去预测句子的生成概率。如下图所示（摘自word2vec论文）：</p><p><img src="/2020/06/16/NLP/%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA/Word%20Representation/distributed_representation.PNG" alt="distributed_representation"></p><p>以上是distributed representation的一个框架，具体的训练方法有几种。</p><p><strong>Collobert and Weston</strong></p><p>这两个人做了两点创新：</p><p>​        一个是改变了传统的使用k-gram的假设，引入了word-window，将使用前k个gram改为了使用目标词的上下文（这个贡献是他们俩人做的）。使用$P(w_3|w_1,w_2,w_4,w_5)$代替了$P(w_5|w_1,w_2,w_3,w_4)$。</p><p>​        另一个是改变了输出的形式，传统的softmax方案（语言模型的输出）是输出字典中每个单词作为目标词的概率。而如果只是注重representation的生成，那么可以适当对输出进行更改，不是输出每个目标词的概率分布，而是输出一个分数（score）。</p><p>​        具体的实现方式如下：对于一个目标单词w, $c_{i:k}$为它的上下文的序列。$V_w(w)$为词嵌入函数，$V_c(c)$为context嵌入函数。这两个函数将目标词和context映射到一个相同的维度$d_{emb}$.Collorbert 和Weston对模型输入一个$（w,c_{1:k}）$对，然后将这个word-context对结合成一个输入$\mathbf{x}$作为神经网络的输入，通过单隐含层，输出一个word-context的分数s。</p><script type="math/tex; mode=display">S(w,c_{1:k}) = g(\mathbf{xU})·\mathbf{v}\\\mathbf{x} = [V_c(c_1);....;V_c(c_k);V_w(w)]\\\mathbf{U} \in R^{(k+1)d_{emb},d_h}, \mathbf{v}\in R^{d_h}.</script><p>损失函数为：</p><script type="math/tex; mode=display">L(w,c,w') = max(0,1-(s(w,c_{1:k})-s(w',c_{1:k})))</script><p>​        w’是一个从字典中随机抽出来的单词。训练过程轮循地遍历word-context对，并且对于每一word-context对，随机采样一个单词w’,使用w’计算损失L(w,c,w’)，并且更新$\mathbf{U,v}$和词向量对去最小化损失。这就是随机负采样的思想。</p><p><strong>word2vec</strong></p><p><strong>NCE</strong></p><p><strong>Glove</strong></p><p>等等。每个都够写一章了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Word-Representation？&quot;&gt;&lt;a href=&quot;#Word-Representation？&quot; class=&quot;headerlink&quot; title=&quot;Word Representation？&quot;&gt;&lt;/a&gt;Word Representation？&lt;/h3&gt;&lt;
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://qingfengbangzuo.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow2.0以上cpu版本安装方法</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/15/%E5%85%B6%E4%BB%96/tensorflow2.0%E4%BB%A5%E4%B8%8Acpu%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/15/%E5%85%B6%E4%BB%96/tensorflow2.0%E4%BB%A5%E4%B8%8Acpu%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95/</id>
    <published>2020-06-15T01:27:15.080Z</published>
    <updated>2020-07-06T11:30:55.635Z</updated>
    
    <content type="html"><![CDATA[<p>安装tensorflow cpu 2.0以上版本。</p><p>python2.0已经支持python3.7了。不过我装的还是3.6的pyhon。</p><p>Anaconda的安装教程就不说了，按照网上来就OK了。这里只简单记录下如何安装tensorflow-cpu版本。</p><p>安装命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0: conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;</span><br><span class="line">0: conda config --set show_channel_urls yes  &#x2F;&#x2F;清华镜像 </span><br><span class="line">1: pip install tensorflow-cpu</span><br><span class="line">2: pip install tensorflow-cpu&#x3D;&#x3D;2.1.0 -i https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple</span><br><span class="line"></span><br><span class="line">3: pip install --ignore-installed --upgrade tensorflow  &#x2F;&#x2F;这个是老方法，已经不行了，最新的tensorflow是将cpu-gpu版本绑定在一起的，使用这条命令下载大概率会报错。推荐使用上面两条</span><br></pre></td></tr></table></figure><p>安装完成，测试一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"> </span><br><span class="line">version = tf.__version__</span><br><span class="line"><span class="comment"># pu_ok = tf.test.is_gpu_available()</span></span><br><span class="line">gpu_ok = tf.config.list_physical_devices(<span class="string">"GPU"</span>)</span><br><span class="line">print(<span class="string">"tf version:"</span>, version, <span class="string">"\nGPU number"</span>, gpu_ok)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tf version: <span class="number">2.1</span><span class="number">.0</span> </span><br><span class="line">GPU number[]</span><br></pre></td></tr></table></figure><p>可能会报错，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"</span>, line <span class="number">58</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    <span class="keyword">from</span> tensorflow.python.pywrap_tensorflow_internal <span class="keyword">import</span> * </span><br><span class="line">File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"</span>, line <span class="number">28</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    _pywrap_tensorflow_internal = swig_import_helper()</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"</span>, line <span class="number">24</span>, <span class="keyword">in</span> swig_import_helper</span><br><span class="line">    _mod = imp.load_module(<span class="string">'_pywrap_tensorflow_internal'</span>, fp, pathname, description)</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\imp.py"</span>, line <span class="number">242</span>, <span class="keyword">in</span> load_module</span><br><span class="line">    <span class="keyword">return</span> load_dynamic(name, filename, file)</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\imp.py"</span>, line <span class="number">342</span>, <span class="keyword">in</span> load_dynamic</span><br><span class="line">    <span class="keyword">return</span> _load(spec)</span><br><span class="line">ImportError: DLL load failed: 找不到指定的模块。</span><br><span class="line"> </span><br><span class="line">During handling of the above exception, another exception occurred:</span><br><span class="line"> </span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\site-packages\tensorflow\__init__.py"</span>, line <span class="number">101</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    <span class="keyword">from</span> tensorflow_core <span class="keyword">import</span> *</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\site-packages\tensorflow_core\__init__.py"</span>, line <span class="number">40</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    <span class="keyword">from</span> tensorflow.python.tools <span class="keyword">import</span> module_util <span class="keyword">as</span> _module_util</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\site-packages\tensorflow\__init__.py"</span>, line <span class="number">50</span>, <span class="keyword">in</span> __getattr__</span><br><span class="line">    module = self._load()</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\site-packages\tensorflow\__init__.py"</span>, line <span class="number">44</span>, <span class="keyword">in</span> _load</span><br><span class="line">    module = _importlib.import_module(self.__name__)</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\importlib\__init__.py"</span>, line <span class="number">127</span>, <span class="keyword">in</span> import_module</span><br><span class="line">    <span class="keyword">return</span> _bootstrap._gcd_import(name[level:], package, level)</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\site-packages\tensorflow_core\python\__init__.py"</span>, line <span class="number">49</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    <span class="keyword">from</span> tensorflow.python <span class="keyword">import</span> pywrap_tensorflow</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"</span>, line <span class="number">74</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    <span class="keyword">raise</span> ImportError(msg)</span><br><span class="line">ImportError: Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"</span>, line <span class="number">58</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    <span class="keyword">from</span> tensorflow.python.pywrap_tensorflow_internal <span class="keyword">import</span> *</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"</span>, line <span class="number">28</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    _pywrap_tensorflow_internal = swig_import_helper()</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"</span>, line <span class="number">24</span>, <span class="keyword">in</span> swig_import_helper</span><br><span class="line">    _mod = imp.load_module(<span class="string">'_pywrap_tensorflow_internal'</span>, fp, pathname, description)</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\imp.py"</span>, line <span class="number">242</span>, <span class="keyword">in</span> load_module</span><br><span class="line">    <span class="keyword">return</span> load_dynamic(name, filename, file)</span><br><span class="line">  File <span class="string">"C:\Programs\Python\tensorflow210_cpu\lib\imp.py"</span>, line <span class="number">342</span>, <span class="keyword">in</span> load_dynamic</span><br><span class="line">    <span class="keyword">return</span> _load(spec)</span><br><span class="line">ImportError: DLL load failed: 找不到指定的模块</span><br><span class="line">————————————————</span><br></pre></td></tr></table></figure><p>原因说不清楚，不过有人给出了解决方法:</p><p><strong>到微<a href="https://visualstudio.microsoft.com/zh-hans/downloads/?q=Microsoft+Visual+C%2B%2B+Redistributable" target="_blank" rel="noopener">软官方网</a>下载最新的Visual C++库的运行组件（链接下方），然后更新，重启，问题解决。</strong></p><p>再把上面的测试代码写一遍就会显示正确信息了。</p><p><img src="/2020/06/15/%E5%85%B6%E4%BB%96/tensorflow2.0%E4%BB%A5%E4%B8%8Acpu%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95/捕获.PNG" alt="测试结果"></p><p>然后去Anaconda用户页面选中需要的库安装，基本的比如ipython,jupyter notebook,spyder这些是要用到的，装一下。</p><p>done!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;安装tensorflow cpu 2.0以上版本。&lt;/p&gt;
&lt;p&gt;python2.0已经支持python3.7了。不过我装的还是3.6的pyhon。&lt;/p&gt;
&lt;p&gt;Anaconda的安装教程就不说了，按照网上来就OK了。这里只简单记录下如何安装tensorflow-cpu版
      
    
    </summary>
    
    
      <category term="疑难杂症" scheme="https://qingfengbangzuo.github.io/categories/%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87/"/>
    
    
      <category term="其他" scheme="https://qingfengbangzuo.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>层次聚类</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/Hierarchical/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/Hierarchical/</id>
    <published>2020-06-12T11:01:59.835Z</published>
    <updated>2020-06-12T11:02:55.502Z</updated>
    
    <content type="html"><![CDATA[<h1 id="层次聚类"><a href="#层次聚类" class="headerlink" title="　层次聚类"></a>　层次聚类</h1><p>聚类方法总体来说分为两类：一类是分割型的(partitioning)，另一种是层次型(hierarchical).</p><p>层次聚类从聚类的方式来看分为bottom-up自底向上(merge)，和top-down自上而下(split)。对于层次型聚类，我们可以选择聚类的个数作为停止条件，也可以选择一个距离threshold,当所有类之间的距离满足threshold之后，聚类停止。</p><p>自上而下的分类型算法大体如下：</p><ol><li>设置停止条件。</li><li>将所有的数据都视为一个大类，然后计算所有数据点之间的距离。(这个距离不只是指欧几里得距离)，在所有的类中，选择距离最大的两个点分别归入类c1和c2。且将这两个点作为类中心。</li><li>对所有数据点进行聚类，也就是计算所有数据点距离c1和c2中心点的距离，距离哪个近就归入哪一类。</li><li>重复步骤2,3，直到满足停止条件。</li></ol><p>自下而上的merge算法大体如下：</p><p>​    1.将所有的样本都当做一个类簇。</p><p>​    2.计算两个类的之间的距离，找到两个距离最小的两个簇c1和c2</p><p>​    3.合并类c1和c2为一个类簇。</p><p>​    4.重复步骤2,3，直到满足停止条件。</p><p>层次聚类的思想是简单的，主要是计算距离的方式：</p><p>1.单连接(single-link).找到两个簇之间距离最小的两个样本的距离最为两个集合的距离，也就是说，最近两个样本之间的距离越小，这两个类之间的相似度越大。</p><p>2.全连接.(complete-link).找到两个簇之间距离最大的两个样本的距离为两个集合的距离。</p><p>3.把两个集合中的点两两的距离全部放在一起求一个平均值，相当于也能得到合适一点的结果。</p><p>4.取两两距离的中值，与取平均值相比更能够解除个别偏离样本对结果的干扰。</p><p>5.把两个集合中的点两两的距离全部放在一起求和然后除以两个结合中的元素个数</p><p>6.求每个集合的中心点（也就是将集合中的所有元素对应维度相加然后除以元素个数得到一个向量），然后用中心点代替集合再去计算集合间的距离。</p><p>7.　ward距离（适用于merge），最小化被聚合的类的方差。<br>8.　余弦距离。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;层次聚类&quot;&gt;&lt;a href=&quot;#层次聚类&quot; class=&quot;headerlink&quot; title=&quot;　层次聚类&quot;&gt;&lt;/a&gt;　层次聚类&lt;/h1&gt;&lt;p&gt;聚类方法总体来说分为两类：一类是分割型的(partitioning)，另一种是层次型(hierarchical).&lt;/p
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/"/>
    
      <category term="聚类算法" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ML" scheme="https://qingfengbangzuo.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>密度聚类(DBSCAN)</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/DBSCAN/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/DBSCAN/</id>
    <published>2020-06-12T10:59:57.087Z</published>
    <updated>2020-06-12T11:01:35.885Z</updated>
    
    <content type="html"><![CDATA[<p>聚类主要分为两种类型，一种是partitioning,另一种是hierarchical。层次聚类属于第一种，是为了解决大数据下的聚类参数不好设置和自动聚类的问题。</p><p>主要依据是利用类内的点密度大于类外噪声。</p><p>The key idea is that for each point of a cluster the neighbor-<br>hood of a given radius has to contain at least a minimum<br>number of points, i.e. the density in the neighborhood has to<br>exceed some threshold。</p><p>距离的定义决定了最后聚类的形状，因此合适的距离函数（相似性函数）对聚类效果有比较大的影响，在实际应用中可以根据实际需求定义合适的距离函数。</p><p>Definition 1: (Eps-neighborhood of a point) The Eps-<br>neighborhood of a point p, denoted by NEps(P), is defined<br>NEps(P) = {q E D I dist(p,q) _&lt; Eps}</p><p>定义２：(直接密度可达direct-density-reachable)一个点p对于点q是密度度可达的条件如下：</p><ol><li><p>$p \in N_{Eps}(q)$</p></li><li><p>$|N_{Eps}|&gt;= MinPts$(core point condition)</p></li></ol><p>定义３：(密度可达(density-reachable))：一个点Ｐ是密度可达点q wrt.Eps and MinPts，当存在一条链，该链满足p1,p2,…pn(pn=q)，$p_{i+1}$是直接密度可达from$p_i$。</p><p>Definition 4: (density-connected) A point p is density-<br>connected to a point q wrt. Eps and MinPts if there is a point<br>o such that both, p and q are density-reachable from o wrt.<br>Eps and MinPts.<img src="/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/DBSCAN/2020-05-13 10-33-44屏幕截图.png" alt="2020-05-13 10-33-44屏幕截图"></p><p>简单解释一下，密度可达是对直接密度可达的扩展，直接密度可达是指，当点q满足核心点条件时，点p直接可达点q，要满足p在以点q为核心，以Eps为半径的园内。</p><p>密度可达对这个概念进行了扩展，密度可达是指当点q满足核心点条件时，p密度可达q，是指在以q为核心，以Eps为半径的园内存在至少一点o，该点也满足核心点条件，并且在以该点o为核心，以Eps为半径的园内存在一点p（包括边界点）。这是三点的情况，可以继续以o为核心继续扩展边界，只要满足p1,p2,…pn满足pn密度可达pn-1。这个概念也是不对称的，因为点p可能不满足核心点条件，因此不能说p密度可达q，而q也密度可达p。</p><p>密度连接性是对上述概念的进一步扩展，指的是，存在一核心点o,在以改点为核心，以Ｅps为半径的园内存在两点a和b。这两个点也满足核心点条件，且p在以a为核心，Eps为半径的圆内，q在以b为核心，以Eps为半径的圆内，那么就可以说p和q是密度连接的。</p><p>有了上述概念，就可以定义聚类标准了：</p><p>Definition 5: (cluster) Let D be a database of points. A<br>cluster C wrt. Eps and MinPts is a non-empty subset of D<br>satisfying the following conditions:<br>1) $All \quad p, q: ifp E C$ and q is density-reachable from p wrt.<br>Eps and MinPts, then $q \in C$. (Maximality)<br>2) V$All \quad p, q \in C$: p is density-connectedto q wrt. EPS and<br>MinPts. (Connectivity)</p><p>Definition 6: (noise) Let $C_t ….. C_k $be the clusters of the<br>database D wrt. parameters Epsi and MinPtsi, i = 1 ….. k.<br>Then we define the noise as the set of points in the database<br>D not belonging to any cluster Ci , i.e. noise = {$p \in D$ I $All\quad i: p \notin Ci$)</p><p>简单讲就是，对于$q \in C_i$，q满足核心点要求，对于所有密度可达q的点p，都有$p \in C_i$。而同一个类内的点p和q，都满足密度可链接。</p><p>道理是清晰的，这个是收敛的，收敛的条件是当聚类外侧的点都不满足核心点条件。</p><p>这里有一个判断是否聚类的标准：</p><p>Lemma1: Let p be a point in D and INEps(p)l &gt; MinPts.<br>Then the set O = {o I o E D and o is density-reachable from<br>p wrt. Eps and MinPts } is a cluster wrt. Eps and MinPts.</p><p>接下来的问题就转换成如何确定合适的Eps和MinPts了。作者给出了一个启发式的算法来自动确定在这两个值，</p><p><img src="/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/DBSCAN/2020-05-13 11-01-45屏幕截图.png" alt="2020-05-13 11-01-45屏幕截图"></p><p>大概意思就是定义一个核心点到第k个最近距离的点p的距离k-dist。然后根据这个距离遍历所有的点，对于噪声有这样的特性，就是噪声的k-dist一般都比较大，因为他们是离群的，而一般聚类内部的点的k-dist都比较小。这样，取第一个峰谷，作为threshold。作者经过实验，发现对于2维数据，k取４就足够了，此时对应的threshold就可以作为Eps，MinPts取４．</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;聚类主要分为两种类型，一种是partitioning,另一种是hierarchical。层次聚类属于第一种，是为了解决大数据下的聚类参数不好设置和自动聚类的问题。&lt;/p&gt;
&lt;p&gt;主要依据是利用类内的点密度大于类外噪声。&lt;/p&gt;
&lt;p&gt;The key idea is that
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/"/>
    
      <category term="聚类算法" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ML" scheme="https://qingfengbangzuo.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Kmeans</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/kmeans/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/kmeans/</id>
    <published>2020-06-12T10:57:42.337Z</published>
    <updated>2020-06-12T10:59:49.658Z</updated>
    
    <content type="html"><![CDATA[<h3 id="kmeans算法数学原理"><a href="#kmeans算法数学原理" class="headerlink" title="kmeans算法数学原理"></a>kmeans算法数学原理</h3><p>目标函数：</p><script type="math/tex; mode=display">J = \sum_{j=1}^K\sum_{i=1}^N z_{i,j} dist(x_i,\hat{x_j})^2</script><p>初始的kmeans算法使用的是欧几里得距离(l2)。</p><p>K代表聚类的数目，N代表数据的个数，$z_j$是latent变量, $\hat{x_j}$表示聚类j内的所有数据点的均值。</p><p>那我们规定:</p><script type="math/tex; mode=display">z_{i,j} = 1, if  \quad j = argmin_j ||x_i-\hat{x_j}||^2\\0,ohters.</script><p>对$\hat{x_j}$求偏导：</p><script type="math/tex; mode=display">2\sum_{i=1}^Nz_{i,j}(x_i-\bar{x_j}) = 0</script><p>可得：</p><script type="math/tex; mode=display">\hat{x_{j}} = \frac{\sum_{i}r_{i,j}x_i}{\sum_{n}r_{i,j}}</script><p>当我们引入latent变量以后，我们发现，通过给定$\hat{x_j}$一个初值，我们可以计算出所有数据的$r_{i,j}$（E步），而后，根据偏导我们又可以得到$\hat{x_j}$，这个值从表面上看就是属于聚类j的所有点的均值(M步)。</p><p>要注意这个值是从偏导得来的。</p><p>因此，kmeans算法可以简单的归结为：</p><p>1.初始化聚类数目和初始的中心点$\hat{x_j}$.</p><p>2.根据最小化$dist(x_i,\hat{x_j})^2$，对隐含变量$z_{i,j}$赋值。</p><p>3.根据得到的$z_{i,j}$,更新中心点$\hat{x_j}$。</p><p>4.重复步骤２，３，直到算法满足停止条件，结束迭代。</p><p>5.输出中心点坐标。</p><p>对于$\hat{x_j}$的更新公式有一个在线版本：</p><script type="math/tex; mode=display">\mathbf{u_j}^{new} = \mathbf{u_j}^{old}+\eta_i(\mathbf{x_i}-\mathbf{u_j^{old}})</script><p><strong>算法不足</strong></p><p>１、对异常点敏感。主要原因是平均值对噪声比较敏感。</p><p>２、很难发现大小差别很大的簇，也就是说小簇很容易被归为大簇。</p><p>３、依赖于k值和初始质心的选择，不好的质心容易陷入局部最优。</p><p>４、对离散特征的处理不是很好。</p><p><strong>初始值</strong></p><p>SSE下降幅度越大，说明初始质心选择教好。</p><p>初始值选择会影响到kmeans最终的收敛效果。</p><h3 id="效果衡量指标"><a href="#效果衡量指标" class="headerlink" title="效果衡量指标"></a>效果衡量指标</h3><p><strong>SSE (sum of square error)：误差平方和</strong></p><script type="math/tex; mode=display">SSE = \sum_{i=1}^k\sum_{p\in C_i}|p-m_i|^2</script><p>$C_i$表示簇i。$m_i$表示簇i的簇质心。$p$属于簇内的样本点。</p><p>SSE表示簇内样本点到质心的距离之和，反应簇内的松散度。簇越紧密SSE越小，反之越大。</p><p>SSE受到初始质心位置的选择的影响，当质心选的不好的时候，SSE的变化教缓。而当质心选的好的时候，SSE的变化会先呈现剧烈下降，而后平缓下降。</p><p><strong>肘策略(Elbow method)</strong></p><p>SSE随着聚类的簇的数量增加会呈现出减小的趋向。当只有一类的时候，此时SSE为整个数据集上最大，当有n类（n是样本量）时，SSE为０．</p><p>从这个角度看，我们可以根据SSE随k变化的走向来选取合适的k。合适的k有这样的特征，小于k的SSE做急速下降，大于k的SSE做平缓下降。</p><p><strong>轮廓系数SC(Silhouette Coefficient)</strong></p><script type="math/tex; mode=display">S = \frac{(b-a)}{max(a,b)}</script><p>从公式可以看出$S \in [0,1]$. a表示样本i到同一簇内的其他点的不相似度（距离）的平均值。b表示样本i到其他簇的平均不相似（距离）度的最小值。a越小说明它更应该被分到该簇，反之相反。b越大说明改点离其他簇的距离越大，说明簇的离散度更好，反之则说明两个簇贴的比较近。</p><p><img src="/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/kmeans/2020-05-09 12-19-07屏幕截图.png" alt="2020-05-09 12-19-07屏幕截图"></p><p>轮廓系数结合了凝聚度和离散度两个指标。每次聚类后每个样本都会有一个轮廓系数，当它为１时，说明这个聚类对于该样本点来说非常好，当它为０的时候，说明这个点可能处在边界上，当小于０时，暗示分类错误了。</p><p>但是仅仅使用sc这一个指标无法很好的衡量聚类效果，因为当一些小的聚类被聚合成了一个大类时，这个指标反应不出来这种情况。因此需要结合另一个指标，轮廓宽度。轮廓宽度即字面意思，表示单个簇的轮廓大小。在sc差不多的情况下，各簇的轮廓宽度越均匀越好。轮廓宽度指的是簇的样本数目大小。</p><p>轮廓宽度可以用平均SSE来衡量？</p><p><strong>ＣＨ(Calinski-Harabasz)</strong></p><p>类别内部数据的距离平方和越小越好，类别之间的距离平方和越大越好，CH的值越大说明聚类效果越好。</p><script type="math/tex; mode=display">CH(k) = \frac{SSB}{SSW}\frac{m-k}{k-1} \\SSW = \sum_{i=1}^m||x_i-C_{pi}||^2 \\SSB = \sum_{j=1}^kn_j||C_j-mean(X)||^2</script><p>m为训练集数目，k为类别数。<img src="/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/kmeans/2020-05-09 12-50-28屏幕截图.png" alt="2020-05-09 12-50-28屏幕截图"></p><p>mean(X)表示训练集所有数据点的中心点。$n_j$表示第j个簇内样本点的个数。$C_j$表示簇内质心。</p><p>(m-k)/(k-1)的含义是希望使用尽可能少的类来聚合尽可能多的数据集。</p><p>还有一个和CH差不多的指标</p><p><strong>WB</strong></p><script type="math/tex; mode=display">WB = K\frac{SSW}{SSB}</script><p>这个指标是越小越好，表达的意思和上面CH差不多。</p><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>鉴于kmeans算法的缺点</p><p>1、对异常值敏感</p><p>解决方法：标准化</p><p>2、容易将小类数据归并为大类</p><p>３、容易收到初始参数的影响：k值和初始知心的选择。</p><p><strong>Canopy(树冠)算法帮助确定k值</strong></p><p>Canopy算法的大致思路为：</p><p>确定一个半径为$T_1$和一个半径为$T_2$的圆，$T_1&lt;T_2$。随机选定一个初始点Ｏ，然后以此为中心以$T_1$ 和$T_2$为半径画两个圆，那么数据集将会被$T_1，T_2$划分为三部分，第一部分为$T_1$内部，被标为初始质心的近点。其次为$T_1$　和$T_2$的截出来的空心圆，落在该区域内的点被标为模棱两可的点。最后是$T_2$外侧的点，被标记为远点。</p><p>然后在落在远点区域的数据点中，随机选出一个数据点E执行以上操作，$T_1$和$T_2$不变，会发现，原先落在空心圆内的一部分数据点现在被标为了E为质心的圆的近点。</p><p>在O和E的共同外点中随机选择一点Q执行以上操作，知道所有的外点都被标为了近点。</p><p>最终根据Canopy算法返回的k值(质心数目)作为kmeans算法k值选取的参考，可以大致确定k值的取值范围。</p><p><strong>评价：</strong>这种方法在一定程度上能够帮助确定kmeans算法k值的选择，但是有一个痛点就是$T_1$和$T_2$的选择不好确定。</p><p><strong>KMeans++</strong></p><p>该算法是对kmeans算法的改进，针对的是kmeans算法初始质心选择的问题。</p><p>kmeans++认为初始质心的选择相隔的越远越好，这个<strong>远</strong>通过公式</p><script type="math/tex; mode=display">P = \frac{D(x)^2}{\sum_{x\in X}D(x)^2}</script><p>来刻画。就是说先随机确定一个质心，然后根据上面的公式选取另Ｐ足够大的另一个数据点作为第二个质心。这里的p是一个概率值，选第二个质心的时候是依据概率Ｐ选的，p值越大，说明被选的概率越大。</p><p>但是这样做有几个地方没有解释清楚，１、这种方式明显受离群值的影响较大。２、到底多远算是合适，选择距离初始质心最远的点显然也不合适。3、多个质心的时候如何选择。</p><p><strong>二分KMeans</strong></p><p>二分kmeans算法遵循了一个原则，SSE越大的簇，聚类情况越不好，内部可能包含了多个小簇。因此对它进行二分。</p><p>算法流程：</p><p>１、对数据集选定两个初始质心，进行二分。</p><p>２、计算SSE，在当前所有的簇中，找出SSE较大的那个簇。</p><p>３、对２步骤中找出簇进行选点，二分。</p><p>４、重复2-3，知道簇的数量达到了k设定的数量。(这里个人认为也可以将SSE下限作为停止条件，或者两者结合使用)</p><p>二分kmeans的优点是算法原理清晰，效率高。且受初始质心的选择的影响小。因为即使一开始选择了一个不怎么好的质心，在接下来的分裂中也会把这个不好的簇再次划分开。但是，受影响小不代表不受影响，如果一开始就选择的质心不好，会使得后续的分裂过程增加，且容易使得某些簇过小。因此，结合kmeans的思想，初始质心的选择最好分的越开越好。</p><p>二分kmeans的另一个优点是，最后划分的簇中，每个簇的SSE都不会很大。</p><p><strong>Kernel KMeans</strong></p><p>原理：在低维线性不可分的数据集，映射到高维可能线性可分。(核函数的机制)</p><p>kernel kmeans对于低维线性不可分的数据相对传统的kmeans算法有较大的提升，但是随之而来的计算开销也陡然上升。</p><p>因此传统的kmeans适用于低维线性可分的情况，而kernel kmeas适用于低维线性不可分的情况。两者各有利弊。</p><p><strong>k-medoids(k-中心聚类)</strong></p><p>该算法针对kmeans算法对异常值敏感这一痛点进行优化。核心思想是利用数据点本身作为质心。这样可以避免异常值带来质心偏移。</p><p>算法流程：</p><p>１、选点，任意选取k个点作为初始质心。</p><p>２、聚类。距离使用的是曼哈顿距离。（sum of Absolute Differences）</p><script type="math/tex; mode=display">SAD = \sum_{m=1}^k\sum_{p_i \in C_i}dist(p_i,o_i)=\sum_{m=1}^k\sum_{p_i \in C_i}\sqrt{\sum_{j=1}^{n_{C_i}}(p_{i,j}-o_{i,j})^２}</script><p>３、遍历。对于第i类中的所有点，遍历第i类中除medoids外的所有点作为质心时的代价，选择代价最小的值作为新的质心。</p><p>４、迭代。重复２－３，直到medoids不再变化。</p><p>５、返回聚类结果。</p><p>k-medoisds算法的pors 和cons</p><p>优点是：对异常值不敏感，因为使用的是实际的数据点。其次，，每次运行的偏差小。最后，对初始的medoisds设置不敏感。<strong>对类别特征也适用。</strong></p><p>缺点是: 运算复杂度高，适合低维小数据量。而当数据量升高时，较少的异常值对kmeans算法的影响也有限，因此kmeans算法实际应用的频率要高于k-medoisds。(kmeans计算复杂度要低)<img src="/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/kmeans/2020-05-09 19-20-14屏幕截图.png" alt="2020-05-09 19-20-14屏幕截图"></p><p><strong>ISODATA(Iterative Self Organizing Data Analysis Techniques Algorithm)</strong></p><p>该算法的一个显著特点就是类别数目会随着聚类的过程而变化，比方说初始设定的k值为２０,那么最终的聚类结果会在[k/2, 2k]之间。</p><p>对类别数的“合并”：当聚类结果的某一类中的样本数目太少，或两个类间的距离太近。</p><p>对类别数的“分裂”：当聚类结果中的某一类的类内方差太大，将该类进行分裂。</p><p>ISODATA算法应用于卫星图像聚类。</p><p><strong>Mini-Batch K-Means</strong>（适合大数据量）</p><p>mini-batch kmeans算法是针对大数据量而设计的一个优化算法，当数据量大于１万的时候，就要考虑使用mini-batch kmeans了。</p><p>顾名思义，mini-batch(分批处理)的方法对数据点之间的距离进行计算。</p><p><img src="/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/kmeans/2020-05-12 11-08-48屏幕截图.png" alt="2020-05-12 11-08-48屏幕截图"></p><p>计算过程不必使用所有样本数据，而是从不同类别的样本数据中抽取一部分出来代表各自类型进行计算。由于样本量少，所以相应的运行时间会减小。</p><p>mini-batch是聚类精度和运行效率之间的一个折中，当使用所有样本进行聚类时，聚类精度会达到最高，当使用随机样本进行聚类时，效率最高，但是精度较差，mini-batch在两者之间取得了一个很好的折中，当batch值选择好的情况下，min-batch kmeans的聚类精度不会很差。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><div class="table-container"><table><thead><tr><th>-</th><th style="text-align:center">-</th></tr></thead><tbody><tr><td>Canopy+kmeans</td><td style="text-align:center">粗聚类选择大致的Ｋ值范围</td></tr><tr><td>kmeans++</td><td style="text-align:center">优化了初始质心的选择问题，初始质心距离越远越好</td></tr><tr><td>二分kmeans</td><td style="text-align:center">解决聚类小类容易归入大类的问题</td></tr><tr><td>ISODATA</td><td style="text-align:center">动态聚类</td></tr><tr><td>k-medoids</td><td style="text-align:center">解决对异常值敏感问题</td></tr><tr><td>mini-batch kmeans</td><td style="text-align:center">解决大数据量样本效率低下问题，分批进行聚类，损失小部分精度换取计算速度</td></tr><tr><td>kernel kmeans</td><td style="text-align:center">解决线性低维现行不可分的问题</td></tr></tbody></table></div><p>各类算法都各有优劣，要明确使用的场景。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;kmeans算法数学原理&quot;&gt;&lt;a href=&quot;#kmeans算法数学原理&quot; class=&quot;headerlink&quot; title=&quot;kmeans算法数学原理&quot;&gt;&lt;/a&gt;kmeans算法数学原理&lt;/h3&gt;&lt;p&gt;目标函数：&lt;/p&gt;
&lt;script type=&quot;math/t
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/"/>
    
      <category term="聚类算法" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ML" scheme="https://qingfengbangzuo.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>高斯过程</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B/</id>
    <published>2020-06-12T00:42:16.850Z</published>
    <updated>2020-06-12T10:56:30.999Z</updated>
    
    <content type="html"><![CDATA[<p>如果不懂高斯过程的话，就假设$f(x) = w^Tx$这个函数为一个变量，而这个变量服从一维高斯分布。换句话理解，假设w先验服从一个高斯分布，那么给定一个确定值x，$w^Tx$是一个服从一维高斯分布的变量。所以说，高斯过程的假设与参数先验分布的假设是等价的。</p><p>高斯过程的假设是：</p><p>对于回归模型$y = f(X)+\epsilon$，若函数$f(X)$的形式不是固定的，则其为潜函数（可理解为一个变量），即它的每个取值都是函数空间的一个测度。GPR假设该函数的先验分布服从均值为0的高斯分布：</p><script type="math/tex; mode=display">f(X) \sim N(0,K(X,X'))</script><p>假设有N个样本，样本空间为$R_d$维度，对于所有的$n\in N,X = \{X_1,X_2…,X_n\}\in R_d$有</p><script type="math/tex; mode=display">P([f(X_1),...,f(X_t)]) \sim N([0,k(X,X')])</script><p>式中k(X,X’)为核函数，0均值高斯过程由其核函数完全决定,高斯过程就是假设这个联合分布服从均值为0，方差为K的高斯分布：</p><script type="math/tex; mode=display">k(X,X') = E[f(X)f(X')]</script><p>该核函数本质是一个关于x的函数，反应的是相应随机变量$f(x_1)$和$f(x_2)$之间的相关程度。</p><h4 id="从线性回归推高斯过程"><a href="#从线性回归推高斯过程" class="headerlink" title="从线性回归推高斯过程"></a>从线性回归推高斯过程</h4><p>从线性回归出发推高斯过程也称作从参数空间的角度出发理解高斯过程。</p><p>考虑一个线性函数</p><script type="math/tex; mode=display">y = w^T\phi (x)+\epsilon</script><p>假设$p(\epsilon)\sim N(0,\delta^2)$，那么</p><script type="math/tex; mode=display">P(\mathbf{y}|X,w,\delta^2) \sim \prod_{n=1}^NN(y_n|w^T\phi(x_n),\delta^2)</script><p>在此基础上，假设模型的权重w服从先验分布$p(w) \sim N(w|0,\alpha I)$.由贝叶斯定理可知</p><script type="math/tex; mode=display">p(\mathbf{w}|\mathbf{y},X)\propto p(\mathbf{y}|\mathbf{w},X)p(\mathbf{w})</script><p>由高斯分布的共轭性可知，对于正太分布的似然函数和方差已知的正太先验分布，其后验分布也为正太分布，因此参数后验分布如下</p><script type="math/tex; mode=display">p(w|y,X) \sim N(w|\frac{1}{\delta^2}S_N\Phi y,S_N)\\S_N^{-1} = \alpha I + \frac{1}{\delta^2}\Phi^T\Phi</script><p>对这个参数后验分布取log可得</p><script type="math/tex; mode=display">lnp(\mathbf{w|y}) = -\frac{1}{2\delta^2}\sum_{n=1}^N\{y_n-\mathbf{w^T}\phi(x_n)\}^2 - \frac{\alpha}{2}\mathbf{w^Tw}+const</script><p>通过这个式子，我们知道所谓的正则化从贝叶斯角度来解释就是对参数做假设先验分布为均值为0的高斯分布。</p><p>对于给定的测试样本X，贝叶斯线性回归可通过边缘化模型权重，即按照其后验积分得到测试结果$f_<em> = f(x_</em>)$的概率分布</p><script type="math/tex; mode=display">p(f_*|\mathbf{X,y.X_*},\delta_n^2) = \int p(f_*|\mathbf{X_*,w},\delta_n^2)p(\mathbf{w|X,y})dw = N（f_*|\mathbf{m_N^T\phi(X_*)},\delta_N^2(X)） \\  \\  \delta^2_N(x) = \delta^2 + \Phi^T S_N \Phi</script><p>方差解释了一个现象，预测值的分布情况受噪声和参数分布的双重影响，<strong>但是随着样本的增多，参数分布带来的影响逐渐趋于0.</strong></p><script type="math/tex; mode=display">f(x_*) = \mathbf{m_N^T}\phi(X_*) = \frac{1}{\delta^2} \phi(x)^TS_N \Phi^T \mathbf{y} = \sum_{n=1}^N \frac{1}{\delta^2}\phi(x)^TS_N\phi(x_n)t_n</script><p>定义核函数</p><script type="math/tex; mode=display">k(x,x') = \frac{1}{\delta^2}\phi(x)^TS_N\phi(x')</script><p>当定义的核函数为一个线性核的时候</p><script type="math/tex; mode=display">k(x,x') = \alpha \phi(x)^T\phi(x')</script><p>上式可改写成</p><script type="math/tex; mode=display">p(f_*|\mathbf{X,yX_*},\delta^2) = N(f_*|\bar{f_*},cov(f_*))\\\bar{f_*} = k(X_*,X)(K+\delta^2I)^{-1}\mathbf{y}\\cov(f_*) = k(X_*,X_*)+\delta^2 -K(X_*,X)(K+\delta^2I)^{-1}k(X,X_*)</script><p>无论哪个版本的核，我们都可以看到，所谓的预测结果都是相同的形式</p><script type="math/tex; mode=display">f_* = \sum_{n=1}^N a·k(x_*,x)</script><p>$a$是常数。</p><h4 id="表示定理"><a href="#表示定理" class="headerlink" title="表示定理"></a>表示定理</h4><p>事实上存在这样一个核<strong>表示定理</strong>：</p><p>令$\\H$为核函数$k$对应的再生希尔伯特空间，$||h||_{\\H}$表示$\H$中关于h的范数，对于任意单调递增函数$\Omega:[0,\infty]\rarr {\\R}$和任意非负损失函数$l:{\\R}^m\rarr[0,\infty]$优化问题</p><script type="math/tex; mode=display">min_{h\in {\\H}} F(h) = \Omega(||h||_{\\H})+l(h(x_1),h(x_2),...,h(x_m))</script><p>的解总可以写成</p><script type="math/tex; mode=display">h^*(x) = \sum_{i=1}^ma_i k(x,x_i)</script><p>表示定理对于损失函数没有限制，对正则化项$\Omega$仅要求单调递增，甚至不要求$\Omega$是凸函数，意味着对于一般的损失函数和正则化项，优化问题的最优解都可以表示为核函数的线性组合。</p><h4 id="从函数空间推高斯过程"><a href="#从函数空间推高斯过程" class="headerlink" title="从函数空间推高斯过程"></a>从函数空间推高斯过程</h4><p>假设回归方程如下，注意这里已经不再涉及参数空间，所以形式上已经没有了w变量</p><script type="math/tex; mode=display">t_n = y_n +\epsilon</script><p>这里应用高斯过程的假设，可以把$y_n = y(x_n)$看成是一个随机变量。$\epsilon$服从均值为0，方差为$\beta^{-1}$,有</p><script type="math/tex; mode=display">p(t_n|y_n,\epsilon) = N(t_n|y_n,\beta^{-1})</script><p>由于噪声独立与样本点，因此考虑目标值$\mathbf{t} = \{t_1,t_2,,…,t_n\}$,和$\mathbf{y} = \{y_n,y_2,…,y_n\}$有</p><script type="math/tex; mode=display">p(\mathbf{t|y}) = N(\mathbf{t|y},\beta^{-1}I)</script><p>由高斯过程假设，</p><script type="math/tex; mode=display">p(\mathbf{y}) = p(t_1,t_2,...,t_n) = N(\mathbf{y}|0,K)</script><p>我们的目标是求得边缘分布$p(\mathbf{t})$</p><script type="math/tex; mode=display">p(\mathbf{t}) = \int p(\mathbf{t|y})p(\mathbf{y})d\mathbf{y} = N(\mathbf{t}|0,C)</script><p>这里的C的元素如下</p><script type="math/tex; mode=display">C(x_N,x_m) = k(x_n,x_m)+\beta^{-1}\delta_{nm}</script><p>可知，p(t)的分布受到p(y)和噪声的影响，这和上面的结论（预测值的分布情况受噪声和参数分布的双重影响）相似.</p><p>由高斯过程假设，对于测试样本有</p><script type="math/tex; mode=display">f_* \sim N(0,k(X_*,K_*)</script><p>那么联合分布</p><script type="math/tex; mode=display">p(\mathbf{t_{N+1}}) = [\begin{matrix}\mathbf{t}\\f_*\end{matrix}] \sim N(0,[\begin{matrix}k(X_N,X_N)+\beta^{-1}\delta_{nm}&k(X_N,X_*)\\k(X_*,X_N)& k(X_*,X_*)+\beta^{-1}\end{matrix}])</script><p>由边缘高斯分布的性质，可以由上面的联合分布，直接求得条件分布$p(f_*|\mathbf{t]})$.</p><script type="math/tex; mode=display">p(f_*|\mathbf{t}) \sim N(\mathbf{k^TC_N^{-1}t},c - \mathbf{k^TC_N^{-1}k})</script><p>换一种形式表示</p><script type="math/tex; mode=display">\mathbf{m_{new}} =  \mathbf{k^TC_N^{-1}t} = k(X_*,X)(K(X_n,X_N)+\beta^{-1}\delta_{nm})^{-1}t \\\mathbf{v_{new}} = c - \mathbf{k^TC_N^{-1}k} =k(X_*,X_*)+\beta^{-1} - k(X_*,X)(K(X_n,X_N)+\beta^{-1})K(X,X_*)</script><p>可见，和上面从参数空间推导的高斯过程结果一致。所以有人说，一般的线性回归是高斯过程的一种特例，它使用的核函数是线性核。</p><p>值得注意的是在GPR过程中，常使用的一种核函数是exponential of a quadratic form,</p><script type="math/tex; mode=display">k(x_n,x_m) = \theta_0 exp\{-\frac{\theta_1}{2}||x_n-x_m||^2\}+\theta_2+\theta_3x_n^Tx_m</script><h3 id="自动关联确定-Automatic-relevance-determination-ARD"><a href="#自动关联确定-Automatic-relevance-determination-ARD" class="headerlink" title="自动关联确定(Automatic relevance determination, ARD)"></a>自动关联确定(Automatic relevance determination, ARD)</h3><p>利用高斯过程可以做这样一件事儿，确定特征向量个元素与target之间的关联性大小。关联性越大，参数值越大，关联性越小，值越小。</p><p>具体原理也不难。假设我们采取这样的核函数</p><script type="math/tex; mode=display">k(x_n,x_m) = \theta_0 exp\{-\frac{1}{2}\sum_{i=1}^D\eta_i(x_{ni}-x_{mi})^2\}+\theta_2+\theta_3x_n^Tx_m</script><p>参数$\eta_i$反应了各特征元素与target之间的关联性。通过对$p(\mathbf{t|\eta})$进行最大似然估计，可以估计出参数向量的值。</p><script type="math/tex; mode=display">p(\mathbf{t}) = \int p(\mathbf{t|y})p(\mathbf{y})d\mathbf{y} = N(\mathbf{t}|0,C) \\ln p(t|θ) = -\frac{1}{2}ln|C_N| - \frac{1}{2}\mathbf{t^TC^{-1}_Nt}-\frac{N}{2}ln(2\pi)</script><p>这里的关联性指的是该特征对拟合target的贡献大小。由于核函数反应的是随机变量$y(x)$方差的变化，而核函数是关于x的函数，当我们对各特征赋予权重求表示随机变量的方差的 时候，权重的大小可以近似的表明该单一特征对随机变量$y(x)$方差的影响。</p><p>总结为一句，在高斯过程中我们可以通过ARD的策略确定特征向量中每个特征与target的关联程度，对于关联程度小的特征，我们可以抛弃。作用上类似于LDA降维。</p><h4 id="使用laplace近似来近似后验分布-p-w-t"><a href="#使用laplace近似来近似后验分布-p-w-t" class="headerlink" title="使用laplace近似来近似后验分布$p(w|t)$"></a>使用laplace近似来近似后验分布$p(w|t)$</h4><p>先弄清楚一个问题，为什么要使用laplace近似？</p><p>我们知道，$t = y = \delta(w^Tx)$，对于这样一个决策函数，参数的后验分布并不是高斯分布，因为引入了sigmod函数。但是我们希望得到一个高斯形式的参数后验分布，所以采用laplace近似策略将真实的参数后验分布近似为一个高斯分布。</p><p>具体近似流程</p><script type="math/tex; mode=display">p(\mathbf{w|t})\propto p(\mathbf{w})p(\mathbf{t|w})</script><p>由于真实的后验分布与似然函数与先验分布成正比，所以在实际使用时我们一般直接使用似然函数与先验分布的乘积。</p><script type="math/tex; mode=display">ln p(\mathbf{w|t}) = -\frac{1}{2}\mathbf{(w-m_0)S_0^{-1}(w-m_0)}+\sum_{n=1}^N\{t_nlny_n+(1-t_n)ln(1-y_n)\}+const</script><p>这里$y_n = \delta(w^T\phi_n)$</p><p>使用最大后验估计（MAP）可以通过上述的式子求出$\mathbf{w_{MAP}}$,求解的方法是求解上述对数似然的一阶导。根据laplace近似的原理，我们需要找到一个该最大后验分布的极值$\mathbf{W_{MAP}}$,常使用的方法是牛顿法。</p><script type="math/tex; mode=display">S_N = -\nabla\nabla lnp(\mathbf{w|t})  = \mathbf{S_0^{-1}} +\sum_{n=1}^Ny_n(1-y_n)\phi_n\phi_n^T</script><p>近似函数</p><script type="math/tex; mode=display">q(w) = N(\mathbf{w|w_{MAP},S_N})</script><h3 id="高斯过程用于分类"><a href="#高斯过程用于分类" class="headerlink" title="高斯过程用于分类"></a>高斯过程用于分类</h3><p>高斯过程可以用于分类。由于高斯过程的输出是整个坐标轴，因此同样需要一个激活函数将其映射到（0，1）范围内，常见的激活函数就是前面使用的sigmod函数。</p><script type="math/tex; mode=display">p(t|a) = \sigma(a)^t(1-\sigma(a))^{1-t}</script><p>高斯过程用于分类，假设随机变量$a（X）$服从高斯过程。假设是</p><script type="math/tex; mode=display">p(a_{N+1}) \sim N(a_{N+1}|0,C_{N+1})  \\C(x_n,x_m) = k(x_n,x_m)+v\delta_{nm}</script><p>目标是$p(t_{N+1}|\mathbf{t_N})$</p><script type="math/tex; mode=display">p(t_{N+1}=1|\mathbf{t_N}) = \int p(t_{N+1} = 1 |a_{N+1})p(a_{N+1}|\mathbf{t_N}) da_{N+1}</script><p>对于$p(t_{N+1}=1|a_{N+1})$有</p><script type="math/tex; mode=display">p(t_{N+1}=1|a_{N+1}) = \sigma(a_{N+1})</script><p>对于第二项</p><script type="math/tex; mode=display">p(a_{N+1}|\mathbf{t_N}) = \int p(a_{N+1}|\mathbf{a_N})p(\mathbf{a_N|t_N})d\mathbf{a_{N}}</script><p>利用高斯过程的结果</p><p>简单回顾一下高斯过程的结果,高斯过程的目标是$p(t_{N+1}|\mathbf{t_N)}$,这里有$p(a_{N+1}|\mathbf{a_N})$,而$t_n = a_n+\epsilon$</p><p>两个变量都服从高斯分布，相差一个噪声，当无燥情况下，两者是相等的。也就是说，当无燥情况下，两者的分布是一致的，所以这里可以使用高斯过程的结果。</p><script type="math/tex; mode=display">p(a_{N+1}|\mathbf{a_N}) \sim N(\mathbf{a_{N+1}|k^TC_N^{-1}t},c - \mathbf{k^TC_N^{-1}k})</script><p>对于第二项采用laplace近似</p><script type="math/tex; mode=display">p(\mathbf{t_N|a_N}) = \prod_{n=1}^N \sigma(a_n)^{t_n}(1-\sigma(a_n))^{1-t_n} = \prod_{n=1}^N e ^{a_nt_n}\sigma(-a_n)</script><p>对后验分布采用laplace近似</p><script type="math/tex; mode=display">\Phi(\mathbf{t_N|a_N}) = lnp(\mathbf{a_N})+lnp(\mathbf{t_N|a_N})</script><p>利用牛顿法求得它的极值点$a_N^*$ ,二阶海塞矩阵的形式为</p><script type="math/tex; mode=display">\mathbf{H} = -\nabla\nabla \Phi(\mathbf{a_N}) = \mathbf{W_n+C_N^{-1}}</script><p>可得</p><script type="math/tex; mode=display">q(\mathbf{a_N}) = N(\mathbf{a_N|a_N^*,H^{-1}})</script><p>逆推回去有</p><script type="math/tex; mode=display">E[a_{N+1}|\mathbf{t_N}] =  \mathbf{k^T(t_N-\sigma_N)}\\var[a_{N+1}|\mathbf{t_N}] =  c- \mathbf{k^T(W_n^{-1}+C_n)^{-1}k}</script><p>最后将上式代入边缘分布积分方程，可得最终结果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;如果不懂高斯过程的话，就假设$f(x) = w^Tx$这个函数为一个变量，而这个变量服从一维高斯分布。换句话理解，假设w先验服从一个高斯分布，那么给定一个确定值x，$w^Tx$是一个服从一维高斯分布的变量。所以说，高斯过程的假设与参数先验分布的假设是等价的。&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/"/>
    
      <category term="概率统计模型" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="ML" scheme="https://qingfengbangzuo.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>强化学习一些笔记</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-06-11T16:00:00.000Z</published>
    <updated>2020-06-22T12:16:44.189Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Incremental-Implementation"><a href="#Incremental-Implementation" class="headerlink" title="Incremental Implementation"></a>Incremental Implementation</h3><script type="math/tex; mode=display">Q_{n+1} = Q_n + \frac{1}{N(A)}[R_n - Q_n]</script><p>注意step_size $N(A)$，它随着时间步的累计而线性增长，但是通常，最近的奖赏相比过去很久的奖赏更有说服力，因此，我们希望给最近发生的奖赏一个稍大的权重。解决办法是将$N(A)$设置为固定值。</p><script type="math/tex; mode=display">\begin{align*}Q_{n+1}& = Q_n + \alpha[R_n - Q_n]\\ &=\alpha R_n +(1-\alpha)Q_n \\&=\alpha R_n +(1-\alpha)[\alpha R_{n-1}+(1-\alpha)Q_{n-1 }] \\&=\alpha R_n+(1-\alpha)\alpha R_{n-1}+(1-\alpha)^2\alpha R_{n-2}+...+(1-\alpha)^{n-1}\alpha R_1+(1-\alpha)^n Q_1 \\&=(1-\alpha)^n Q_1+\sum_{i=1}^n \alpha(1-\alpha)^{n-i}R_i\end{align*}</script><p>可以看到，当设置步长为一个固定值的时候，我们实现了如期的效果。原文称之为:<em>exponential<br>recency-weighted average.</em></p><p>对于采样平均策略中$a = N(A)$,由大数定理可知必会依概率1收敛。</p><script type="math/tex; mode=display">\sum_{n=1}^{\infty} a_n(a) = \infty \\\sum_{n=1}^{\infty}a_n^2(a) < \infty</script><p>第一个条件保证了步长足够大使得最终克服初始值或随机影响而收敛。第二个条件保证了最终，步长变得足够小从而保证收敛。</p><p>固定值$a$是不满足上述两个条件的，表明预测值不会完全收敛但是会继续随着最近接收到的reward而变化，这种属性适合nonstaionary 环境或者nonstationart问题。并且，满足上述条件的步长$a$需要调参来达到合适的收敛率，这在理论工作中比较常用，但是在实际问题中并不常用。</p><h3 id="optimistic-initial-values"><a href="#optimistic-initial-values" class="headerlink" title="optimistic initial values"></a>optimistic initial values</h3><p>书中提到了三种简单的method，一种是sample-average，一种是exponential recency-weighted average,最后一种是这块要介绍的optimistic initial values。</p><p>这种方案是聚焦在初始值上，因为大多数stationary问题都需要依赖初始的Q值，被称为偏差，如上一块公式所示，这个偏差随着时间并不会消失（虽然随着时间在减小）。显然我们不满足于将初始值设为0，乐观初始值是指将初始值设为 一个比奖赏大的数，比如+5，这样，因为所有的奖赏都小于这个预设值，因此，一轮更新过后，agent会竭尽所能的去探索。因此，它的最优率相对于sample-average收敛较快。</p><h3 id="Upper-Confidence-Bound-Action-Selection"><a href="#Upper-Confidence-Bound-Action-Selection" class="headerlink" title="Upper-Confidence-Bound Action Selection"></a>Upper-Confidence-Bound Action Selection</h3><script type="math/tex; mode=display">A_t = argmax_a[Q_t(a)+c\sqrt{\frac{lnt}{N_t(a)}}]</script><p>c是调节参数，$lnt$表示随着时间增大，不确定性的增长变缓。$N_t(a)$表示t之前动作a被选择的次数。整个公式的作用是随着时间的进行，那些Q值小的动作和那些被选过好多次的动作再次被选中的频率会下降，而那些未被选择过得动作则有越来越大的机会被选中。最终所有的动作都会被选中，实验效果表明，采用UCB的效果较$\epsilon$-greedy略好。</p><h3 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h3><script type="math/tex; mode=display">V_*(s) = max_a E[R_{t+1}+\gamma V_*(S_{t+1})|S_t = s,A_t =a]\\=max_a \sum_{s',r}p(s',r|s,a)[r+\gamma V_*(s')]\\q_*(s,a) = E[R_{t+1}+\gamma max_{a'}q_*(S_{t+1},a')|S_t=s,A_t=a]\\= \sum_{s',r}p(s',r|s,a)[r+\gamma max_{a'}q_*(s',a')]</script><p>贝尔曼最优方程有这样一个特性，即V（s） = $max_a$q(s,a),即它的状态值不断靠近当前状态下的最大动作值。</p><h3 id="policy-evaluation"><a href="#policy-evaluation" class="headerlink" title="policy evaluation"></a>policy evaluation</h3><script type="math/tex; mode=display">V_\pi(s)= \sum_a\pi(a|s)\sum_{s',r} p(s',r|s,a)[r+\gamma V_\pi(s')]</script><p>给定一确定策略，通过Policy evaluation可以得到该策略下的值函数。（多次迭代，最终收敛于一个个确定值）</p><h3 id="policy-Increament"><a href="#policy-Increament" class="headerlink" title="policy Increament"></a>policy Increament</h3><script type="math/tex; mode=display">q_\pi (s,\pi'(s))\geq v_\pi(s)</script><p>对于所有的s满足上式，则$\pi’$优于$\pi$。</p><p>Greedy策略满足上述要求</p><script type="math/tex; mode=display">\pi'(s ) = argmax_a q_\pi(s,a)=argmax_a\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]</script><script type="math/tex; mode=display">v_{\pi'}(s) = max_a E[R_{t+1}+\gamma v_{\pi'}(S_{t+1})|S_t =s,A_t =a]\\= max_a \sum_{s',r}p(s',r|s,a)[r+\gamma v_{\pi'}(s')]</script><h3 id="policy-Iteration"><a href="#policy-Iteration" class="headerlink" title="policy Iteration"></a>policy Iteration</h3><script type="math/tex; mode=display">\pi_0 \rarr v_{\pi_0}\rarr\pi_1\rarr v_{\pi_1} \rarr \pi_2\rarr...\rarr\pi_*\rarr  v_{*}</script><p>给定一个策略，根据policy evaluation可以计算当前策略下的值函数，然后对当前策略进行提升，循环往复，最终收敛到最优策略和最优值函数。</p><h3 id="value-Iteration"><a href="#value-Iteration" class="headerlink" title="value Iteration"></a>value Iteration</h3><p>one update of each state is called value iteration.相比于策略迭代，需要迭代到该策略下的最优value值。</p><p>利用贝尔曼最优方程：</p><script type="math/tex; mode=display">V_{k+1}(s) = max_a E[R_{t+1}+\gamma V_k(S_{t+1})|S_t = s,A_t =a]\\=max_a \sum_{s',r}p(s',r|s,a)[r+\gamma V_k(s')]\\</script><p>对于任意$v_0$，序列{$v_k$}在相同的条件下能够收敛到$v_*$。</p><p>如何终止呢？</p><p>当值函数变化被压缩在一个阈值$\theta$内时，就可以强行进行终止了。</p><p>Value Iteration加快了收敛速度，因为不需要进行完整的evaluation,它是通过使用贝尔曼最优方程的特性，使得V(s)不断地靠地$max_a$ q(s,a)</p><h3 id="Asynchronous-Dynamic-Programming"><a href="#Asynchronous-Dynamic-Programming" class="headerlink" title="Asynchronous Dynamic Programming"></a>Asynchronous Dynamic Programming</h3><p>这里的非同步动态规划是指取消了更新轮数的限制，即并非在一轮更新中更新所有的状态，而是仅仅针对某些或者某个状态进行更新，这样就会出现，某些状态可能更新了n多次，但是某些状态仅仅只更新了一两次。</p><p>policy evaluation 、value Iteration、asynchronous dynamic programming三者的区别：</p><p>policy evaluation和value Iterationd的区别在于前者更新知道达到当前策略下的最优v值，而后者通过更新公式仅仅更新一次（轮）。非同步动态规划与前两者的区别是它没有更新轮次的概念，可能仅仅与最优行为有关，那些与最优行为无关的状态可能不会得到太大的关注。它的更新粒度最小。</p><h3 id="MOnte-carlo-methord"><a href="#MOnte-carlo-methord" class="headerlink" title="MOnte carlo methord"></a>MOnte carlo methord</h3><p>较DP优势：</p><p>1、没有用到bootstrap,且每个状态的estimate都是独立的，因为在估计状态值的时候没有用到其他状态值。</p><p>2、估计单个状态值的计算代价与状态的数量无关。只与采样有关。</p><p>3、我们可以从任何状态起开始采样，且状态值估计不受这个影响。</p><p>蒙特卡罗采样只与experience有关，这个experience可以是实际经历也可以是virtual experience，分为every-visit average 和 first-visit average。这两种平均都能收敛到$v_\pi$，前提是episodes的数量趋近于无穷。</p><p>在环境信息已知的情况下，通过v值就可以确定最优策略，但是在蒙特卡罗采样策略下，只通过v值确定最优策略是不充分的，我们还需要确定$q_\pi（s,a）$。</p><p>在蒙特卡罗采样策略下，因为v值和q值都是通过从experience中采样得到的，但是在stationary策略下，每个状态下的动作是确定的，也就是说experience中可能只有某个状态下的某个动作（或状态），其他动作不会出现，而他们相应的q值因为无法采样而为0.这会影响收敛，因为收敛需要遍历每个状态和动作。</p><p>解决这个问题有两种方法，一种是对初始状态进行随机化，每个状态以某个概率随机出现，这样，随着episodes数量趋于无穷，状态会得到遍历，最终会收敛，这被称作<strong>Exloration Starting</strong>。但是这种方法在实际场景中不能保证收敛性，也不实用。另一种是在每个状态下，给予其他动作以一定的出现概率。这样就会使得策略变成non-stationary。但是效果较好。</p><p>目前讨论的蒙特卡罗策略有两个假设。</p><p>One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes.</p><p>下面的内容介绍了去除这两个假设的方法。</p><p>去掉假设2是相当对容易的，有两种思路，一种是在evaluation中保证足够多的迭代次数。另一种是引入Value Iteration的思想，不需要得到准确的$v_\pi$值，换成蒙特卡罗采样也就是，在策略$\pi$下，不需要大量采样，只需要一次采样，即一个episode（蒙特卡洛的采样数量对应了DP的更新迭代次数，因为DP下，更新次数越多，越接近$V_\pi$，但在蒙特卡罗策略下采样次数决定了靠近$V_\pi$的距离）。这样也就是变向的将evaluation和improvement交替进行，即下面Monte carlo ES所示。</p><h3 id="Monte-carlo-ES"><a href="#Monte-carlo-ES" class="headerlink" title="Monte carlo ES"></a>Monte carlo ES</h3><p>这个算法的特殊之处在于它将evaluation过程和improvement过程交替进行，这样依然会收敛于最优策略，但是似乎还未被证明。</p><p>可以使用Incremental Implementation来替代上述中维护return列表的做法。</p><p>而解决假设1的也有两种方案，即on-policy和off-policy。</p><p>终于找到了on-policy和off-policy的区别。这两种方法被用来解决Exploration Staring问题。</p><p>On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas o↵-policy methods evaluate or improve a policy di↵erent from that used to generate the data.</p><h3 id="On-policy-and-Off-policy"><a href="#On-policy-and-Off-policy" class="headerlink" title="On-policy and Off-policy"></a>On-policy and Off-policy</h3><p>在继续介绍on-policy和off-policy之前，先给出两个概念：</p><p>所谓的off-policy是指使用生成数据的策略和要学习的策略是不同的两个策略，前者被称为<strong>behaivior policy</strong>后者被称为<strong>target policy</strong>，这两个策略是不同的，也就是说我们用来学习数据是通过其他的策略产生的，或者说这个数据本身就是一些经验数据，但是我们需要从这些经验数据中学习我们的最优策略。所谓的学习策略就是更新策略。on-policy就是将生成数据的策略与要学习的策略为同一策略，是off-policy的一种特例。</p><p>On - policy策略一般是soft的，即$\pi(a|s)&gt;0$，也就是每个动作的选择概率均大于0，将贪婪策略soft化，就是常见的$\epsilon-greedy$策略。</p><p>在off-policy control中，生成数据的策略通常也使用$\epsilon-greedy$ policy，因为behavior-policy需要是exporation policy。而学习策略一般是greedy policy。</p><p>注：off-policy的一个优点</p><p>‘’An advantage of this separation is that the target policy may be deterministic (e.g., greedy), while   the behavior policy can continue to sample all possible actions.‘’</p><p>由于我们使用的数据是经过behavior policy下产生的，所以我们得到的v值是behavior policy策略下的，而不是我们的学习策略下的v值。由于我们的最终目标是不断改善我们的target policy，得到最优v值和最优策略，因此我们需要得到的是target policy下的v值，这就产生了一个问题，如何转换？</p><h3 id="Importance-Sampling"><a href="#Importance-Sampling" class="headerlink" title="Importance Sampling"></a>Importance Sampling</h3><p>重要性采样用来解决上述问题。</p><p>假设有N个episodes，每个episodes在不同的策略下都对应这一个出现概率，计算方法如下：</p><script type="math/tex; mode=display">Pr = \prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)</script><p>定义重要性采样率（importance-sampling ratio）</p><script type="math/tex; mode=display">p_{t:T-1} = \frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_k,A_k)}=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}</script><p>通过转换公式</p><script type="math/tex; mode=display">E[p_{t:T-1}G_t|S_t=s] = v_\pi(s)</script><p>以上就是重要性采样的思想。</p><p>那么计算$v_\pi（s）$有两种metrics（“初访” 情况下）。分别是</p><p>ordinary importance sample :</p><script type="math/tex; mode=display">V(s) = \frac{\sum_{t\in J(s)}p_{t:T-1}G_t}{|J(s)|}</script><p>the set of all time steps in which state s is visited, denoted J(s).</p><p>weighted importance sampling：</p><script type="math/tex; mode=display">V(s) = \frac{\sum_{t\in J(s)}p_{t:T-1}G_t}{\sum_{t\in J(s)}p_{t:T-1}}</script><p>可见weighted importance sampling对V（s）进行了方差处理，而ordinary importance sampling没有，实际上ordinary 的方差可以是无穷大的，但是它的biase很小，且随着samples数量趋近于无穷，它的方差也会随之下降，但是这是一个缓慢的过程，在例子中可以看到，millions steps后，它并没有准确收敛到1。而weighted效果要好一点，它始终是围绕着均值上下波动的，但是该均值是有偏的，但是同样会随着samples的增多而收敛到正确的均值，也就是最优v值。</p><p>在实际应用中，“每访”的效果更佳，但是”每访“情况下，ordinary和weighted都是有偏的，但是都会随着samples的增加而趋于正确的均值。</p><p>注：infinite variance</p><p>当MDP中出现了闭环，那么会导致无限方差的情况，这种情况下，随着sample增加到很大，可能也不能很好的收敛到正确值，具体例子看P107页例。</p><h3 id="Incremental-Implementation-on-Monte-carlo-sampling"><a href="#Incremental-Implementation-on-Monte-carlo-sampling" class="headerlink" title="Incremental Implementation on Monte carlo sampling"></a>Incremental Implementation on Monte carlo sampling</h3><p>根据前面提到的Incremental Implementation 方法来用于有重要性采样的Monte carlo问题中。</p><p>对于ordinary importance sampling，直接将采样公式变换成Incremental Implementation形式就行。</p><p>对于weighted 采样，需要做一点改变</p><script type="math/tex; mode=display">V_{n+1} = V_n + \frac{W_n}{C_n}[G_n -V_n]\\C_{n+1} = C_n + W_{n+1}</script><p>这个是”每访”下的算法，注意算法中的C和Q是两个列表，也就是说C和Q各维持了一个存放不同（S_t,A_t）的列表，每一轮episode来临时，都会更新列表中相应的状态动作对值，对于“初访”情形，需要设置一个条件来访问：</p><p>‘’’’’’——————————————————————————————</p><p>​    $G\larr\gamma G+R_{t+1}$</p><p>​        unless the Pair ($S_t,A_t$) appear in $S_0,A_0,S_1,A_1,…S_{t-1},A_{t-1}$</p><p>​            $C(S_t,A_t)\larr C(S_t,A_t)+W$</p><p>​            $Q(S_t,A_t)\larr Q(S_t,A_t)+ \frac{W}{C(S_t,A_t)}[G-Q(S_t,A_t)]$</p><p>​    $W \larr W\frac{\pi(A_t|S_t)}{b(A_t|S_t)}$</p><p>‘’’————————————————————————————————</p><p>最后一行，由于使用的是greedy策略，所以$\pi(a|s)= 1或0$,注意倒数第二句，如果$A_t \neq\pi(S_t)$程序就会退出，此时$\pi(A_t|S_t) = 0 $.</p><p><strong>当episode中大部分动作是non-greedy，尤其靠前部分是Non-greedy的时候，这个算法收敛将变得十分缓慢。</strong></p><p>出现这样的现象是直观的，因为这个算法倒数第二行设置了退出条件，由于算法是从尾部遍历到头部，所以当尾部出现了non-greedy的动作，那么就会退出内循环，开始下一个episode，这样就会造成该non-greedy动作之前的所有episode报废。</p><h3 id="Timporal-Difference-Learning"><a href="#Timporal-Difference-Learning" class="headerlink" title="Timporal-Difference Learning"></a>Timporal-Difference Learning</h3><p>TD是DP和Monte carlo采样的结合，因为在更新过程中，它既对$\pi$进行了采样，又使用了bootstrap，这里的bootstrap和DP中的不同，在DP中使用的expected return 和expected value（即根据分布得到的return 和value均值），而TD在更新时，需要根据一步采样，得到$R_{t+1}$和$S’$，使用$R_{t+1}$+$\gamma V(S’)$来更新，这里的$R_{t+1}$是采样值，这里的$V（S’）$是估计值。</p><p>无论是Mento carlo 还是TD，它们都是通过”error”来更新的，并且它们之间存在关系如下：</p><script type="math/tex; mode=display">G_t - V(S_t) = R_{t+1} + \gamma G_{t+1} - V（S_t）-\gamma V(S_{t+1})\\=\delta_t +\gamma (G_{t+1}-V(S_{t+1}))\\=\delta_t + \gamma \delta_{t+1}+ \gamma^2（G_{t+2}-V（S_{t+2}))\\=\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2}+...+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-1}(G_T-V(S_T))\\=\delta_t + \gamma\delta_{t+1}+\gamma^2\delta_{t+2}+...+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t}(0-0)\\=\sum_{k=t}^{T-1}\gamma^{k-t}\delta_k</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Incremental-Implementation&quot;&gt;&lt;a href=&quot;#Incremental-Implementation&quot; class=&quot;headerlink&quot; title=&quot;Incremental Implementation&quot;&gt;&lt;/a&gt;Incremen
      
    
    </summary>
    
    
      <category term="Reinforcement Learning" scheme="https://qingfengbangzuo.github.io/categories/Reinforcement-Learning/"/>
    
    
      <category term="RL" scheme="https://qingfengbangzuo.github.io/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="https://qingfengbangzuo.github.io/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%91%E7%B1%BB%E6%A8%A1%E5%9E%8B/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://qingfengbangzuo.github.io/2020/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A0%91%E7%B1%BB%E6%A8%A1%E5%9E%8B/%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2020-06-11T16:00:00.000Z</published>
    <updated>2020-06-12T10:53:46.127Z</updated>
    
    <content type="html"><![CDATA[<p>决策树</p><h3 id="信息熵的定义"><a href="#信息熵的定义" class="headerlink" title="信息熵的定义"></a>信息熵的定义</h3><ul><li><p>下面是一位大神从编码的角度考虑信息熵的定义</p><p>假如有ABCD四种答案，且每一种答案的概率均为四分之一。那么编码这个答案的所需要的位数为2（基于二进制编码，二进制编码会带来诸多好处）。用数学语言描述就是需要</p><script type="math/tex; mode=display">n = log_2N</script><p>N是事件不同情况的取值,这里是四。下面做一个变换，将对数的变量改为概率：</p><script type="math/tex; mode=display">log_2（4）=log_2(\frac{1}{1/4})=-log_2(1/4) = -log_2(p)</script><p>这里进行了泛化，但在逻辑上并不严谨，因为假设是所有情况出现的概率相等。</p><p>信息熵的定义： </p></li></ul><script type="math/tex; mode=display">H(X) = E[-log(P(X))]= - \sum_{a\in D}P(X)log_2(P(X))</script><p>​    将上面问题的答案带入后会发现$H(X)$和$n$是相等的。所以熵又叫做平均编码长度。</p><p>​    将假设推广之后就是信息熵的正式定义了。但上述的推导是不严谨的，仅限于有助于理解。</p><p>​    当答案出现的的概率不相等时，比如</p><script type="math/tex; mode=display">P(A) = 1/4\quad P(B)=1/8\quad P(C)=1/,2\quad P(D)=1/8</script><p>​    那么编码A,B,C,D所需要的位数会变为：2, 3，1, 3</p><p>​    那么平均编码长度为（按照熵的定义） </p><script type="math/tex; mode=display">H(X) = \frac{1}{4}\times 2+\frac{1}{8}\times3+\frac{1}{2}\times1+\frac{1}{8}\times3 = 1.75</script><p>​    这就是压缩编码。换一个角度来考虑，事件的不确定性降低（C出现的概率最大），对应的熵也    会跟着降低。</p><hr><p>YJango视屏中对熵的又一个解释：</p><ul><li><p>当一个事件 有多种情况时，这件事对观察者来讲具体是某种情况的不确定性叫做熵。</p></li><li><p>而能够消除观察这对这件事情不确定性的事物叫做信息。</p><ol><li>调整概率可以减小不确定性。</li><li>能够排除干扰</li><li>确定情况</li></ol></li></ul><p>信息是相对的，不同的信息对与不同的观察者来讲意义不同，如果小红知道会这道题，不管告不告诉小红正确答案，小红对这到题的熵都为0比特。小名不会这到题，那么小名对这道题的熵为2比特。</p><p>由于事件的不确定性是指数性的，而不是线性的（千克），因此不能用简单的除法，而要用对数，</p><p>参照物是2也就是底。如果事件发生的概率相等，也就是服从均匀分布，那么采用</p><script type="math/tex; mode=display">n = log_2N</script><p>就能计算熵，但是如果概率不相等时，怎么办呢，我们平均的思想来计算</p><script type="math/tex; mode=display">H(X) = E[-log(P(X))]= - \sum_{a\in D}P(X)log_2(P(X))</script><h3 id="Gini指数"><a href="#Gini指数" class="headerlink" title="Gini指数"></a>Gini指数</h3><p>基尼指数反应的也是事件的不确定性，只不过它是从抽样概率的角度来解释的：</p><script type="math/tex; mode=display">Gini（D） = \sum_{x=1}\sum_{y\ne x}P(X_i)P(X_y) = 1-\sum_{x=1}^{|Y|}p_x^2</script><p>基尼指数反应了从数据集中随机抽取两个样本，其类别标记不一致的概率。因此，基尼指数越小，则数据集D的纯度越高。</p><h2 id="ID3、C4-5、CART"><a href="#ID3、C4-5、CART" class="headerlink" title="ID3、C4.5、CART"></a>ID3、C4.5、CART</h2><p>ID3使用的是信息增益准则，即：</p><script type="math/tex; mode=display">a_*=argmax_{a\in A}Gain(D,a)\\Gain(D,a) = Ent(D) - \sum_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)</script><p>$D^v$是属性$a$上的第$v$个分支结果中包含了D中所有在属性$a$上取值为$a^v$的样本。直观上的理解就是未分支时的熵减去以某个属性为根节点分支后的熵。</p><p><strong>信息增益至少说明了两件事，一件是这个过程是个提取属性的过程，提取属性意味着总体事件取值的种类减少，事实也确实如此，每一次分支，我们都是相当于给定某一些属性变量的确定值的前提下计算熵的。所以总熵必定减小，因为引入了有效信息，这就是信息增益。另一件事是每个属性对于整体的熵贡献不同，属性多的熵贡献程度大，这十分符合我们的常识，我们常说的复杂刻画的就是变量取值太多而造成的熵巨大的情况。</strong></p><p><strong>那么使用信息增益来构建决策树的思想就很清晰了，我们本着一个原则“树的深度最小”，然后逐层熵减，直至熵为0未知，也就是系统中不存在不确定性。那么，为了迎合这个前提，我们减熵的过程就需要按照属性对熵的贡献程度来分支啦。</strong></p><p>ID3存在对多取值属性有偏好的问题，因此 C4.5弥补了这种不足。</p><p>C4.5采用增益率来作为分支决策：</p><script type="math/tex; mode=display">Gain_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}\\其中 \quad IV(a) = -\sum_{v=1}^V\frac{|D^v|}{|D|}log_2 \frac{|D^v|}{|D|}</script><p>其中的$IV(a)$是节点自身的熵。</p><p>CART 是基于Gini指数来进行分支决策的一类决策树。它采用下面的准则来生成树</p><script type="math/tex; mode=display">Gini\_index(D,a) = \sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)</script><p>《统计学习原理》从概率角度给出了决策树的另一个解释。</p><p>决策树本质上是基于条件概率模型的算法。结社X为表示特征变量的随机变量，Y为表示类的随机变量，那么这个条件概率分布可以表示为$P(Y|X)$。X取值于给定划分下单元的集合，Y取值于类的集合，各叶子节点上的条件概率往往偏向与某一个类，即属于某一类的概率比较大。决策树分类时将该节点强行分到概率大的那一类去。那从这个角度来看，决策树的目的就是寻找一组有效的条件$(x_1,x_2,…x_n)$，使得$P(Y|(x_1,x_2,…x_n))$最大，那么如果新样本符合条件(x_1,x_2,…x_n），那么根据条件概率，我们就可以预测它的类别。</p><p>如果任由树形算法生长，那么最终会生成一个一个小单元，也就是叶子节点，每个叶子节点依据上述的的条件概率大小来判断样本的分类。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;决策树&lt;/p&gt;
&lt;h3 id=&quot;信息熵的定义&quot;&gt;&lt;a href=&quot;#信息熵的定义&quot; class=&quot;headerlink&quot; title=&quot;信息熵的定义&quot;&gt;&lt;/a&gt;信息熵的定义&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;下面是一位大神从编码的角度考虑信息熵的定义&lt;/p&gt;
&lt;p&gt;假如有AB
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/"/>
    
      <category term="树类算法" scheme="https://qingfengbangzuo.github.io/categories/Machine-Learning/%E6%A0%91%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ML" scheme="https://qingfengbangzuo.github.io/tags/ML/"/>
    
  </entry>
  
</feed>
